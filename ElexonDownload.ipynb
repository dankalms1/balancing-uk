{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dea8d38",
   "metadata": {},
   "source": [
    "# High-throughput Elexon bulk crawler\n",
    "\n",
    "Usage:\n",
    "* Specify `START_DATE`, `END_DATE`\n",
    "* Run required cell to define function\n",
    "* Run final cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0191aa9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad53b62a-3ce0-47e8-8b82-83e65cbf5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import datetime as dt\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "BASE_URL = \"https://data.elexon.co.uk/bmrs/api/v1\"\n",
    "\n",
    "START_DATE = dt.date(2021, 6, 30)\n",
    "END_DATE   = dt.date(2025, 6, 30)\n",
    "\n",
    "BASE_DIR = Path(\"bmrs_json_raw\")\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PER_MINUTE   = AsyncLimiter(4920, 60)\n",
    "PER_SECOND   = AsyncLimiter(82, 1)\n",
    "TIMEOUT      = aiohttp.ClientTimeout(sock_connect=3, sock_read=5)\n",
    "CONCURRENCY  = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04cb81",
   "metadata": {},
   "source": [
    "# Crawler Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a08da",
   "metadata": {},
   "source": [
    "## Historical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc363-45af-4771-9de8-4ef9bea87cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINTS = {\n",
    "    # ------------------ Generation ----------------------------------------\n",
    "    \"GEN_PER_TYPE\":\n",
    "        \"/datasets/AGPT?publishDateTimeFrom={from_ts}&publishDateTimeTo={to_ts}\",\n",
    "    \"INTER\":\n",
    "        \"/generation/outturn/interconnectors?settlementDateFrom={date}&settlementDateTo={date}\",\n",
    "    \"DAYAHEAD_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?from={from_ts}&to={to_ts}&processType=day%20ahead\",\n",
    "    \"INTRADAYPROCESS_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?from={from_ts}&to={to_ts}&processType=intraday%20process\",\n",
    "    \"INTRADAYTOTAL_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?from={from_ts}&to={to_ts}&processType=intraday%20total\",\n",
    "    \"ACTUAL_GEN_WIND_SOLAR\":\n",
    "        \"/datasets/AGWS?publishDateTimeFrom={from_ts}&publishDateTimeTo={to_ts}\",\n",
    "\n",
    "    # ------------------ Demand -------------------------------------------\n",
    "    \"DAYAHEAD_DEMAND\":\n",
    "        \"/forecast/demand/day-ahead/history?publishTime={date}\",\n",
    "    \"INDICATED_DAYAHEAD_DEMAND\":\n",
    "        \"/forecast/indicated/day-ahead/history?publishTime={date}\",\n",
    "    \"ACTUAL_DEMAND\":\n",
    "        \"/demand/outturn?settlementDateFrom={date}&settlementDateTo={date}\",\n",
    "\n",
    "    # ------------------ Balancing ----------------------------------------\n",
    "    \"SYSTEM_PRICES\":\n",
    "        \"/balancing/settlement/system-prices/{date}\",\n",
    "    \"BSAD\":\n",
    "        \"/datasets/netbsad?from={from_ts}&to={to_ts}\",\n",
    "    \"MID\":\n",
    "        \"/datasets/mid?from={from_ts}&to={to_ts}\",\n",
    "    \"NONBM\":\n",
    "        \"/datasets/NONBM?from={from_ts}&to={to_ts}\",\n",
    "\n",
    "    # ------------------ Transmission -------------------------------------\n",
    "    \"LOLPDRM\":\n",
    "        \"/forecast/system/loss-of-load?from={from_ts}&to={to_ts}\",\n",
    "\n",
    "    # ------------------ Temperature --------------------------------\n",
    "    \"TEMPERATURE\":\n",
    "        \"/temperature?from={from_ts}&to={to_ts}\",\n",
    "}\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def _ph(day: dt.date):\n",
    "    \"\"\"Return placeholders dict for a given date.\"\"\"\n",
    "    iso = day.isoformat()\n",
    "    return {\n",
    "        \"date\"   : iso,\n",
    "        \"from_ts\": f\"{iso}T00:00:00Z\",\n",
    "        \"to_ts\"  : f\"{iso}T23:59:59Z\",\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Stats\n",
    "latencies = []\n",
    "total_requests = 0\n",
    "start_time_global = None\n",
    "\n",
    "async def _fetch(sess: aiohttp.ClientSession, url: str, dest: Path, log: Path):\n",
    "    global total_requests\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        start_time = time.perf_counter()\n",
    "        async with sess.get(url) as r:\n",
    "            if r.status == 404:\n",
    "                return\n",
    "            if r.status == 429:\n",
    "                await asyncio.sleep(5)\n",
    "                async with sess.get(url) as r2:\n",
    "                    if r2.status in (404, 429):\n",
    "                        return\n",
    "                    r2.raise_for_status()\n",
    "                    payload = await r2.json()\n",
    "            else:\n",
    "                r.raise_for_status()\n",
    "                payload = await r.json()\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        total_requests += 1\n",
    "\n",
    "    if isinstance(payload, dict) and payload.get(\"data\") == []:\n",
    "        log.write_text((log.read_text() + dest.stem + \"\\n\") if log.exists()\n",
    "                       else dest.stem + \"\\n\")\n",
    "        return\n",
    "\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(payload, fh)\n",
    "\n",
    "async def _all_tasks():\n",
    "    day = START_DATE\n",
    "    while day <= END_DATE:\n",
    "        ph = _ph(day)\n",
    "        for code, tpl in ENDPOINTS.items():\n",
    "            sub = BASE_DIR / code\n",
    "            sub.mkdir(exist_ok=True)\n",
    "            if code == \"TEMPERATURE\":\n",
    "                ph_temp = ph.copy()\n",
    "                ph_temp[\"from_ts\"] = ph_temp[\"date\"]\n",
    "                ph_temp[\"to_ts\"]   = ph_temp[\"date\"]\n",
    "                url = f\"{BASE_URL}{tpl.format(**ph_temp)}\"\n",
    "            else:\n",
    "                url = f\"{BASE_URL}{tpl.format(**ph)}\"\n",
    "            dest = sub / f\"{ph['date']}.json.gz\"\n",
    "            log  = sub / \"empty.log\"\n",
    "            yield url, dest, log\n",
    "        day += dt.timedelta(days=1)\n",
    "\n",
    "async def progress_report():\n",
    "    print(f\"{'Time':>8} | {'Reqs':>8} | {'Avg Lat':>8} | {'Max Lat':>8} | {'Thr/sec':>8} | {'Thr/min':>8}\")\n",
    "    print(\"-\" * 62)\n",
    "    while True:\n",
    "        await asyncio.sleep(15)\n",
    "        avg_lat = mean(latencies) if latencies else 0\n",
    "        max_lat = max(latencies) if latencies else 0\n",
    "        elapsed = (dt.datetime.now() - start_time_global).total_seconds()\n",
    "        tput_sec = total_requests / elapsed if elapsed > 0 else 0\n",
    "        tput_min = tput_sec * 60\n",
    "        print(f\"{dt.datetime.now().strftime('%H:%M:%S'):>8} | \"\n",
    "              f\"{total_requests:8d} | \"\n",
    "              f\"{avg_lat:8.3f} | \"\n",
    "              f\"{max_lat:8.3f} | \"\n",
    "              f\"{tput_sec:8.2f} | \"\n",
    "              f\"{tput_min:8.0f}\")\n",
    "\n",
    "async def main():\n",
    "    global start_time_global\n",
    "    start_time_global = dt.datetime.now()\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        async for url, dest, log in _all_tasks():\n",
    "            tasks.append(asyncio.create_task(_fetch(sess, url, dest, log)))\n",
    "        reporter = asyncio.create_task(progress_report())\n",
    "        await asyncio.gather(*tasks)\n",
    "        reporter.cancel()\n",
    "\n",
    "    elapsed_run = (dt.datetime.now() - start_time_global).total_seconds()\n",
    "    avg_lat = mean(latencies) if latencies else 0\n",
    "    max_lat = max(latencies) if latencies else 0\n",
    "    tput_sec = total_requests / elapsed_run if elapsed_run > 0 else 0\n",
    "    tput_min = tput_sec * 60\n",
    "    print(\"\\nFinal Summary\")\n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total reqs:':<15}{total_requests}\")\n",
    "    print(f\"{'Avg latency:':<15}{avg_lat:.3f} s\")\n",
    "    print(f\"{'Max latency:':<15}{max_lat:.3f} s\")\n",
    "    print(f\"{'Thr/sec:':<15}{tput_sec:.2f} req/sec\")\n",
    "    print(f\"{'Thr/min:':<15}{tput_min:.0f} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385a9db",
   "metadata": {},
   "source": [
    "## History of the wind generation forecast (WINDFOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "# Config\n",
    "ENDPOINT     = \"/forecast/generation/wind/history\"\n",
    "\n",
    "# Explicit mapping of local clock to settlementPeriod\n",
    "PUBLISH_MAP    = {\n",
    "    \"03:30\": 6,\n",
    "    \"05:30\": 10,\n",
    "    \"08:30\": 16,\n",
    "    \"10:30\": 20,\n",
    "    \"12:30\": 24,\n",
    "    \"16:30\": 32,\n",
    "    \"19:30\": 38,\n",
    "    \"23:30\": 46,\n",
    "}\n",
    "\n",
    "FINAL_DIR   = BASE_DIR / \"DETAILED_WINDFOR\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stats\n",
    "latencies = []\n",
    "total_requests = 0\n",
    "start_time_global = None\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, date: datetime.date, pt: str, sp: int):\n",
    "    global total_requests\n",
    "    publish_str = f\"{date.isoformat()}T{pt}Z\"\n",
    "    dest = FINAL_DIR / f\"{date}_{sp}.json.gz\"\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        start_time = time.perf_counter()\n",
    "        resp = await sess.get(f\"{BASE_URL}{ENDPOINT}\", params={\"publishTime\": publish_str})\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        total_requests += 1\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"]   = pd.to_datetime(df[\"startTime\"],   utc=True)\n",
    "\n",
    "    pub_dt = df[\"publishTime\"].iloc[0]\n",
    "    window_start = pub_dt\n",
    "    window_end   = pub_dt + timedelta(hours=24)\n",
    "    filtered = df[(df[\"startTime\"] >= window_start) & (df[\"startTime\"] <= window_end)].copy()\n",
    "\n",
    "    filtered[\"publishTime\"] = filtered[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    filtered[\"startTime\"]   = filtered[\"startTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    out = {\"metadata\": payload.get(\"metadata\", {}), \"data\": filtered.to_dict(orient=\"records\")}\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def progress_report():\n",
    "    print(f\"{'Time':>8} | {'Reqs':>8} | {'Avg Lat':>8} | {'Max Lat':>8} | {'Thr/sec':>8} | {'Thr/min':>8}\")\n",
    "    print(\"-\" * 62)\n",
    "    while True:\n",
    "        await asyncio.sleep(15)\n",
    "        avg_lat = mean(latencies) if latencies else 0\n",
    "        max_lat = max(latencies) if latencies else 0\n",
    "        elapsed = (datetime.now() - start_time_global).total_seconds()\n",
    "        tput_sec = total_requests / elapsed if elapsed > 0 else 0\n",
    "        tput_min = tput_sec * 60\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S'):>8} | \"\n",
    "              f\"{total_requests:8d} | \"\n",
    "              f\"{avg_lat:8.3f} | \"\n",
    "              f\"{max_lat:8.3f} | \"\n",
    "              f\"{tput_sec:8.2f} | \"\n",
    "              f\"{tput_min:8.0f}\")\n",
    "\n",
    "async def main():\n",
    "    global start_time_global\n",
    "    start_time_global = datetime.now()\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        reporter = asyncio.create_task(progress_report())\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            tasks = [fetch_and_save(sess, day, pt, sp) for pt, sp in PUBLISH_MAP.items()]\n",
    "            await asyncio.gather(*tasks)\n",
    "            day += timedelta(days=1)\n",
    "        reporter.cancel()\n",
    "\n",
    "    elapsed_run = (datetime.now() - start_time_global).total_seconds()\n",
    "    avg_lat = mean(latencies) if latencies else 0\n",
    "    max_lat = max(latencies) if latencies else 0\n",
    "    tput_sec = total_requests / elapsed_run if elapsed_run > 0 else 0\n",
    "    tput_min = tput_sec * 60\n",
    "    print(\"\\nFinal Summary\")\n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total reqs:':<15}{total_requests}\")\n",
    "    print(f\"{'Avg latency:':<15}{avg_lat:.3f} s\")\n",
    "    print(f\"{'Max latency:':<15}{max_lat:.3f} s\")\n",
    "    print(f\"{'Thr/sec:':<15}{tput_sec:.2f} req/sec\")\n",
    "    print(f\"{'Thr/min:':<15}{tput_min:.0f} req/min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8ce31",
   "metadata": {},
   "source": [
    "## Evolution of the Wind Generation Forecast Over Time (WINDFOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "ENDPOINT = \"/forecast/generation/wind/evolution\"\n",
    "\n",
    "FINAL_DIR     = BASE_DIR / \"EVOLUTION_WINDFOR\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Rate limits\n",
    "PER_MINUTE   = AsyncLimiter(4920, 60)\n",
    "PER_SECOND   = AsyncLimiter(82, 1)\n",
    "TIMEOUT      = aiohttp.ClientTimeout(sock_connect=3, sock_read=5)\n",
    "CONCURRENCY  = 128\n",
    "\n",
    "async def fetch_and_save_evolution(sess: aiohttp.ClientSession, start_dt: datetime):\n",
    "    \"\"\"\n",
    "    Fetch the evolution series for a given forecast startTime,\n",
    "    keep the 8 latest publishes at least 1h before start_dt, and save to gzipped JSON.\n",
    "    \"\"\"\n",
    "    iso_start = start_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    stamp     = start_dt.strftime(\"%Y-%m-%d_%H%M\")\n",
    "    dest      = FINAL_DIR / f\"{stamp}.json.gz\"\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    params = {\"startTime\": iso_start, \"format\": \"json\"}\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        resp = await sess.get(f\"{BASE_URL}{ENDPOINT}\", params=params)\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "\n",
    "    # filter to publishes at least 1h before the startTime\n",
    "    cutoff = start_dt - timedelta(hours=1)\n",
    "    df = df[df[\"publishTime\"] <= cutoff]\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # take the 8 most recent publishes\n",
    "    df = df.sort_values(\"publishTime\", ascending=False).head(8).copy()\n",
    "    df[\"publishTime\"] = df[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    out = {\n",
    "        \"metadata\": payload.get(\"metadata\", {}),\n",
    "        \"data\": df.to_dict(orient=\"records\")\n",
    "    }\n",
    "\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        day = START_DATE\n",
    "        one_day = timedelta(days=1)\n",
    "        half_hour = timedelta(minutes=30)\n",
    "\n",
    "        while day <= END_DATE:\n",
    "            # start at 00:00 UTC of this day\n",
    "            current = datetime.combine(day, time(0, 0), tzinfo=timezone.utc)\n",
    "            end_of_day = current + one_day\n",
    "\n",
    "            # step in 30-minute increments\n",
    "            while current < end_of_day:\n",
    "                tasks.append(asyncio.create_task(fetch_and_save_evolution(sess, current)))\n",
    "                current += half_hour\n",
    "\n",
    "            day += one_day\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "        print(f\"✅ Completed all {len(tasks)} evolution fetches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5d72d",
   "metadata": {},
   "source": [
    "## Day-Ahead Demand Forecast History (NDF, TSDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7866a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "# Config\n",
    "ENDPOINT   = \"/forecast/demand/day-ahead/history\"\n",
    "\n",
    "FINAL_DIR = BASE_DIR / \"DEMAND_FORECASTS\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stats\n",
    "latencies = []\n",
    "total_requests = 0\n",
    "start_time_global = None\n",
    "\n",
    "def _round_up_to_next_30(dt: datetime) -> datetime:\n",
    "    dt0 = dt.replace(second=0, microsecond=0)\n",
    "    extra = dt0.minute % 30\n",
    "    if extra == 0 and dt.second == 0 and dt.microsecond == 0:\n",
    "        return dt0\n",
    "    return dt0 + timedelta(minutes=(30 - extra))\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, query_dt: datetime):\n",
    "    global total_requests\n",
    "    publish_str = query_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    rounded = _round_up_to_next_30(query_dt)\n",
    "    fname = f\"{rounded.date().isoformat()}_{rounded.strftime('%H%M')}.json.gz\"\n",
    "    dest = FINAL_DIR / fname\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        start_time = time.perf_counter()\n",
    "        resp = await sess.get(f\"{BASE_URL}{ENDPOINT}\",\n",
    "                              params={\"publishTime\": publish_str})\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        total_requests += 1\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"] = pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "\n",
    "    pub_dt = df[\"publishTime\"].iloc[0]\n",
    "    window_start = pub_dt + timedelta(minutes=30)\n",
    "    window_end = pub_dt + timedelta(hours=24)\n",
    "    df = df[(df[\"startTime\"] >= window_start) & (df[\"startTime\"] <= window_end)].copy()\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    df[\"publishTime\"] = df[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    df[\"startTime\"] = df[\"startTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    out = {\"metadata\": payload.get(\"metadata\", {}), \"data\": df.to_dict(orient=\"records\")}\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def progress_report():\n",
    "    print(f\"{'Time':>8} | {'Reqs':>8} | {'Avg Lat':>8} | {'Max Lat':>8} | {'Thr/sec':>8} | {'Thr/min':>8}\")\n",
    "    print(\"-\" * 62)\n",
    "    while True:\n",
    "        await asyncio.sleep(15)\n",
    "        avg_lat = mean(latencies) if latencies else 0\n",
    "        max_lat = max(latencies) if latencies else 0\n",
    "        elapsed = (datetime.now() - start_time_global).total_seconds()\n",
    "        tput_sec = total_requests / elapsed if elapsed > 0 else 0\n",
    "        tput_min = tput_sec * 60\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S'):>8} | \"\n",
    "              f\"{total_requests:8d} | \"\n",
    "              f\"{avg_lat:8.3f} | \"\n",
    "              f\"{max_lat:8.3f} | \"\n",
    "              f\"{tput_sec:8.2f} | \"\n",
    "              f\"{tput_min:8.0f}\")\n",
    "\n",
    "async def main():\n",
    "    global start_time_global\n",
    "    start_time_global = datetime.now()\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        reporter = asyncio.create_task(progress_report())\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            cursor = datetime.combine(day, datetime.min.time(), tzinfo=timezone.utc)\n",
    "            end_dt = cursor + timedelta(days=1)\n",
    "            tasks = []\n",
    "            while cursor < end_dt:\n",
    "                tasks.append(fetch_and_save(sess, cursor))\n",
    "                cursor += timedelta(minutes=30)\n",
    "            await asyncio.gather(*tasks)\n",
    "            day += timedelta(days=1)\n",
    "        reporter.cancel()\n",
    "\n",
    "    elapsed_run = (datetime.now() - start_time_global).total_seconds()\n",
    "    avg_lat = mean(latencies) if latencies else 0\n",
    "    max_lat = max(latencies) if latencies else 0\n",
    "    tput_sec = total_requests / elapsed_run if elapsed_run > 0 else 0\n",
    "    tput_min = tput_sec * 60\n",
    "    print(\"\\nFinal Summary\")\n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total reqs:':<15}{total_requests}\")\n",
    "    print(f\"{'Avg latency:':<15}{avg_lat:.3f} s\")\n",
    "    print(f\"{'Max latency:':<15}{max_lat:.3f} s\")\n",
    "    print(f\"{'Thr/sec:':<15}{tput_sec:.2f} req/sec\")\n",
    "    print(f\"{'Thr/min:':<15}{tput_min:.0f} req/min\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d90f7",
   "metadata": {},
   "source": [
    "## Market-Wide Bid-Offer Acceptances (BOALF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "# Config\n",
    "ENDPOINT     = \"/balancing/acceptances/all\"\n",
    "\n",
    "FINAL_DIR = BASE_DIR / \"BIDOFFER_ACCEPTANCES\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stats\n",
    "latencies = []\n",
    "total_requests = 0\n",
    "start_time_global = None\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, query_dt: date, query_sp: int):\n",
    "    global total_requests\n",
    "    settlement_date = query_dt.strftime(\"%Y-%m-%d\")\n",
    "    settlement_period = str(query_sp)\n",
    "    fname = f\"{settlement_date}_{settlement_period}.json.gz\"\n",
    "    dest = FINAL_DIR / fname\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        start_time = time.perf_counter()\n",
    "        resp = await sess.get(f\"{BASE_URL}{ENDPOINT}\",\n",
    "                              params={\"settlementDate\": settlement_date, \"settlementPeriod\": settlement_period})\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        total_requests += 1\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[df[\"settlementPeriodFrom\"] == query_sp]\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    out = {\"metadata\": payload.get(\"metadata\", {}), \"data\": df.to_dict(orient=\"records\")}\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def progress_report():\n",
    "    # Table header\n",
    "    print(f\"{'Time':>8} | {'Reqs':>8} | {'Avg Lat':>8} | {'Max Lat':>8} | {'Thr/sec':>8} | {'Thr/min':>8}\")\n",
    "    print(\"-\" * 62)\n",
    "    while True:\n",
    "        await asyncio.sleep(15)\n",
    "        avg_lat = mean(latencies) if latencies else 0\n",
    "        max_lat = max(latencies) if latencies else 0\n",
    "        elapsed = (datetime.now() - start_time_global).total_seconds()\n",
    "        tput_sec = total_requests / elapsed if elapsed > 0 else 0\n",
    "        tput_min = tput_sec * 60\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S'):>8} | \"\n",
    "              f\"{total_requests:8d} | \"\n",
    "              f\"{avg_lat:8.3f} | \"\n",
    "              f\"{max_lat:8.3f} | \"\n",
    "              f\"{tput_sec:8.2f} | \"\n",
    "              f\"{tput_min:8.0f}\")\n",
    "\n",
    "async def main():\n",
    "    global start_time_global\n",
    "    start_time_global = datetime.now()\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        reporter = asyncio.create_task(progress_report())\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            tasks = [fetch_and_save(sess, day, sp) for sp in range(1, 51)]\n",
    "            await asyncio.gather(*tasks)\n",
    "            day += timedelta(days=1)\n",
    "        reporter.cancel()\n",
    "\n",
    "    elapsed_run = (datetime.now() - start_time_global).total_seconds()\n",
    "    avg_lat = mean(latencies) if latencies else 0\n",
    "    max_lat = max(latencies) if latencies else 0\n",
    "    tput_sec = total_requests / elapsed_run if elapsed_run > 0 else 0\n",
    "    tput_min = tput_sec * 60\n",
    "    print(\"\\nFinal Summary\")\n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total reqs:':<15}{total_requests}\")\n",
    "    print(f\"{'Avg latency:':<15}{avg_lat:.3f} s\")\n",
    "    print(f\"{'Max latency:':<15}{max_lat:.3f} s\")\n",
    "    print(f\"{'Thr/sec:':<15}{tput_sec:.2f} req/sec\")\n",
    "    print(f\"{'Thr/min:':<15}{tput_min:.0f} req/min\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a64648",
   "metadata": {},
   "source": [
    "## Acceptances by Settlement Period (ISPSTACK, BOALF, BOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb189ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "FINAL_DIR = BASE_DIR / \"BIDOFFER_PRICES\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, query_dt: date, query_sp: int):\n",
    "    settlement_date = query_dt.strftime(\"%Y-%m-%d\")\n",
    "    settlement_period = str(query_sp)\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        url = f\"{BASE_URL}/balancing/settlement/acceptances/all/{settlement_date}/{settlement_period}\"\n",
    "        resp = await sess.get(url)\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # keep exact SP\n",
    "    if \"settlementPeriodFrom\" in df.columns:\n",
    "        df = df[df[\"settlementPeriodFrom\"] == query_sp]\n",
    "        if df.empty:\n",
    "            return\n",
    "\n",
    "    # determine filename by settlement period\n",
    "    fname = f\"{settlement_date}_{settlement_period}.json.gz\"\n",
    "    dest = FINAL_DIR / fname\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    out = {\n",
    "        \"metadata\": payload.get(\"metadata\", {}),\n",
    "        \"data\": df.to_dict(orient=\"records\")\n",
    "    }\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            for sp in range(1, 51):  # SP 1–50\n",
    "                tasks.append(fetch_and_save(sess, day, sp))\n",
    "            day += timedelta(days=1)\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "        print(f\"✅ Completed all {len(tasks)} fetches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70841f58",
   "metadata": {},
   "source": [
    "## Settlement Bid-Offer Stacks by Settlement Period (ISPSTACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "# Configuration\n",
    "FINAL_DIR    = BASE_DIR / \"ISPSTACK\"\n",
    "FINAL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Limits\n",
    "PER_MINUTE   = AsyncLimiter(4920, 60)\n",
    "PER_SECOND   = AsyncLimiter(82, 1)\n",
    "TIMEOUT      = aiohttp.ClientTimeout(sock_connect=3, sock_read=5)\n",
    "CONCURRENCY  = 128\n",
    "\n",
    "# Stats\n",
    "latencies = []\n",
    "total_requests = 0\n",
    "start_time_global = None\n",
    "\n",
    "async def fetch_stack(sess: aiohttp.ClientSession, bid_offer_type: str, day: date, sp: int):\n",
    "    global total_requests\n",
    "    settlement_date = day.strftime(\"%Y-%m-%d\")\n",
    "    settlement_period = str(sp)\n",
    "    fname = f\"{bid_offer_type}_{settlement_date}_{settlement_period}.json.gz\"\n",
    "    dest = FINAL_DIR / fname\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        start_time = time.perf_counter()\n",
    "        url = f\"{BASE_URL}/balancing/settlement/stack/all/{bid_offer_type}/{settlement_date}/{settlement_period}\"\n",
    "        async with sess.get(url) as resp:\n",
    "            resp.raise_for_status()\n",
    "            payload = await resp.json()\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        latencies.append(elapsed)\n",
    "        total_requests += 1\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "    out = {\"metadata\": payload.get(\"metadata\", {}), \"data\": data}\n",
    "    with gzip.open(dest, \"wt\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def fetch_both(sess: aiohttp.ClientSession, day: date, sp: int):\n",
    "    await asyncio.gather(\n",
    "        fetch_stack(sess, \"bid\", day, sp),\n",
    "        fetch_stack(sess, \"offer\", day, sp)\n",
    "    )\n",
    "\n",
    "async def progress_report():\n",
    "    # Print header once\n",
    "    print(f\"{'Time':>8} | {'Reqs':>8} | {'Avg Lat':>8} | {'Max Lat':>8} | {'Thr/sec':>8} | {'Thr/min':>8}\")\n",
    "    print(\"-\" * 62)\n",
    "    while True:\n",
    "        await asyncio.sleep(15)\n",
    "        avg_lat = mean(latencies) if latencies else 0\n",
    "        max_lat = max(latencies) if latencies else 0\n",
    "        elapsed = (datetime.now() - start_time_global).total_seconds()\n",
    "        tput_sec = total_requests / elapsed if elapsed > 0 else 0\n",
    "        tput_min = tput_sec * 60\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S'):>8} | \"\n",
    "              f\"{total_requests:8d} | \"\n",
    "              f\"{avg_lat:8.3f} | \"\n",
    "              f\"{max_lat:8.3f} | \"\n",
    "              f\"{tput_sec:8.2f} | \"\n",
    "              f\"{tput_min:8.0f}\")\n",
    "\n",
    "async def main():\n",
    "    global start_time_global\n",
    "    start_time_global = datetime.now()\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        reporter = asyncio.create_task(progress_report())\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            tasks = [fetch_both(sess, day, sp) for sp in range(1, 51)]\n",
    "            await asyncio.gather(*tasks)\n",
    "            day += timedelta(days=1)\n",
    "        reporter.cancel()\n",
    "\n",
    "    elapsed_run = (datetime.now() - start_time_global).total_seconds()\n",
    "    avg_lat = mean(latencies) if latencies else 0\n",
    "    max_lat = max(latencies) if latencies else 0\n",
    "    tput_sec = total_requests / elapsed_run if elapsed_run > 0 else 0\n",
    "    tput_min = tput_sec * 60\n",
    "\n",
    "    print(\"\\nFinal Summary\")\n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total reqs:':<15}{total_requests}\")\n",
    "    print(f\"{'Avg latency:':<15}{avg_lat:.3f} s\")\n",
    "    print(f\"{'Max latency:':<15}{max_lat:.3f} s\")\n",
    "    print(f\"{'Thr/sec:':<15}{tput_sec:.2f} req/sec\")\n",
    "    print(f\"{'Thr/min:':<15}{tput_min:.0f} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2f682",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73daedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Time |     Reqs |  Avg Lat |  Max Lat |  Thr/sec |  Thr/min\n",
      "--------------------------------------------------------------\n",
      "04:28:36 |     1292 |    0.031 |    0.149 |    86.12 |     5167\n",
      "\n",
      "Final Summary\n",
      "--------------------------------------------------------------\n",
      "Total reqs:    1652\n",
      "Avg latency:   0.029 s\n",
      "Max latency:   0.149 s\n",
      "Thr/sec:       85.36 req/sec\n",
      "Thr/min:       5122 req/min\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
