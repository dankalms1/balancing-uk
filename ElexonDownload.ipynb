{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53b62a-3ce0-47e8-8b82-83e65cbf5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================  High-throughput Elexon bulk crawler  ==================\n",
    "import asyncio, aiohttp, gzip, json, datetime as dt\n",
    "from pathlib import Path\n",
    "import nest_asyncio, aiolimiter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc363-45af-4771-9de8-4ef9bea87cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://data.elexon.co.uk/bmrs/api/v1\"\n",
    "\n",
    "ENDPOINTS = {\n",
    "\n",
    "    # ------------------ Generation ----------------------------------------\n",
    "    \"GEN_PER_TYPE\":\n",
    "        \"/datasets/AGPT?\"\n",
    "        \"publishDateTimeFrom={from_ts}&publishDateTimeTo={to_ts}\",\n",
    "\n",
    "    \"INTER\":\n",
    "        \"/generation/outturn/interconnectors?\"\n",
    "        \"settlementDateFrom={date}&settlementDateTo={date}\",\n",
    "\n",
    "    \"DAYAHEAD_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?\"\n",
    "        \"from={from_ts}&to={to_ts}&processType=day%20ahead\",\n",
    "\n",
    "    \"INTRADAYPROCESS_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?\"\n",
    "        \"from={from_ts}&to={to_ts}&processType=intraday%20process\",\n",
    "\n",
    "    \"INTRADAYTOTAL_GEN_WIND_SOLAR\":\n",
    "        \"/forecast/generation/wind-and-solar/day-ahead?\"\n",
    "        \"from={from_ts}&to={to_ts}&processType=intraday%20total\",\n",
    "\n",
    "    \"ACTUAL_GEN_WIND_SOLAR\":\n",
    "        \"/datasets/AGWS?publishDateTimeFrom={from_ts}&publishDateTimeTo={to_ts}\",\n",
    "\n",
    "    # ------------------ Demand -------------------------------------------\n",
    "    \"DAYAHEAD_DEMAND\":\n",
    "        \"/forecast/demand/day-ahead/history?\"\n",
    "        \"publishTime={date}\",\n",
    "\n",
    "    \"INDICATED_DAYAHEAD_DEMAND\":\n",
    "        \"/forecast/indicated/day-ahead/history?\"\n",
    "        \"publishTime={date}\",\n",
    "\n",
    "    \"ACTUAL_DEMAND\":\n",
    "        \"/demand/outturn?\"\n",
    "        \"settlementDateFrom={date}&settlementDateTo={date}\",\n",
    "\n",
    "    # ------------------ Balancing ----------------------------------------\n",
    "    \"SYSTEM_PRICES\":\n",
    "        \"/balancing/settlement/system-prices/{date}\",\n",
    "\n",
    "    \"BSAD\":\n",
    "        \"/datasets/netbsad?from={from_ts}&to={to_ts}\",\n",
    "\n",
    "    \"MID\":\n",
    "        \"/datasets/mid?from={from_ts}&to={to_ts}\",\n",
    "\n",
    "    \"NONBM\":\n",
    "        \"/datasets/NONBM?from={from_ts}&to={to_ts}\",\n",
    "\n",
    "    # ------------------ Transmission -------------------------------------\n",
    "    \"LOLPDRM\":\n",
    "        \"/forecast/system/loss-of-load?from={from_ts}&to={to_ts}\",\n",
    "}\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "START_DATE = dt.date(2017, 1, 1)\n",
    "END_DATE   = dt.date(2025, 7, 1)\n",
    "\n",
    "BASE_DIR = Path(\"bmrs_json_raw\")\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def _ph(day: dt.date):\n",
    "    \"\"\"Return placeholders dict for a given date.\"\"\"\n",
    "    iso = day.isoformat()\n",
    "    return {\n",
    "        \"date\"   : iso,\n",
    "        \"from_ts\": f\"{iso}T00:00:00Z\",\n",
    "        \"to_ts\"  : f\"{iso}T23:59:59Z\",\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "CONCURRENCY = 256                 # sockets\n",
    "PER_MINUTE  = aiolimiter.AsyncLimiter(4500, 60)   \n",
    "PER_SECOND  = aiolimiter.AsyncLimiter(75,   1)    \n",
    "TIMEOUT     = aiohttp.ClientTimeout(total=40)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "async def _fetch(sess: aiohttp.ClientSession, url: str, dest: Path, log: Path):\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    # acquire from **both** buckets\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        async with sess.get(url) as r:\n",
    "            if r.status == 404:\n",
    "                return\n",
    "            if r.status == 429:\n",
    "                # simple exponential back-off then retry once\n",
    "                await asyncio.sleep(5)\n",
    "                async with sess.get(url) as r2:\n",
    "                    if r2.status == 404 or r2.status == 429:\n",
    "                        return\n",
    "                    r2.raise_for_status()\n",
    "                    payload = await r2.json()\n",
    "            else:\n",
    "                r.raise_for_status()\n",
    "                payload = await r.json()\n",
    "\n",
    "    if isinstance(payload, dict) and payload.get(\"data\") == []:\n",
    "        log.write_text((log.read_text() + dest.stem + \"\\n\") if log.exists()\n",
    "                       else dest.stem + \"\\n\")\n",
    "        return\n",
    "\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(payload, fh)\n",
    "\n",
    "\n",
    "async def _all_tasks():\n",
    "    day = START_DATE\n",
    "    while day <= END_DATE:\n",
    "        ph = _ph(day)\n",
    "        for code, tpl in ENDPOINTS.items():\n",
    "            sub   = BASE_DIR / code\n",
    "            sub.mkdir(exist_ok=True)\n",
    "            url   = f\"{BASE_URL}{tpl.format(**ph)}\"\n",
    "            dest  = sub / f\"{ph['date']}.json.gz\"\n",
    "            log   = sub / \"empty.log\"\n",
    "            yield url, dest, log\n",
    "        day += dt.timedelta(days=1)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector,\n",
    "                                     timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        async for url, dest, log in _all_tasks():\n",
    "            tasks.append(asyncio.create_task(_fetch(sess, url, dest, log)))\n",
    "        CHUNK = 10_000\n",
    "        for i in range(0, len(tasks), CHUNK):\n",
    "            await asyncio.gather(*tasks[i:i+CHUNK])\n",
    "            print(f\"✓ {min(i+CHUNK, len(tasks)):,} / {len(tasks):,} done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268857bb-1a4a-4d50-8ba9-6065ba07fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 10,000 / 43,456 done\n",
      "✓ 20,000 / 43,456 done\n",
      "✓ 30,000 / 43,456 done\n",
      "✓ 40,000 / 43,456 done\n",
      "✓ 43,456 / 43,456 done\n",
      "✅  completed\n"
     ]
    }
   ],
   "source": [
    "# This cell actually runs the download\n",
    "await main()\n",
    "print(\"✅  completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BASE_URL       = \"https://data.elexon.co.uk/bmrs/api/v1\"\n",
    "HISTORY_EP     = \"/forecast/generation/wind/history\"\n",
    "\n",
    "# Explicit mapping of local clock to settlementPeriod\n",
    "PUBLISH_MAP    = {\n",
    "    \"03:30\": 6,\n",
    "    \"05:30\": 10,\n",
    "    \"08:30\": 16,\n",
    "    \"10:30\": 20,\n",
    "    \"12:30\": 24,\n",
    "    \"16:30\": 32,\n",
    "    \"19:30\": 38,\n",
    "    \"23:30\": 46,\n",
    "}\n",
    "START_DATE     = datetime(2016,12,31).date()\n",
    "END_DATE       = datetime(2025,6,30).date()\n",
    "BASE_DIR       = Path(\"bmrs_json_raw\")\n",
    "DETAILED_DIR   = BASE_DIR / \"DETAILED_WINDFOR\"\n",
    "DETAILED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Rate limits\n",
    "PER_MINUTE     = AsyncLimiter(4500, 60)\n",
    "PER_SECOND     = AsyncLimiter(75,   1)\n",
    "TIMEOUT        = aiohttp.ClientTimeout(total=40)\n",
    "CONCURRENCY    = 256\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, date: datetime.date, pt: str, sp: int):\n",
    "    publish_str = f\"{date.isoformat()}T{pt}Z\"\n",
    "    dest = DETAILED_DIR / f\"{date}_{sp}.json.gz\"\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        resp = await sess.get(f\"{BASE_URL}{HISTORY_EP}\", params={\"publishTime\": publish_str})\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"]   = pd.to_datetime(df[\"startTime\"],   utc=True)\n",
    "\n",
    "    pub_dt = df[\"publishTime\"].iloc[0]\n",
    "    window_start = pub_dt\n",
    "    window_end   = pub_dt + timedelta(hours=24)\n",
    "    filtered = df[(df[\"startTime\"] >= window_start) & (df[\"startTime\"] <= window_end)].copy()\n",
    "\n",
    "    # format for JSON\n",
    "    filtered[\"publishTime\"] = filtered[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    filtered[\"startTime\"]   = filtered[\"startTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    out = {\"metadata\": payload.get(\"metadata\", {}), \"data\": filtered.to_dict(orient=\"records\")}\n",
    "\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            for pt, sp in PUBLISH_MAP.items():\n",
    "                tasks.append(asyncio.create_task(fetch_and_save(sess, day, pt, sp)))\n",
    "            day += timedelta(days=1)\n",
    "        await asyncio.gather(*tasks)\n",
    "        print(f\"✅ Completed all {len(tasks)} fetches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a2ae1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed all 24832 fetches\n",
      "✅  completed\n"
     ]
    }
   ],
   "source": [
    "await main()\n",
    "print(\"✅  completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b71ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BASE_URL     = \"https://data.elexon.co.uk/bmrs/api/v1\"\n",
    "EVOLUTION_EP = \"/forecast/generation/wind/evolution\"\n",
    "\n",
    "START_DATE   = date(2017, 1, 1)\n",
    "END_DATE     = date(2025, 6, 30)\n",
    "\n",
    "BASE_DIR     = Path(\"bmrs_json_raw\")\n",
    "EVOL_DIR     = BASE_DIR / \"EVOLUTION_WINDFOR\"\n",
    "EVOL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Rate limits\n",
    "PER_MINUTE   = AsyncLimiter(4500, 60)\n",
    "PER_SECOND   = AsyncLimiter(75,   1)\n",
    "TIMEOUT      = aiohttp.ClientTimeout(total=40)\n",
    "CONCURRENCY  = 256\n",
    "\n",
    "async def fetch_and_save_evolution(sess: aiohttp.ClientSession, start_dt: datetime):\n",
    "    \"\"\"\n",
    "    Fetch the evolution series for a given forecast startTime,\n",
    "    keep the 8 latest publishes at least 1h before start_dt, and save to gzipped JSON.\n",
    "    \"\"\"\n",
    "    iso_start = start_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    stamp     = start_dt.strftime(\"%Y-%m-%d_%H%M\")\n",
    "    dest      = EVOL_DIR / f\"{stamp}.json.gz\"\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    params = {\"startTime\": iso_start, \"format\": \"json\"}\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        resp = await sess.get(f\"{BASE_URL}{EVOLUTION_EP}\", params=params)\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "\n",
    "    # filter to publishes at least 1h before the startTime\n",
    "    cutoff = start_dt - timedelta(hours=1)\n",
    "    df = df[df[\"publishTime\"] <= cutoff]\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # take the 8 most recent publishes\n",
    "    df = df.sort_values(\"publishTime\", ascending=False).head(8).copy()\n",
    "    df[\"publishTime\"] = df[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    out = {\n",
    "        \"metadata\": payload.get(\"metadata\", {}),\n",
    "        \"data\": df.to_dict(orient=\"records\")\n",
    "    }\n",
    "\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        day = START_DATE\n",
    "        one_day = timedelta(days=1)\n",
    "        half_hour = timedelta(minutes=30)\n",
    "\n",
    "        while day <= END_DATE:\n",
    "            # start at 00:00 UTC of this day\n",
    "            current = datetime.combine(day, time(0, 0), tzinfo=timezone.utc)\n",
    "            end_of_day = current + one_day\n",
    "\n",
    "            # step in 30-minute increments\n",
    "            while current < end_of_day:\n",
    "                tasks.append(asyncio.create_task(fetch_and_save_evolution(sess, current)))\n",
    "                current += half_hour\n",
    "\n",
    "            day += one_day\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "        print(f\"✅ Completed all {len(tasks)} evolution fetches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47051a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed all 148944 evolution fetches\n",
      "✅  completed\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7866a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from aiolimiter import AsyncLimiter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BASE_URL     = \"https://data.elexon.co.uk/bmrs/api/v1\"\n",
    "HISTORY_EP   = \"/forecast/demand/day-ahead/history\"\n",
    "START_DATE   = datetime(2021,7,1).date()\n",
    "END_DATE     = datetime(2025,6,30).date()\n",
    "BASE_DIR     = Path(\"bmrs_json_raw\")\n",
    "DETAILED_DIR = BASE_DIR / \"DEMAND_FORECASTS\"\n",
    "DETAILED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Rate limiting\n",
    "PER_MINUTE   = AsyncLimiter(4500, 60)\n",
    "PER_SECOND   = AsyncLimiter(75,   1)\n",
    "TIMEOUT      = aiohttp.ClientTimeout(total=40)\n",
    "CONCURRENCY  = 64\n",
    "\n",
    "def _round_up_to_next_30(dt: datetime) -> datetime:\n",
    "    dt0    = dt.replace(second=0, microsecond=0)\n",
    "    extra  = dt0.minute % 30\n",
    "    if extra == 0 and dt.second == 0 and dt.microsecond == 0:\n",
    "        return dt0\n",
    "    return dt0 + timedelta(minutes=(30 - extra))\n",
    "\n",
    "async def fetch_and_save(sess: aiohttp.ClientSession, query_dt: datetime):\n",
    "    publish_str = query_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    async with PER_SECOND, PER_MINUTE:\n",
    "        resp = await sess.get(f\"{BASE_URL}{HISTORY_EP}\",\n",
    "                              params={\"publishTime\": publish_str})\n",
    "        resp.raise_for_status()\n",
    "        payload = await resp.json()\n",
    "\n",
    "    data = payload.get(\"data\", [])\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"]   = pd.to_datetime(df[\"startTime\"],   utc=True)\n",
    "\n",
    "    # keep only the next 24 hours\n",
    "    pub_dt = df[\"publishTime\"].iloc[0]\n",
    "    window_start = pub_dt + timedelta(minutes=30)\n",
    "    window_end   = pub_dt + timedelta(hours=24)\n",
    "    df = df[(df[\"startTime\"] >= window_start)\n",
    "           & (df[\"startTime\"] <= window_end)].copy()\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # re-format timestamps for JSON\n",
    "    df[\"publishTime\"] = df[\"publishTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    df[\"startTime\"]   = df[\"startTime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # determine filename by rounding *actual* publishTime up\n",
    "    rounded = _round_up_to_next_30(pub_dt)\n",
    "    fname   = f\"{rounded.date().isoformat()}_{rounded.strftime('%H%M')}.json.gz\"\n",
    "    dest    = DETAILED_DIR / fname\n",
    "    if dest.exists():\n",
    "        return\n",
    "\n",
    "    out = {\n",
    "        \"metadata\": payload.get(\"metadata\", {}),\n",
    "        \"data\":     df.to_dict(orient=\"records\")\n",
    "    }\n",
    "    with gzip.open(dest, \"wt\") as fh:\n",
    "        json.dump(out, fh)\n",
    "\n",
    "async def main():\n",
    "    connector = aiohttp.TCPConnector(limit=CONCURRENCY, ttl_dns_cache=300)\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as sess:\n",
    "        tasks = []\n",
    "        day = START_DATE\n",
    "        while day <= END_DATE:\n",
    "            # start at midnight UTC on 'day'\n",
    "            cursor = datetime.combine(day, datetime.min.time(), tzinfo=timezone.utc)\n",
    "            end_dt = cursor + timedelta(days=1)\n",
    "            # step through in 30-minute increments\n",
    "            while cursor < end_dt:\n",
    "                tasks.append(fetch_and_save(sess, cursor))\n",
    "                cursor += timedelta(minutes=30)\n",
    "            day += timedelta(days=1)\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "        print(f\"✅ Completed all {len(tasks)} fetches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8706d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed all 70128 fetches\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
