{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4333e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import gc\n",
    "from itertools import chain\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edf6a8",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING\n",
    "\n",
    "## BUILDING THE FINAL .csv FROM .json\n",
    "\n",
    "* **Imports & configuration** – import `json`, `gzip`, `pathlib`, `itertools`, `pandas`, `tqdm`, `numpy`; set `RAW_DIR`, `CSV_DIR`, and the global `start_date`/`end_date`.\n",
    "\n",
    "* **Raw JSON loading** – `_load_json_z` reads a single `.json.gz`; `_folder_to_df` concatenates all files in a folder into one DataFrame.\n",
    "\n",
    "* **Timestamp construction** – `build_start_time` returns a UTC‐like `startTime` column, either from an existing field or by combining `settlementDate`+`settlementPeriod`.\n",
    "\n",
    "* **Type coercion** – `_coerce_type` downcasts non-key columns to `float32` and normalises the date/time columns to naïve `datetime64[ns]`.\n",
    "\n",
    "* **Small-gap interpolation** – `fill_small_gaps` linearly fills runs of up to two missing half-hours in numeric columns, leaving longer gaps untouched.\n",
    "\n",
    "* **UK half-hour calendar** – `build_uk_halfhour_calendar` generates a full DST-aware sequence of half-hour intervals between any two dates.\n",
    "\n",
    "* **Padding missing intervals** – `_pad_missing` merges data onto the full calendar (trimming to the global date range) so every expected interval appears.\n",
    "\n",
    "* **Finalising pipeline** – `_finish` selects the requested columns, coerces types, pads missing, drops duplicates, interpolates small gaps, then sorts and resets the index.\n",
    "\n",
    "* **Dataset builders** – each `b_<dataset>` function (e.g. `b_actual_demand`, `b_gen_per_type`, `b_system_prices`, etc.) computes `startTime`, selects its own `want` column list, and hands off to `_finish` to produce the final CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ac9b343-ce17-4d94-bd14-1327f29d3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "RAW_DIR     = Path(\"bmrs_json_raw\")\n",
    "CSV_DIR     = Path(\"bmrs_csv_raw\")\n",
    "LOG_DIR     = Path(\"logs\")\n",
    "\n",
    "CSV_DIR.mkdir(exist_ok=True)\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Date range\n",
    "start_date = \"2017-01-01\"\n",
    "end_date   = \"2025-06-30\"\n",
    "\n",
    "\n",
    "\n",
    "def _load_json_z(path: Path) -> list[dict]:\n",
    "    with gzip.open(path, \"rt\") as fh:\n",
    "        return json.load(fh)[\"data\"]\n",
    "\n",
    "\n",
    "def _folder_to_df(folder: Path) -> pd.DataFrame:\n",
    "    files = sorted(folder.glob(\"*.json.gz\"))\n",
    "    rows  = chain.from_iterable((_load_json_z(f) for f in files))\n",
    "    return pd.DataFrame.from_records(rows)\n",
    "\n",
    "\n",
    "def build_start_time(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return UTC-like timestamp (start of settlement period).\"\"\"\n",
    "    if \"startTime\" in df.columns:\n",
    "        return pd.to_datetime(df[\"startTime\"], errors=\"coerce\")\n",
    "    # otherwise compose from date + SP (SP1 = 00:00 UTC *winter*)\n",
    "    base = pd.to_datetime(df[\"settlementDate\"])\n",
    "    off  = pd.to_timedelta(df[\"settlementPeriod\"].astype(int).sub(1) * 30,\n",
    "                           unit=\"m\")\n",
    "    return base + off\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── finisher ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "KEY_COLS = {\"startTime\", \"settlementDate\", \"settlementPeriod\"}\n",
    "\n",
    "def _coerce_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • convert every non-key column to float32\n",
    "    • normalise datetime columns\n",
    "    \"\"\"\n",
    "    # 1) numeric columns  → float32\n",
    "    num_cols = [c for c in df.columns if c not in KEY_COLS]\n",
    "    df[num_cols] = df[num_cols].apply(\n",
    "        pd.to_numeric, errors=\"coerce\", downcast=\"float\"\n",
    "    )\n",
    "\n",
    "    # 2) settlementDate  → 00:00 of that day, no timezone\n",
    "    df[\"settlementDate\"] = (\n",
    "        pd.to_datetime(df[\"settlementDate\"], utc=True)   # ensure tz-aware\n",
    "          .dt.normalize()                                # strip hh:mm:ss\n",
    "          .dt.tz_localize(None)                          # drop timezone\n",
    "    )\n",
    "\n",
    "    # 3) startTime  → no timezone (but keep hh:mm)\n",
    "    if \"startTime\" in df.columns:\n",
    "        df[\"startTime\"] = (\n",
    "            pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "              .dt.tz_localize(None)\n",
    "        )\n",
    "\n",
    "    # settlementPeriod stays int32\n",
    "    if \"settlementPeriod\" in df.columns:\n",
    "        df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_uk_halfhour_calendar(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Build UK half-hour calendar with correct DST handling:\n",
    "      • Spring-forward days: 46 periods (including the skipped 01:00/01:30)\n",
    "      • Normal days: 48 periods 00:00-23:30\n",
    "      • BST days: 48 periods 23:00(prev day)-22:30\n",
    "      • Autumn-back days: 50 periods 23:00(prev day)-22:30\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_date(x):\n",
    "        if isinstance(x, str):\n",
    "            \n",
    "            if x.count(\"-\") == 2 and x[4] == \"-\": # ISO format\n",
    "                return datetime.date.fromisoformat(x)\n",
    "            return datetime.datetime.strptime(x, \"%d/%m/%Y\").date() # UK format\n",
    "        if isinstance(x, pd.Timestamp):\n",
    "            return x.date()\n",
    "        return x\n",
    "\n",
    "    start = _to_date(start_date)\n",
    "    end   = _to_date(end_date)\n",
    "\n",
    "    london = ZoneInfo(\"Europe/London\")\n",
    "    utc    = ZoneInfo(\"UTC\")\n",
    "    rows   = []\n",
    "\n",
    "    for single in pd.date_range(start, end, freq=\"D\"):\n",
    "        D   = single.date()\n",
    "        # local midnights in London\n",
    "        dt0 = datetime.datetime(D.year, D.month, D.day, tzinfo=london)\n",
    "        dt1 = dt0 + datetime.timedelta(days=1)\n",
    "\n",
    "        # number of half-hours that actually occur\n",
    "        total_secs = (dt1.astimezone(utc) - dt0.astimezone(utc)).total_seconds()\n",
    "        n_periods = int(total_secs // 1800)\n",
    "\n",
    "        # align to UTC-naive base for SP1\n",
    "        offset_h = dt0.utcoffset().total_seconds() / 3600\n",
    "        if offset_h > 0:\n",
    "            base = datetime.datetime(D.year, D.month, D.day) - datetime.timedelta(hours=int(offset_h))\n",
    "        else:\n",
    "            base = datetime.datetime(D.year, D.month, D.day)\n",
    "\n",
    "        for i in range(n_periods):\n",
    "            rows.append({\n",
    "                \"startTime\":        base + datetime.timedelta(minutes=30 * i),\n",
    "                \"settlementDate\":   D,\n",
    "                \"settlementPeriod\": i + 1\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ─── coerce to pandas time types ───\n",
    "    df[\"startTime\"]      = pd.to_datetime(df[\"startTime\"])\n",
    "    df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "    # ───────────────────────────────────\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _pad_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each settlementDate in df, build exactly the UK\n",
    "    SP‐calendar via build_uk_halfhour_calendar(min,max), then\n",
    "    left‐merge your data on (Date,Period,startTime).\n",
    "    Also trims the input df to only include rows between start_date and end_date (inclusive).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "    df=df.drop_duplicates(subset=[\"startTime\"])\n",
    "    # ensure proper types\n",
    "    df[\"settlementDate\"]   = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(int)\n",
    "    df[\"startTime\"]        = pd.to_datetime(df[\"startTime\"])\n",
    "\n",
    "    lo = start_date\n",
    "    hi = end_date\n",
    "\n",
    "    # trim input df to only include rows between start_date and end_date (inclusive)\n",
    "    mask = (\n",
    "        (df[\"settlementDate\"] >= pd.to_datetime(start_date)) &\n",
    "        (df[\"settlementDate\"] <= pd.to_datetime(end_date))\n",
    "    )\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    # build the master calendar\n",
    "    cal = build_uk_halfhour_calendar(lo, hi)\n",
    "    cal[\"settlementDate\"]   = pd.to_datetime(cal[\"settlementDate\"])\n",
    "    cal[\"settlementPeriod\"] = cal[\"settlementPeriod\"].astype(int)\n",
    "    cal[\"startTime\"]        = pd.to_datetime(cal[\"startTime\"], dayfirst=True)\n",
    "\n",
    "    # left‐join your actual data onto the calendar\n",
    "    out = (\n",
    "        cal\n",
    "        .merge(df,\n",
    "               on=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "               how=\"left\",\n",
    "               sort=False)\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _finish(out: pd.DataFrame, want: list[str]) -> pd.DataFrame:\n",
    "    # 1) keep only requested columns\n",
    "    out = out[want]\n",
    "    # 2) downcast\n",
    "    out = _coerce_type(out)\n",
    "    # 3) pad missing with DST‐aware UK calendar\n",
    "    out = _pad_missing(out)\n",
    "    # 4) drop duplicate rows\n",
    "    out = out.drop_duplicates()\n",
    "    \n",
    "    return out.sort_values(\"startTime\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── builders ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def b_actual_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"initialDemandOutturn\",\n",
    "            \"initialTransmissionSystemDemandOutturn\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def _pivot_wind_solar(df, value_name):\n",
    "    map_ = {\n",
    "        \"Wind Onshore\" : \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\"        : \"solarGeneration\",\n",
    "    }\n",
    "    df  = df.replace({\"psrType\": map_})\n",
    "    out = (df.pivot_table(index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "                          columns=\"psrType\",\n",
    "                          values=value_name, aggfunc=\"first\")\n",
    "             .reset_index())\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for c in map_.values():\n",
    "        if c not in out.columns:\n",
    "            out[c] = pd.NA\n",
    "    return out\n",
    "\n",
    "def b_actual_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar(df.rename(columns={\"quantity\":\"gen\"}), \"gen\")\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"windOnshoreGeneration\",\"windOffshoreGeneration\",\n",
    "            \"solarGeneration\"]\n",
    "    out = tidy[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _pivot_wind_solar_forecast(df, process_type, value_name):\n",
    "    map_ = {\n",
    "        \"Wind Onshore\":  \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\":         \"solarGeneration\",\n",
    "    }\n",
    "    df = (\n",
    "        df.loc[df[\"processType\"] == process_type]\n",
    "          .rename(columns={\"quantity\": value_name})\n",
    "          .replace({\"psrType\": map_})\n",
    "    )\n",
    "    out = (\n",
    "        df.pivot_table(\n",
    "            index=[\"settlementDate\", \"settlementPeriod\", \"startTime\", \"publishTime\"],\n",
    "            columns=\"psrType\",\n",
    "            values=value_name,\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for col in map_.values():\n",
    "        if col not in out.columns:\n",
    "            out[col] = pd.NA\n",
    "    return out\n",
    "\n",
    "def b_dayahead_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Day ahead\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "def b_intradayprocess_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Intraday process\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "def b_intradaytotal_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Intraday total\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _folder_to_df_detailed(folder: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Like _folder_to_df, but also injects `publishSP` from each filename:\n",
    "      bmrs_json_raw/DETAILED_WINDFOR/YYYY-MM-DD_<SP>.json.gz\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for f in sorted(folder.glob(\"*.json.gz\")):\n",
    "        name = f.name  # e.g. \"2017-01-01_10.json.gz\"\n",
    "        # strip off the \".json.gz\" bit, then split on \"_\"\n",
    "        base = name[:-len(\".json.gz\")]            # → \"2017-01-01_10\"\n",
    "        sp   = int(base.split(\"_\", 1)[1])         # → 10\n",
    "        # … load JSON …\n",
    "        payload = json.load(gzip.open(f, \"rt\"))\n",
    "        for rec in payload.get(\"data\", []):\n",
    "           rec[\"publishSP\"] = sp\n",
    "           rows.append(rec)\n",
    "    return pd.DataFrame.from_records(rows)\n",
    "\n",
    "def b_windfor_evolution(df):\n",
    "    \"\"\"\n",
    "    Intraday wind forecasts with 8 columns f_1…f_8,\n",
    "    where f_1 is the latest run ≥1h before delivery,\n",
    "    f_2 the next‐latest, …, f_8 the 8th‐latest.\n",
    "    \"\"\"\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\"] + [f\"f_{i}\" for i in range(1,9)]\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=want)\n",
    "\n",
    "    # 1) parse\n",
    "    df[\"publishTimeUTC\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"]      = pd.to_datetime(df[\"startTime\"],   utc=True)\n",
    "\n",
    "    # 2) gate‐closure: only keep runs published ≥1h before delivery\n",
    "    df = df[df[\"publishTimeUTC\"] <= df[\"startTime\"] - pd.Timedelta(hours=1)]\n",
    "\n",
    "    # 3) sort by slot & publishTime descending\n",
    "    df = df.sort_values(\n",
    "        [\"settlementDate\",\"settlementPeriod\",\"startTime\",\"publishTimeUTC\"],\n",
    "        ascending=[True, True, True, False]\n",
    "    )\n",
    "\n",
    "    # 4) rank each slot’s forecasts 1…n by recency\n",
    "    df[\"rank\"] = (\n",
    "        df\n",
    "        .groupby([\"settlementDate\",\"settlementPeriod\",\"startTime\"])\n",
    "        .cumcount()\n",
    "        + 1\n",
    "    )\n",
    "\n",
    "    # 5) keep only the top 8 for each slot\n",
    "    df8 = df[df[\"rank\"] <= 8]\n",
    "\n",
    "    # 6) pivot so rank → f_<rank>\n",
    "    wide = (\n",
    "        df8.pivot_table(\n",
    "            index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "            columns=\"rank\",\n",
    "            values=\"generation\",\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 7) rename and ensure f_1…f_8 exist\n",
    "    for i in range(1,9):\n",
    "        col = f\"f_{i}\"\n",
    "        if i in wide.columns:\n",
    "            wide = wide.rename(columns={i: col})\n",
    "        else:\n",
    "            wide[col] = pd.NA\n",
    "\n",
    "    # 8) rebuild startTime (UTC→naive half‐hour)\n",
    "    wide[\"startTime\"] = build_start_time(wide)\n",
    "\n",
    "    # 9) finish (dtype coercion, calendar padding, dedupe, sort)\n",
    "    return _finish(wide[want], want)\n",
    "\n",
    "\n",
    "def b_windfor(df):\n",
    "    \"\"\"\n",
    "    Intraday wind forecasts pivoted into wind_f1…wind_f48,\n",
    "    indexed purely by publish‐time SP, with calendar‐merge\n",
    "    for settlementDate/settlementPeriod.\n",
    "    \"\"\"\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\"] \\\n",
    "         + [f\"wind_f{i}\" for i in range(1,49)]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=want)\n",
    "\n",
    "    # 1) parse timestamps\n",
    "    df[\"publishTime\"]  = pd.to_datetime(df[\"publishTime\"])\n",
    "    df[\"deliveryTime\"] = pd.to_datetime(df[\"startTime\"])\n",
    "\n",
    "    # 2) use publishTime as our row index\n",
    "    df[\"startTime\"] = df[\"publishTime\"]\n",
    "\n",
    "    # 3) compute horizon = # half-hours ahead\n",
    "    df[\"horizon\"] = ((df[\"deliveryTime\"] - df[\"publishTime\"])\n",
    "                      .dt.total_seconds() // 1800).astype(int)\n",
    "\n",
    "    # 4) pivot only on startTime\n",
    "    wide = (\n",
    "      df\n",
    "      .pivot_table(\n",
    "          index=\"startTime\",\n",
    "          columns=\"horizon\",\n",
    "          values=\"generation\",\n",
    "          aggfunc=\"first\"\n",
    "      )\n",
    "      .reset_index()\n",
    "      .rename(columns={i: f\"wind_f{i}\" for i in range(1,49)})\n",
    "    )\n",
    "\n",
    "    wide[\"startTime\"] = wide[\"startTime\"].dt.tz_convert(None)\n",
    "\n",
    "    # 5) bring in settlementDate & settlementPeriod via full calendar\n",
    "    cal = build_uk_halfhour_calendar(start_date, end_date)\n",
    "    wide = cal.merge(wide, on=\"startTime\", how=\"left\")\n",
    "\n",
    "    # 6) ensure exactly wind_f1…wind_f48 exist\n",
    "    for i in range(1,49):\n",
    "        col = f\"wind_f{i}\"\n",
    "        if col not in wide.columns:\n",
    "            wide[col] = pd.NA\n",
    "\n",
    "    # 7) coerce types, pad missing SPs, drop duplicates, sort\n",
    "    return _finish(wide[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_dayahead_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"transmissionSystemDemand\", \"nationalDemand\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_indicated(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"indicatedGeneration\", \"indicatedDemand\",\n",
    "            \"indicatedMargin\", \"indicatedImbalance\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "IC_NAME_TO_COL = {\n",
    "    \"Eleclink (INTELEC)\"      : \"INTELEC\",\n",
    "    \"Ireland(East-West)\"      : \"INTEW\",\n",
    "    \"France(IFA)\"             : \"INTFR\",\n",
    "    \"Ireland (Greenlink)\"     : \"INTGRNL\",\n",
    "    \"IFA2 (INTIFA2)\"          : \"INTIFA2\",\n",
    "    \"Northern Ireland(Moyle)\" : \"INTIRL\",\n",
    "    \"Netherlands(BritNed)\"    : \"INTNED\",\n",
    "    \"Belgium (Nemolink)\"      : \"INTNEM\",\n",
    "    \"North Sea Link (INTNSL)\" : \"INTNSL\",\n",
    "    \"Denmark (Viking link)\"   : \"INTVKL\",\n",
    "}\n",
    "\n",
    "def b_inter(df):\n",
    "    df = df.replace({\"interconnectorName\": IC_NAME_TO_COL})\n",
    "\n",
    "    df = (df.pivot_table(index=[\"settlementDate\", \"settlementPeriod\", \"startTime\"],\n",
    "                         columns=\"interconnectorName\",\n",
    "                         values=\"generation\",\n",
    "                         aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    for col in IC_NAME_TO_COL.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    \n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\"] + list(IC_NAME_TO_COL.values())\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_mid(df):\n",
    "    df = df.loc[df[\"dataProvider\"] == \"APXMIDP\"].copy()\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"price\", \"volume\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_nonbm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"generation\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "HORIZONS = [1, 2, 4, 8, 12]\n",
    "\n",
    "def b_lolpdrm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    # keep only horizons we care about\n",
    "    df = df.loc[df[\"forecastHorizon\"].isin(HORIZONS),\n",
    "                [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "                 \"forecastHorizon\", \"lossOfLoadProbability\",\n",
    "                 \"deratedMargin\"]]\n",
    "\n",
    "    # ----------  LOLP (horizon 1)  ----------\n",
    "    lolp = (df[df[\"forecastHorizon\"] == 1].rename(columns={\"lossOfLoadProbability\": \"1hLOLP\"}).loc[:, [\"startTime\", \"settlementDate\",\n",
    "                       \"settlementPeriod\", \"1hLOLP\"]])\n",
    "\n",
    "    # ----------  DRM (pivot all horizons)  ----------\n",
    "    drm = (df.pivot_table(index=[\"startTime\", \"settlementDate\",\n",
    "                                 \"settlementPeriod\"],\n",
    "                          columns=\"forecastHorizon\",\n",
    "                          values=\"deratedMargin\")\n",
    "            .rename(columns={h: f\"{h}hDRM\" for h in HORIZONS})\n",
    "            .reset_index())\n",
    "\n",
    "    # ----------  merge & order columns  ----------\n",
    "    out = lolp.merge(drm, on=[\"startTime\", \"settlementDate\",\n",
    "                              \"settlementPeriod\"])\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"1hLOLP\", \"1hDRM\"]\n",
    "    out = out[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "def b_lolpdrm_sp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot de-rated margin for settlement-period horizons 1…48 into drm_f1…drm_f48,\n",
    "    indexed by publishingPeriodCommencingTime.\n",
    "    \"\"\"\n",
    "    # target columns\n",
    "    drm_cols = [f\"drm_f{i}\" for i in range(1,49)]\n",
    "    want     = [\"startTime\", \"settlementDate\", \"settlementPeriod\"] + drm_cols\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=want)\n",
    "\n",
    "    # 1) parse times\n",
    "    df[\"publishTime\"] = pd.to_datetime(df[\"publishingPeriodCommencingTime\"], utc=True)\n",
    "    df[\"deliveryTime\"] = pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "\n",
    "    # 2) use the exact publish-period time as our row index\n",
    "    df[\"startTime\"] = df[\"publishTime\"].dt.tz_localize(None)\n",
    "\n",
    "    # 3) compute half-hour horizon (in SPs)\n",
    "    df[\"horizon_sp\"] = (\n",
    "        (df[\"deliveryTime\"] - df[\"publishTime\"])\n",
    "        .dt.total_seconds() // 1800\n",
    "    ).astype(int)\n",
    "\n",
    "    # 4) pivot ALL horizons 1…48\n",
    "    wide = (\n",
    "        df\n",
    "        .pivot_table(\n",
    "            index=\"startTime\",\n",
    "            columns=\"horizon_sp\",\n",
    "            values=\"deratedMargin\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={i: f\"drm_f{i}\" for i in range(1,49)})\n",
    "    )\n",
    "\n",
    "    # 5) full calendar merge to recover settlementDate/Period\n",
    "    cal = build_uk_halfhour_calendar(start_date, end_date)\n",
    "    wide = cal.merge(wide, on=\"startTime\", how=\"left\")\n",
    "\n",
    "    # 6) ensure drm_f1…drm_f48 all exist\n",
    "    for col in drm_cols:\n",
    "        if col not in wide.columns:\n",
    "            wide[col] = pd.NA\n",
    "\n",
    "    # 7) final cleanup: dtype coercion, calendar-pad, dedupe, sort\n",
    "    return _finish(wide[want], want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_system_prices(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"systemSellPrice\"    : \"systemPrice\",      # SSP / SBP\n",
    "        \"netImbalanceVolume\" : \"netImbalanceVolume\"\n",
    "    })\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"systemPrice\", \"netImbalanceVolume\",\n",
    "            \"sellPriceAdjustment\", \"buyPriceAdjustment\",\n",
    "            \"replacementPrice\", \"replacementPriceReferenceVolume\",\n",
    "            \"totalAcceptedOfferVolume\", \"totalAcceptedBidVolume\",\n",
    "            \"totalAdjustmentSellVolume\", \"totalAdjustmentBuyVolume\",\n",
    "            \"totalSystemTaggedAcceptedOfferVolume\",\n",
    "            \"totalSystemTaggedAcceptedBidVolume\",\n",
    "            \"totalSystemTaggedAdjustmentSellVolume\",\n",
    "            \"totalSystemTaggedAdjustmentBuyVolume\"]\n",
    "\n",
    "    # create any missing columns so _finish keeps dtype order\n",
    "    for col in want:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "652108b0-5e7b-46c7-abe5-79a299986eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Looking in folder: bmrs_json_raw/LOLPDRM\n",
      "   Found 3104 files: [PosixPath('bmrs_json_raw/LOLPDRM/2017-01-01.json.gz'), PosixPath('bmrs_json_raw/LOLPDRM/2017-01-02.json.gz'), PosixPath('bmrs_json_raw/LOLPDRM/2017-01-03.json.gz'), PosixPath('bmrs_json_raw/LOLPDRM/2017-01-04.json.gz'), PosixPath('bmrs_json_raw/LOLPDRM/2017-01-05.json.gz')]…\n",
      "   Loaded raw rows: 737014\n",
      "   Builder produced 148942 rows\n",
      "✓ DRM_FORECASTS: wrote 148,942 rows → bmrs_csv_raw/DRM_FORECASTS.csv\n"
     ]
    }
   ],
   "source": [
    "# BUILDERS = {\n",
    "#     \"ACTUAL_DEMAND\"                  : b_actual_demand,\n",
    "#     \"ACTUAL_GEN_WIND_SOLAR\"          : b_actual_gen_ws,\n",
    "#     \"DAYAHEAD_DEMAND\"                : b_dayahead_demand,\n",
    "#     \"DETAILED_WINDFOR\"               : b_windfor,\n",
    "#     \"DAYAHEAD_GEN_WIND_SOLAR\"        : b_dayahead_gen_ws,\n",
    "#     \"INTRADAYPROCESS_GEN_WIND_SOLAR\" : b_intradayprocess_gen_ws,\n",
    "#     \"INTRADAYTOTAL_GEN_WIND_SOLAR\"   : b_intradaytotal_gen_ws,\n",
    "#     \"GEN_PER_TYPE\"                   : b_gen_per_type,\n",
    "#     \"INDICATED_DAYAHEAD_DEMAND\"      : b_indicated,\n",
    "#     \"INTER\"                          : b_inter,\n",
    "#     \"LOLPDRM\"                        : b_lolpdrm,\n",
    "#     \"NONBM\"                          : b_nonbm,\n",
    "#     \"MID\"                            : b_mid,\n",
    "#     \"SYSTEM_PRICES\"                  : b_system_prices,\n",
    "# }\n",
    "\n",
    "BUILDERS = {\n",
    "    \"DRM_FORECASTS\"               : b_lolpdrm_sp,\n",
    "}\n",
    "\n",
    "\n",
    "def process_one(code: str, builder):\n",
    "    # Map output codes to their real input folders\n",
    "    if code == \"DRM_FORECASTS\":\n",
    "        folder = RAW_DIR / \"LOLPDRM\"\n",
    "    elif code == \"WIND_FORECASTS\":\n",
    "        folder = RAW_DIR / \"WINDFOR_HISTORY\"\n",
    "    else:\n",
    "        folder = RAW_DIR / code\n",
    "\n",
    "    print(f\"\\n→ Looking in folder: {folder}\")\n",
    "    files = sorted(folder.glob(\"*.json.gz\"))\n",
    "    print(f\"   Found {len(files)} files: {files[:5]}{'…' if len(files)>5 else ''}\")\n",
    "\n",
    "    if \"WINDFOR\" in folder.name:\n",
    "        df_raw = _folder_to_df_detailed(folder)\n",
    "    else:\n",
    "        df_raw = _folder_to_df(folder)\n",
    "\n",
    "    print(f\"   Loaded raw rows: {len(df_raw)}\")\n",
    "    if df_raw.empty:\n",
    "        print(f\"⚠ {code}: no data, skipping\")\n",
    "        return None\n",
    "\n",
    "    df_tidy = builder(df_raw)\n",
    "    print(f\"   Builder produced {len(df_tidy)} rows\")\n",
    "\n",
    "    out = CSV_DIR / f\"{code}.csv\"\n",
    "    df_tidy.to_csv(out, index=False)\n",
    "    print(f\"✓ {code}: wrote {len(df_tidy):,} rows → {out}\")\n",
    "    return df_tidy\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) run all builders and write CSVs\n",
    "    for code, builder in BUILDERS.items():\n",
    "        result = process_one(code, builder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d8d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
