{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4333e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import gc\n",
    "from itertools import chain\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edf6a8",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING\n",
    "\n",
    "## BUILDING THE FINAL .csv FROM .json\n",
    "\n",
    "* **Imports & configuration** – import `json`, `gzip`, `pathlib`, `itertools`, `pandas`, `tqdm`, `numpy`; set `RAW_DIR`, `CSV_DIR`, and the global `start_date`/`end_date`.\n",
    "\n",
    "* **Raw JSON loading** – `_load_json_z` reads a single `.json.gz`; `_folder_to_df` concatenates all files in a folder into one DataFrame.\n",
    "\n",
    "* **Timestamp construction** – `build_start_time` returns a UTC‐like `startTime` column, either from an existing field or by combining `settlementDate`+`settlementPeriod`.\n",
    "\n",
    "* **Type coercion** – `_coerce_type` downcasts non-key columns to `float32` and normalises the date/time columns to naïve `datetime64[ns]`.\n",
    "\n",
    "* **Small-gap interpolation** – `fill_small_gaps` linearly fills runs of up to two missing half-hours in numeric columns, leaving longer gaps untouched.\n",
    "\n",
    "* **UK half-hour calendar** – `build_uk_halfhour_calendar` generates a full DST-aware sequence of half-hour intervals between any two dates.\n",
    "\n",
    "* **Padding missing intervals** – `_pad_missing` merges data onto the full calendar (trimming to the global date range) so every expected interval appears.\n",
    "\n",
    "* **Finalising pipeline** – `_finish` selects the requested columns, coerces types, pads missing, drops duplicates, interpolates small gaps, then sorts and resets the index.\n",
    "\n",
    "* **Dataset builders** – each `b_<dataset>` function (e.g. `b_actual_demand`, `b_gen_per_type`, `b_system_prices`, etc.) computes `startTime`, selects its own `want` column list, and hands off to `_finish` to produce the final CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac9b343-ce17-4d94-bd14-1327f29d3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "RAW_DIR = Path(\"bmrs_json_raw\")\n",
    "CSV_DIR = Path(\"bmrs_csv_raw\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "\n",
    "CSV_DIR.mkdir(exist_ok=True)\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Date range\n",
    "start_date = \"2017-01-01\"\n",
    "end_date   = \"2025-05-31\"\n",
    "\n",
    "\n",
    "\n",
    "def _load_json_z(path: Path) -> list[dict]:\n",
    "    with gzip.open(path, \"rt\") as fh:\n",
    "        return json.load(fh)[\"data\"]\n",
    "\n",
    "\n",
    "def _folder_to_df(folder: Path) -> pd.DataFrame:\n",
    "    files = sorted(folder.glob(\"*.json.gz\"))\n",
    "    rows  = chain.from_iterable((_load_json_z(f) for f in files))\n",
    "    return pd.DataFrame.from_records(rows)\n",
    "\n",
    "\n",
    "def build_start_time(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return UTC-like timestamp (start of settlement period).\"\"\"\n",
    "    if \"startTime\" in df.columns:\n",
    "        return pd.to_datetime(df[\"startTime\"], errors=\"coerce\")\n",
    "    # otherwise compose from date + SP (SP1 = 00:00 UTC *winter*)\n",
    "    base = pd.to_datetime(df[\"settlementDate\"])\n",
    "    off  = pd.to_timedelta(df[\"settlementPeriod\"].astype(int).sub(1) * 30,\n",
    "                           unit=\"m\")\n",
    "    return base + off\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── finisher ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "KEY_COLS = {\"startTime\", \"settlementDate\", \"settlementPeriod\"}\n",
    "\n",
    "def _coerce_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • convert every non-key column to float32\n",
    "    • normalise datetime columns\n",
    "    \"\"\"\n",
    "    # 1) numeric columns  → float32\n",
    "    num_cols = [c for c in df.columns if c not in KEY_COLS]\n",
    "    df[num_cols] = df[num_cols].apply(\n",
    "        pd.to_numeric, errors=\"coerce\", downcast=\"float\"\n",
    "    )\n",
    "\n",
    "    # 2) settlementDate  → 00:00 of that day, no timezone\n",
    "    df[\"settlementDate\"] = (\n",
    "        pd.to_datetime(df[\"settlementDate\"], utc=True)   # ensure tz-aware\n",
    "          .dt.normalize()                                # strip hh:mm:ss\n",
    "          .dt.tz_localize(None)                          # drop timezone\n",
    "    )\n",
    "\n",
    "    # 3) startTime  → no timezone (but keep hh:mm)\n",
    "    if \"startTime\" in df.columns:\n",
    "        df[\"startTime\"] = (\n",
    "            pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "              .dt.tz_localize(None)\n",
    "        )\n",
    "\n",
    "    # settlementPeriod stays int32\n",
    "    if \"settlementPeriod\" in df.columns:\n",
    "        df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_uk_halfhour_calendar(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Build UK half-hour calendar with correct DST handling:\n",
    "      • Spring-forward days: 46 periods (including the skipped 01:00/01:30)\n",
    "      • Normal days: 48 periods 00:00-23:30\n",
    "      • BST days: 48 periods 23:00(prev day)-22:30\n",
    "      • Autumn-back days: 50 periods 23:00(prev day)-22:30\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_date(x):\n",
    "        if isinstance(x, str):\n",
    "            \n",
    "            if x.count(\"-\") == 2 and x[4] == \"-\": # ISO format\n",
    "                return datetime.date.fromisoformat(x)\n",
    "            return datetime.datetime.strptime(x, \"%d/%m/%Y\").date() # UK format\n",
    "        if isinstance(x, pd.Timestamp):\n",
    "            return x.date()\n",
    "        return x\n",
    "\n",
    "    start = _to_date(start_date)\n",
    "    end   = _to_date(end_date)\n",
    "\n",
    "    london = ZoneInfo(\"Europe/London\")\n",
    "    utc    = ZoneInfo(\"UTC\")\n",
    "    rows   = []\n",
    "\n",
    "    for single in pd.date_range(start, end, freq=\"D\"):\n",
    "        D   = single.date()\n",
    "        # local midnights in London\n",
    "        dt0 = datetime.datetime(D.year, D.month, D.day, tzinfo=london)\n",
    "        dt1 = dt0 + datetime.timedelta(days=1)\n",
    "\n",
    "        # number of half-hours that actually occur\n",
    "        total_secs = (dt1.astimezone(utc) - dt0.astimezone(utc)).total_seconds()\n",
    "        n_periods = int(total_secs // 1800)\n",
    "\n",
    "        # align to UTC-naive base for SP1\n",
    "        offset_h = dt0.utcoffset().total_seconds() / 3600\n",
    "        if offset_h > 0:\n",
    "            base = datetime.datetime(D.year, D.month, D.day) - datetime.timedelta(hours=int(offset_h))\n",
    "        else:\n",
    "            base = datetime.datetime(D.year, D.month, D.day)\n",
    "\n",
    "        for i in range(n_periods):\n",
    "            rows.append({\n",
    "                \"startTime\":        base + datetime.timedelta(minutes=30 * i),\n",
    "                \"settlementDate\":   D,\n",
    "                \"settlementPeriod\": i + 1\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ─── coerce to pandas time types ───\n",
    "    df[\"startTime\"]      = pd.to_datetime(df[\"startTime\"])\n",
    "    df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "    # ───────────────────────────────────\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _pad_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each settlementDate in df, build exactly the UK\n",
    "    SP‐calendar via build_uk_halfhour_calendar(min,max), then\n",
    "    left‐merge your data on (Date,Period,startTime).\n",
    "    Also trims the input df to only include rows between start_date and end_date (inclusive).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "    df=df.drop_duplicates(subset=[\"startTime\"])\n",
    "    # ensure proper types\n",
    "    df[\"settlementDate\"]   = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(int)\n",
    "    df[\"startTime\"]        = pd.to_datetime(df[\"startTime\"])\n",
    "\n",
    "    lo = start_date\n",
    "    hi = end_date\n",
    "\n",
    "    # trim input df to only include rows between start_date and end_date (inclusive)\n",
    "    mask = (\n",
    "        (df[\"settlementDate\"] >= pd.to_datetime(start_date)) &\n",
    "        (df[\"settlementDate\"] <= pd.to_datetime(end_date))\n",
    "    )\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    # build the master calendar\n",
    "    cal = build_uk_halfhour_calendar(lo, hi)\n",
    "    cal[\"settlementDate\"]   = pd.to_datetime(cal[\"settlementDate\"])\n",
    "    cal[\"settlementPeriod\"] = cal[\"settlementPeriod\"].astype(int)\n",
    "    cal[\"startTime\"]        = pd.to_datetime(cal[\"startTime\"], dayfirst=True)\n",
    "\n",
    "    # left‐join your actual data onto the calendar\n",
    "    out = (\n",
    "        cal\n",
    "        .merge(df,\n",
    "               on=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "               how=\"left\",\n",
    "               sort=False)\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _finish(out: pd.DataFrame, want: list[str]) -> pd.DataFrame:\n",
    "    # 1) keep only requested columns\n",
    "    out = out[want]\n",
    "    # 2) downcast\n",
    "    out = _coerce_type(out)\n",
    "    # 3) pad missing with DST‐aware UK calendar\n",
    "    out = _pad_missing(out)\n",
    "    # 4) drop duplicate rows\n",
    "    out = out.drop_duplicates()\n",
    "    \n",
    "    return out.sort_values(\"startTime\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── builders ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def b_actual_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"initialDemandOutturn\",\n",
    "            \"initialTransmissionSystemDemandOutturn\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def _pivot_wind_solar(df, value_name):\n",
    "    map_ = {\n",
    "        \"Wind Onshore\" : \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\"        : \"solarGeneration\",\n",
    "    }\n",
    "    df  = df.replace({\"psrType\": map_})\n",
    "    out = (df.pivot_table(index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "                          columns=\"psrType\",\n",
    "                          values=value_name, aggfunc=\"first\")\n",
    "             .reset_index())\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for c in map_.values():\n",
    "        if c not in out.columns:\n",
    "            out[c] = pd.NA\n",
    "    return out\n",
    "\n",
    "\n",
    "def b_actual_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar(df.rename(columns={\"quantity\":\"gen\"}),\n",
    "                             \"gen\")\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"windOnshoreGeneration\",\"windOffshoreGeneration\",\n",
    "            \"solarGeneration\"]\n",
    "    out = tidy[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "def b_dayahead_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar(df.rename(columns={\"quantity\":\"forecast\"}),\n",
    "                             \"forecast\")\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"windOnshoreGeneration\",\"windOffshoreGeneration\",\n",
    "            \"solarGeneration\"]\n",
    "    out = tidy[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "GEN_MAP = {\n",
    "    \"Hydro Pumped Storage\":\"hydroPumpedStorage\",\n",
    "    \"Fossil Hard coal\"    :\"fossilCoal\",\n",
    "    \"Fossil Gas\"          :\"fossilGas\",\n",
    "    \"Fossil Oil\"          :\"fossilOil\",\n",
    "    \"Nuclear\"             :\"nuclear\",\n",
    "    \"Other\"               :\"other\",\n",
    "    \"Wind Onshore\"        :\"windOnshore\",\n",
    "    \"Wind Offshore\"       :\"windOffshore\",\n",
    "    \"Solar\"               :\"solar\",\n",
    "}\n",
    "def b_gen_per_type(df):\n",
    "    df = (df.replace({\"psrType\": GEN_MAP})\n",
    "            .pivot_table(index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "                         columns=\"psrType\",\n",
    "                         values=\"quantity\", aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    for col in GEN_MAP.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\"]+list(GEN_MAP.values())\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_dayahead_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"transmissionSystemDemand\", \"nationalDemand\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_indicated(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"indicatedGeneration\", \"indicatedDemand\",\n",
    "            \"indicatedMargin\", \"indicatedImbalance\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "IC_NAME_TO_COL = {\n",
    "    \"Eleclink (INTELEC)\"      : \"INTELEC\",\n",
    "    \"Ireland(East-West)\"      : \"INTEW\",\n",
    "    \"France(IFA)\"             : \"INTFR\",\n",
    "    \"Ireland (Greenlink)\"     : \"INTGRNL\",\n",
    "    \"IFA2 (INTIFA2)\"          : \"INTIFA2\",\n",
    "    \"Northern Ireland(Moyle)\" : \"INTIRL\",\n",
    "    \"Netherlands(BritNed)\"    : \"INTNED\",\n",
    "    \"Belgium (Nemolink)\"      : \"INTNEM\",\n",
    "    \"North Sea Link (INTNSL)\" : \"INTNSL\",\n",
    "    \"Denmark (Viking link)\"   : \"INTVKL\",\n",
    "}\n",
    "\n",
    "def b_inter(df):\n",
    "    df = df.replace({\"interconnectorName\": IC_NAME_TO_COL})\n",
    "\n",
    "    df = (df.pivot_table(index=[\"settlementDate\", \"settlementPeriod\", \"startTime\"],\n",
    "                         columns=\"interconnectorName\",\n",
    "                         values=\"generation\",\n",
    "                         aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    for col in IC_NAME_TO_COL.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    \n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\"] + list(IC_NAME_TO_COL.values())\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_mid(df):\n",
    "    df = df.loc[df[\"dataProvider\"] == \"APXMIDP\"].copy()\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"price\", \"volume\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_nonbm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"generation\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "HORIZONS = [1, 2, 4, 8, 12]\n",
    "\n",
    "def b_lolpdrm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    # keep only horizons we care about\n",
    "    df = df.loc[df[\"forecastHorizon\"].isin(HORIZONS),\n",
    "                [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "                 \"forecastHorizon\", \"lossOfLoadProbability\",\n",
    "                 \"deratedMargin\"]]\n",
    "\n",
    "    # ----------  LOLP (horizon 1)  ----------\n",
    "    lolp = (df[df[\"forecastHorizon\"] == 1]\n",
    "              .rename(columns={\"lossOfLoadProbability\": \"1hLOLP\"})\n",
    "              .loc[:, [\"startTime\", \"settlementDate\",\n",
    "                       \"settlementPeriod\", \"1hLOLP\"]])\n",
    "\n",
    "    # ----------  DRM (pivot all horizons)  ----------\n",
    "    drm = (df.pivot_table(index=[\"startTime\", \"settlementDate\",\n",
    "                                 \"settlementPeriod\"],\n",
    "                          columns=\"forecastHorizon\",\n",
    "                          values=\"deratedMargin\")\n",
    "             .rename(columns={h: f\"{h}hDRM\" for h in HORIZONS})\n",
    "             .reset_index())\n",
    "\n",
    "    # ----------  merge & order columns  ----------\n",
    "    out = lolp.merge(drm, on=[\"startTime\", \"settlementDate\",\n",
    "                              \"settlementPeriod\"])\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"1hLOLP\", \"1hDRM\"]\n",
    "    out = out[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_system_prices(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"systemSellPrice\"    : \"systemPrice\",      # SSP / SBP\n",
    "        \"netImbalanceVolume\" : \"netImbalanceVolume\"\n",
    "    })\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"systemPrice\", \"netImbalanceVolume\",\n",
    "            \"sellPriceAdjustment\", \"buyPriceAdjustment\",\n",
    "            \"replacementPrice\", \"replacementPriceReferenceVolume\",\n",
    "            \"totalAcceptedOfferVolume\", \"totalAcceptedBidVolume\",\n",
    "            \"totalAdjustmentSellVolume\", \"totalAdjustmentBuyVolume\",\n",
    "            \"totalSystemTaggedAcceptedOfferVolume\",\n",
    "            \"totalSystemTaggedAcceptedBidVolume\",\n",
    "            \"totalSystemTaggedAdjustmentSellVolume\",\n",
    "            \"totalSystemTaggedAdjustmentBuyVolume\"]\n",
    "\n",
    "    # create any missing columns so _finish keeps dtype order\n",
    "    for col in want:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "652108b0-5e7b-46c7-abe5-79a299986eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ACTUAL_DEMAND: 147,502 rows → bmrs_csv_raw/ACTUAL_DEMAND.csv\n",
      "✓ ACTUAL_GEN_WIND_SOLAR: 147,502 rows → bmrs_csv_raw/ACTUAL_GEN_WIND_SOLAR.csv\n",
      "✓ DAYAHEAD_DEMAND: 147,502 rows → bmrs_csv_raw/DAYAHEAD_DEMAND.csv\n",
      "✓ DAYAHEAD_GEN_WIND_SOLAR: 147,502 rows → bmrs_csv_raw/DAYAHEAD_GEN_WIND_SOLAR.csv\n",
      "✓ GEN_PER_TYPE: 147,502 rows → bmrs_csv_raw/GEN_PER_TYPE.csv\n",
      "✓ INDICATED_DAYAHEAD_DEMAND: 147,502 rows → bmrs_csv_raw/INDICATED_DAYAHEAD_DEMAND.csv\n",
      "✓ INTER: 147,502 rows → bmrs_csv_raw/INTER.csv\n",
      "✓ LOLPDRM: 147,502 rows → bmrs_csv_raw/LOLPDRM.csv\n",
      "✓ NONBM: 147,502 rows → bmrs_csv_raw/NONBM.csv\n",
      "✓ MID: 147,502 rows → bmrs_csv_raw/MID.csv\n",
      "✓ SYSTEM_PRICES: 147,502 rows → bmrs_csv_raw/SYSTEM_PRICES.csv\n",
      "↺ Mutual fill applied to ACTUAL_GEN_WIND_SOLAR and GEN_PER_TYPE\n"
     ]
    }
   ],
   "source": [
    "BUILDERS = {\n",
    "    \"ACTUAL_DEMAND\"            : b_actual_demand,\n",
    "    \"ACTUAL_GEN_WIND_SOLAR\"    : b_actual_gen_ws,\n",
    "    \"DAYAHEAD_DEMAND\"          : b_dayahead_demand,\n",
    "    \"DAYAHEAD_GEN_WIND_SOLAR\"  : b_dayahead_gen_ws,\n",
    "    \"GEN_PER_TYPE\"             : b_gen_per_type,\n",
    "    \"INDICATED_DAYAHEAD_DEMAND\": b_indicated,\n",
    "    \"INTER\"                    : b_inter,\n",
    "    \"LOLPDRM\"                  : b_lolpdrm,\n",
    "    \"NONBM\"                    : b_nonbm,\n",
    "    \"MID\"                      : b_mid,\n",
    "    \"SYSTEM_PRICES\"            : b_system_prices,\n",
    "}\n",
    "\n",
    "def process_one(code: str, builder):\n",
    "    folder = RAW_DIR / code\n",
    "    if not folder.exists():\n",
    "        print(f\"⚠ {code}: folder missing → skipped\")\n",
    "        return None\n",
    "    df_raw = _folder_to_df(folder)\n",
    "    if df_raw.empty:\n",
    "        print(f\"⚠ {code}: empty → skipped\")\n",
    "        return None\n",
    "    df_tidy = builder(df_raw)\n",
    "    out = CSV_DIR / f\"{code}.csv\"\n",
    "    df_tidy.to_csv(out, index=False)\n",
    "    print(f\"✓ {code}: {len(df_tidy):,} rows → {out}\")\n",
    "    return df_tidy\n",
    "\n",
    "def main():\n",
    "    # placeholders to capture the two dataframes\n",
    "    actual_ws   = None\n",
    "    gen_per_type = None\n",
    "\n",
    "    # 1) run all builders and write CSVs\n",
    "    for code, builder in BUILDERS.items():\n",
    "        result = process_one(code, builder)\n",
    "        if code == \"ACTUAL_GEN_WIND_SOLAR\":\n",
    "            actual_ws = result.copy() if result is not None else None\n",
    "        elif code == \"GEN_PER_TYPE\":\n",
    "            gen_per_type = result.copy() if result is not None else None\n",
    "\n",
    "    # 2) mutual fill between those two\n",
    "    if actual_ws is not None and gen_per_type is not None:\n",
    "        for gen_col, act_col in [\n",
    "            (\"windOffshore\",           \"windOffshoreGeneration\"),\n",
    "            (\"windOnshore\",            \"windOnshoreGeneration\"),\n",
    "            (\"solar\",                  \"solarGeneration\")\n",
    "        ]:\n",
    "            # fill actual from gen_per_type\n",
    "            actual_ws[act_col] = actual_ws[act_col].combine_first(\n",
    "                                     gen_per_type[gen_col]\n",
    "                                 )\n",
    "            # fill gen_per_type from actual\n",
    "            gen_per_type[gen_col] = gen_per_type[gen_col].combine_first(\n",
    "                                        actual_ws[act_col]\n",
    "                                    )\n",
    "\n",
    "        # 3) overwrite the two CSVs\n",
    "        actual_ws.to_csv(CSV_DIR/\"ACTUAL_GEN_WIND_SOLAR.csv\", index=False)\n",
    "        gen_per_type.to_csv(CSV_DIR/\"GEN_PER_TYPE.csv\",       index=False)\n",
    "        print(\"↺ Mutual fill applied to ACTUAL_GEN_WIND_SOLAR and GEN_PER_TYPE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
