{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4333e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import gzip\n",
    "import gc\n",
    "from itertools import chain\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edf6a8",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING\n",
    "\n",
    "## BUILDING THE FINAL .csv FROM .json\n",
    "\n",
    "* **Imports & configuration** – import `json`, `gzip`, `pathlib`, `itertools`, `pandas`, `tqdm`, `numpy`; set `RAW_DIR`, `CSV_DIR`, and the global `start_date`/`end_date`.\n",
    "\n",
    "* **Raw JSON loading** – `_load_json_z` reads a single `.json.gz`; `_folder_to_df` concatenates all files in a folder into one DataFrame.\n",
    "\n",
    "* **Timestamp construction** – `build_start_time` returns a UTC‐like `startTime` column, either from an existing field or by combining `settlementDate`+`settlementPeriod`.\n",
    "\n",
    "* **Type coercion** – `_coerce_type` downcasts non-key columns to `float32` and normalises the date/time columns to naïve `datetime64[ns]`.\n",
    "\n",
    "* **Small-gap interpolation** – `fill_small_gaps` linearly fills runs of up to two missing half-hours in numeric columns, leaving longer gaps untouched.\n",
    "\n",
    "* **UK half-hour calendar** – `build_uk_halfhour_calendar` generates a full DST-aware sequence of half-hour intervals between any two dates.\n",
    "\n",
    "* **Padding missing intervals** – `_pad_missing` merges data onto the full calendar (trimming to the global date range) so every expected interval appears.\n",
    "\n",
    "* **Finalising pipeline** – `_finish` selects the requested columns, coerces types, pads missing, drops duplicates, interpolates small gaps, then sorts and resets the index.\n",
    "\n",
    "* **Dataset builders** – each `b_<dataset>` function (e.g. `b_actual_demand`, `b_gen_per_type`, `b_system_prices`, etc.) computes `startTime`, selects its own `want` column list, and hands off to `_finish` to produce the final CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac9b343-ce17-4d94-bd14-1327f29d3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "RAW_DIR     = Path(\"bmrs_json_raw\")\n",
    "CSV_DIR     = Path(\"bmrs_csv_raw\")\n",
    "LOG_DIR     = Path(\"logs\")\n",
    "\n",
    "CSV_DIR.mkdir(exist_ok=True)\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Date range\n",
    "start_date = \"2017-01-01\"\n",
    "end_date   = \"2025-06-30\"\n",
    "\n",
    "\n",
    "\n",
    "def _load_json_z(path: Path) -> list[dict]:\n",
    "    with gzip.open(path, \"rt\") as fh:\n",
    "        return json.load(fh)[\"data\"]\n",
    "\n",
    "\n",
    "def _folder_to_df(folder: Path) -> pd.DataFrame:\n",
    "    files = sorted(folder.glob(\"*.json.gz\"))\n",
    "    rows  = chain.from_iterable((_load_json_z(f) for f in files))\n",
    "    return pd.DataFrame.from_records(rows)\n",
    "\n",
    "\n",
    "def build_start_time(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Return UTC-like timestamp (start of settlement period).\"\"\"\n",
    "    if \"startTime\" in df.columns:\n",
    "        return pd.to_datetime(df[\"startTime\"], errors=\"coerce\")\n",
    "    # otherwise compose from date + SP (SP1 = 00:00 UTC *winter*)\n",
    "    base = pd.to_datetime(df[\"settlementDate\"])\n",
    "    off  = pd.to_timedelta(df[\"settlementPeriod\"].astype(int).sub(1) * 30,\n",
    "                           unit=\"m\")\n",
    "    return base + off\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── finisher ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "KEY_COLS = {\"startTime\", \"settlementDate\", \"settlementPeriod\"}\n",
    "\n",
    "def _coerce_type(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • convert every non-key column to float32\n",
    "    • normalise datetime columns\n",
    "    \"\"\"\n",
    "    # 1) numeric columns  → float32\n",
    "    num_cols = [c for c in df.columns if c not in KEY_COLS]\n",
    "    df[num_cols] = df[num_cols].apply(\n",
    "        pd.to_numeric, errors=\"coerce\", downcast=\"float\"\n",
    "    )\n",
    "\n",
    "    # 2) settlementDate  → 00:00 of that day, no timezone\n",
    "    df[\"settlementDate\"] = (\n",
    "        pd.to_datetime(df[\"settlementDate\"], utc=True)   # ensure tz-aware\n",
    "          .dt.normalize()                                # strip hh:mm:ss\n",
    "          .dt.tz_localize(None)                          # drop timezone\n",
    "    )\n",
    "\n",
    "    # 3) startTime  → no timezone (but keep hh:mm)\n",
    "    if \"startTime\" in df.columns:\n",
    "        df[\"startTime\"] = (\n",
    "            pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "              .dt.tz_localize(None)\n",
    "        )\n",
    "\n",
    "    # settlementPeriod stays int32\n",
    "    if \"settlementPeriod\" in df.columns:\n",
    "        df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_uk_halfhour_calendar(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Build UK half-hour calendar with correct DST handling:\n",
    "      • Spring-forward days: 46 periods (including the skipped 01:00/01:30)\n",
    "      • Normal days: 48 periods 00:00-23:30\n",
    "      • BST days: 48 periods 23:00(prev day)-22:30\n",
    "      • Autumn-back days: 50 periods 23:00(prev day)-22:30\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_date(x):\n",
    "        if isinstance(x, str):\n",
    "            \n",
    "            if x.count(\"-\") == 2 and x[4] == \"-\": # ISO format\n",
    "                return datetime.date.fromisoformat(x)\n",
    "            return datetime.datetime.strptime(x, \"%d/%m/%Y\").date() # UK format\n",
    "        if isinstance(x, pd.Timestamp):\n",
    "            return x.date()\n",
    "        return x\n",
    "\n",
    "    start = _to_date(start_date)\n",
    "    end   = _to_date(end_date)\n",
    "\n",
    "    london = ZoneInfo(\"Europe/London\")\n",
    "    utc    = ZoneInfo(\"UTC\")\n",
    "    rows   = []\n",
    "\n",
    "    for single in pd.date_range(start, end, freq=\"D\"):\n",
    "        D   = single.date()\n",
    "        # local midnights in London\n",
    "        dt0 = datetime.datetime(D.year, D.month, D.day, tzinfo=london)\n",
    "        dt1 = dt0 + datetime.timedelta(days=1)\n",
    "\n",
    "        # number of half-hours that actually occur\n",
    "        total_secs = (dt1.astimezone(utc) - dt0.astimezone(utc)).total_seconds()\n",
    "        n_periods = int(total_secs // 1800)\n",
    "\n",
    "        # align to UTC-naive base for SP1\n",
    "        offset_h = dt0.utcoffset().total_seconds() / 3600\n",
    "        if offset_h > 0:\n",
    "            base = datetime.datetime(D.year, D.month, D.day) - datetime.timedelta(hours=int(offset_h))\n",
    "        else:\n",
    "            base = datetime.datetime(D.year, D.month, D.day)\n",
    "\n",
    "        for i in range(n_periods):\n",
    "            rows.append({\n",
    "                \"startTime\":        base + datetime.timedelta(minutes=30 * i),\n",
    "                \"settlementDate\":   D,\n",
    "                \"settlementPeriod\": i + 1\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ─── coerce to pandas time types ───\n",
    "    df[\"startTime\"]      = pd.to_datetime(df[\"startTime\"])\n",
    "    df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(\"int32\")\n",
    "    # ───────────────────────────────────\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _pad_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each settlementDate in df, build exactly the UK\n",
    "    SP‐calendar via build_uk_halfhour_calendar(min,max), then\n",
    "    left‐merge your data on (Date,Period,startTime).\n",
    "    Also trims the input df to only include rows between start_date and end_date (inclusive).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "    df=df.drop_duplicates(subset=[\"startTime\"])\n",
    "    # ensure proper types\n",
    "    df[\"settlementDate\"]   = pd.to_datetime(df[\"settlementDate\"]).dt.normalize()\n",
    "    df[\"settlementPeriod\"] = df[\"settlementPeriod\"].astype(int)\n",
    "    df[\"startTime\"]        = pd.to_datetime(df[\"startTime\"])\n",
    "\n",
    "    lo = start_date\n",
    "    hi = end_date\n",
    "\n",
    "    # trim input df to only include rows between start_date and end_date (inclusive)\n",
    "    mask = (\n",
    "        (df[\"settlementDate\"] >= pd.to_datetime(start_date)) &\n",
    "        (df[\"settlementDate\"] <= pd.to_datetime(end_date))\n",
    "    )\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    # build the master calendar\n",
    "    cal = build_uk_halfhour_calendar(lo, hi)\n",
    "    cal[\"settlementDate\"]   = pd.to_datetime(cal[\"settlementDate\"])\n",
    "    cal[\"settlementPeriod\"] = cal[\"settlementPeriod\"].astype(int)\n",
    "    cal[\"startTime\"]        = pd.to_datetime(cal[\"startTime\"], dayfirst=True)\n",
    "\n",
    "    # left‐join your actual data onto the calendar\n",
    "    out = (\n",
    "        cal\n",
    "        .merge(df,\n",
    "               on=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "               how=\"left\",\n",
    "               sort=False)\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _finish(out: pd.DataFrame, want: list[str]) -> pd.DataFrame:\n",
    "    # 1) keep only requested columns\n",
    "    out = out[want]\n",
    "    # 2) downcast\n",
    "    out = _coerce_type(out)\n",
    "    # 3) pad missing with DST‐aware UK calendar\n",
    "    out = _pad_missing(out)\n",
    "    # 4) drop duplicate rows\n",
    "    out = out.drop_duplicates()\n",
    "    \n",
    "    return out.sort_values(\"startTime\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────── builders ────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def b_actual_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"initialDemandOutturn\",\n",
    "            \"initialTransmissionSystemDemandOutturn\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def _pivot_wind_solar(df, value_name):\n",
    "    map_ = {\n",
    "        \"Wind Onshore\" : \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\"        : \"solarGeneration\",\n",
    "    }\n",
    "    df  = df.replace({\"psrType\": map_})\n",
    "    out = (df.pivot_table(index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "                          columns=\"psrType\",\n",
    "                          values=value_name, aggfunc=\"first\")\n",
    "             .reset_index())\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for c in map_.values():\n",
    "        if c not in out.columns:\n",
    "            out[c] = pd.NA\n",
    "    return out\n",
    "\n",
    "def b_actual_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar(df.rename(columns={\"quantity\":\"gen\"}), \"gen\")\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\",\n",
    "            \"windOnshoreGeneration\",\"windOffshoreGeneration\",\n",
    "            \"solarGeneration\"]\n",
    "    out = tidy[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _pivot_wind_solar_forecast(df, process_type, value_name):\n",
    "    map_ = {\n",
    "        \"Wind Onshore\":  \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\":         \"solarGeneration\",\n",
    "    }\n",
    "    df = (\n",
    "        df.loc[df[\"processType\"] == process_type]\n",
    "          .rename(columns={\"quantity\": value_name})\n",
    "          .replace({\"psrType\": map_})\n",
    "    )\n",
    "    out = (\n",
    "        df.pivot_table(\n",
    "            index=[\"settlementDate\", \"settlementPeriod\", \"startTime\", \"publishTime\"],\n",
    "            columns=\"psrType\",\n",
    "            values=value_name,\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for col in map_.values():\n",
    "        if col not in out.columns:\n",
    "            out[col] = pd.NA\n",
    "    return out\n",
    "\n",
    "def b_dayahead_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Day ahead\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "def b_intradayprocess_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Intraday process\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "def b_intradaytotal_gen_ws(df):\n",
    "    tidy = _pivot_wind_solar_forecast(df, \"Intraday total\", \"forecast\")\n",
    "    want = [\n",
    "        \"startTime\", \"settlementDate\", \"settlementPeriod\", \"publishTime\",\n",
    "        \"windOnshoreGeneration\", \"windOffshoreGeneration\", \"solarGeneration\"\n",
    "    ]\n",
    "    return _finish(tidy[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _folder_to_df_detailed(folder: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Like _folder_to_df, but also injects `publishSP` from each filename:\n",
    "      bmrs_json_raw/DETAILED_WINDFOR/YYYY-MM-DD_<SP>.json.gz\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for f in sorted(folder.glob(\"*.json.gz\")):\n",
    "        name = f.name  # e.g. \"2017-01-01_10.json.gz\"\n",
    "        # strip off the \".json.gz\" bit, then split on \"_\"\n",
    "        base = name[:-len(\".json.gz\")]            # → \"2017-01-01_10\"\n",
    "        sp   = int(base.split(\"_\", 1)[1])         # → 10\n",
    "        # … load JSON …\n",
    "        payload = json.load(gzip.open(f, \"rt\"))\n",
    "        for rec in payload.get(\"data\", []):\n",
    "           rec[\"publishSP\"] = sp\n",
    "           rows.append(rec)\n",
    "    return pd.DataFrame.from_records(rows)\n",
    "\n",
    "def b_windfor(df):\n",
    "    \"\"\"\n",
    "    Intraday wind forecasts with 8 columns f_1…f_8,\n",
    "    where f_1 is the latest run ≥1h before delivery,\n",
    "    f_2 the next‐latest, …, f_8 the 8th‐latest.\n",
    "    \"\"\"\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\"] + [f\"f_{i}\" for i in range(1,9)]\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=want)\n",
    "\n",
    "    # 1) parse\n",
    "    df[\"publishTimeUTC\"] = pd.to_datetime(df[\"publishTime\"], utc=True)\n",
    "    df[\"startTime\"]      = pd.to_datetime(df[\"startTime\"],   utc=True)\n",
    "\n",
    "    # 2) gate‐closure: only keep runs published ≥1h before delivery\n",
    "    df = df[df[\"publishTimeUTC\"] <= df[\"startTime\"] - pd.Timedelta(hours=1)]\n",
    "\n",
    "    # 3) sort by slot & publishTime descending\n",
    "    df = df.sort_values(\n",
    "        [\"settlementDate\",\"settlementPeriod\",\"startTime\",\"publishTimeUTC\"],\n",
    "        ascending=[True, True, True, False]\n",
    "    )\n",
    "\n",
    "    # 4) rank each slot’s forecasts 1…n by recency\n",
    "    df[\"rank\"] = (\n",
    "        df\n",
    "        .groupby([\"settlementDate\",\"settlementPeriod\",\"startTime\"])\n",
    "        .cumcount()\n",
    "        + 1\n",
    "    )\n",
    "\n",
    "    # 5) keep only the top 8 for each slot\n",
    "    df8 = df[df[\"rank\"] <= 8]\n",
    "\n",
    "    # 6) pivot so rank → f_<rank>\n",
    "    wide = (\n",
    "        df8.pivot_table(\n",
    "            index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "            columns=\"rank\",\n",
    "            values=\"generation\",\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 7) rename and ensure f_1…f_8 exist\n",
    "    for i in range(1,9):\n",
    "        col = f\"f_{i}\"\n",
    "        if i in wide.columns:\n",
    "            wide = wide.rename(columns={i: col})\n",
    "        else:\n",
    "            wide[col] = pd.NA\n",
    "\n",
    "    # 8) rebuild startTime (UTC→naive half‐hour)\n",
    "    wide[\"startTime\"] = build_start_time(wide)\n",
    "\n",
    "    # 9) finish (dtype coercion, calendar padding, dedupe, sort)\n",
    "    return _finish(wide[want], want)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "GEN_MAP = {\n",
    "    \"Hydro Pumped Storage\":\"hydroPumpedStorage\",\n",
    "    \"Fossil Hard coal\"    :\"fossilCoal\",\n",
    "    \"Fossil Gas\"          :\"fossilGas\",\n",
    "    \"Fossil Oil\"          :\"fossilOil\",\n",
    "    \"Nuclear\"             :\"nuclear\",\n",
    "    \"Other\"               :\"other\",\n",
    "    \"Wind Onshore\"        :\"windOnshore\",\n",
    "    \"Wind Offshore\"       :\"windOffshore\",\n",
    "    \"Solar\"               :\"solar\",\n",
    "}\n",
    "def b_gen_per_type(df):\n",
    "    df = (df.replace({\"psrType\": GEN_MAP})\n",
    "            .pivot_table(index=[\"settlementDate\",\"settlementPeriod\",\"startTime\"],\n",
    "                         columns=\"psrType\",\n",
    "                         values=\"quantity\", aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    for col in GEN_MAP.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    want = [\"startTime\",\"settlementDate\",\"settlementPeriod\"]+list(GEN_MAP.values())\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_dayahead_demand(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"transmissionSystemDemand\", \"nationalDemand\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_indicated(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"indicatedGeneration\", \"indicatedDemand\",\n",
    "            \"indicatedMargin\", \"indicatedImbalance\"]\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "IC_NAME_TO_COL = {\n",
    "    \"Eleclink (INTELEC)\"      : \"INTELEC\",\n",
    "    \"Ireland(East-West)\"      : \"INTEW\",\n",
    "    \"France(IFA)\"             : \"INTFR\",\n",
    "    \"Ireland (Greenlink)\"     : \"INTGRNL\",\n",
    "    \"IFA2 (INTIFA2)\"          : \"INTIFA2\",\n",
    "    \"Northern Ireland(Moyle)\" : \"INTIRL\",\n",
    "    \"Netherlands(BritNed)\"    : \"INTNED\",\n",
    "    \"Belgium (Nemolink)\"      : \"INTNEM\",\n",
    "    \"North Sea Link (INTNSL)\" : \"INTNSL\",\n",
    "    \"Denmark (Viking link)\"   : \"INTVKL\",\n",
    "}\n",
    "\n",
    "def b_inter(df):\n",
    "    df = df.replace({\"interconnectorName\": IC_NAME_TO_COL})\n",
    "\n",
    "    df = (df.pivot_table(index=[\"settlementDate\", \"settlementPeriod\", \"startTime\"],\n",
    "                         columns=\"interconnectorName\",\n",
    "                         values=\"generation\",\n",
    "                         aggfunc=\"first\")\n",
    "            .reset_index())\n",
    "\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    for col in IC_NAME_TO_COL.values():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    \n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\"] + list(IC_NAME_TO_COL.values())\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_mid(df):\n",
    "    df = df.loc[df[\"dataProvider\"] == \"APXMIDP\"].copy()\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"price\", \"volume\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_nonbm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"generation\"]\n",
    "    return _finish(df[want], want)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "HORIZONS = [1, 2, 4, 8, 12]\n",
    "\n",
    "def b_lolpdrm(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    # keep only horizons we care about\n",
    "    df = df.loc[df[\"forecastHorizon\"].isin(HORIZONS),\n",
    "                [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "                 \"forecastHorizon\", \"lossOfLoadProbability\",\n",
    "                 \"deratedMargin\"]]\n",
    "\n",
    "    # ----------  LOLP (horizon 1)  ----------\n",
    "    lolp = (df[df[\"forecastHorizon\"] == 1]\n",
    "              .rename(columns={\"lossOfLoadProbability\": \"1hLOLP\"})\n",
    "              .loc[:, [\"startTime\", \"settlementDate\",\n",
    "                       \"settlementPeriod\", \"1hLOLP\"]])\n",
    "\n",
    "    # ----------  DRM (pivot all horizons)  ----------\n",
    "    drm = (df.pivot_table(index=[\"startTime\", \"settlementDate\",\n",
    "                                 \"settlementPeriod\"],\n",
    "                          columns=\"forecastHorizon\",\n",
    "                          values=\"deratedMargin\")\n",
    "             .rename(columns={h: f\"{h}hDRM\" for h in HORIZONS})\n",
    "             .reset_index())\n",
    "\n",
    "    # ----------  merge & order columns  ----------\n",
    "    out = lolp.merge(drm, on=[\"startTime\", \"settlementDate\",\n",
    "                              \"settlementPeriod\"])\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"1hLOLP\", \"1hDRM\"]\n",
    "    out = out[want]\n",
    "    return _finish(out, want)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def b_system_prices(df):\n",
    "    df[\"startTime\"] = build_start_time(df)\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"systemSellPrice\"    : \"systemPrice\",      # SSP / SBP\n",
    "        \"netImbalanceVolume\" : \"netImbalanceVolume\"\n",
    "    })\n",
    "\n",
    "    want = [\"startTime\", \"settlementDate\", \"settlementPeriod\",\n",
    "            \"systemPrice\", \"netImbalanceVolume\",\n",
    "            \"sellPriceAdjustment\", \"buyPriceAdjustment\",\n",
    "            \"replacementPrice\", \"replacementPriceReferenceVolume\",\n",
    "            \"totalAcceptedOfferVolume\", \"totalAcceptedBidVolume\",\n",
    "            \"totalAdjustmentSellVolume\", \"totalAdjustmentBuyVolume\",\n",
    "            \"totalSystemTaggedAcceptedOfferVolume\",\n",
    "            \"totalSystemTaggedAcceptedBidVolume\",\n",
    "            \"totalSystemTaggedAdjustmentSellVolume\",\n",
    "            \"totalSystemTaggedAdjustmentBuyVolume\"]\n",
    "\n",
    "    # create any missing columns so _finish keeps dtype order\n",
    "    for col in want:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    out = df[want]\n",
    "    return _finish(out, want)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "652108b0-5e7b-46c7-abe5-79a299986eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DAYAHEAD_GEN_WIND_SOLAR: 148,942 rows → bmrs_csv_raw/DAYAHEAD_GEN_WIND_SOLAR.csv\n",
      "✓ INTRADAYPROCESS_GEN_WIND_SOLAR: 148,942 rows → bmrs_csv_raw/INTRADAYPROCESS_GEN_WIND_SOLAR.csv\n",
      "✓ INTRADAYTOTAL_GEN_WIND_SOLAR: 148,942 rows → bmrs_csv_raw/INTRADAYTOTAL_GEN_WIND_SOLAR.csv\n"
     ]
    }
   ],
   "source": [
    "# BUILDERS = {\n",
    "#     \"ACTUAL_DEMAND\"                  : b_actual_demand,\n",
    "#     \"ACTUAL_GEN_WIND_SOLAR\"          : b_actual_gen_ws,\n",
    "#     \"DAYAHEAD_DEMAND\"                : b_dayahead_demand,\n",
    "#     \"DETAILED_WINDFOR\"               : b_windfor,\n",
    "#     \"DAYAHEAD_GEN_WIND_SOLAR\"        : b_dayahead_gen_ws,\n",
    "#     \"INTRADAYPROCESS_GEN_WIND_SOLAR\" : b_intradayprocess_gen_ws,\n",
    "#     \"INTRADAYTOTAL_GEN_WIND_SOLAR\"   : b_intradaytotal_gen_ws,\n",
    "#     \"GEN_PER_TYPE\"                   : b_gen_per_type,\n",
    "#     \"INDICATED_DAYAHEAD_DEMAND\"      : b_indicated,\n",
    "#     \"INTER\"                          : b_inter,\n",
    "#     \"LOLPDRM\"                        : b_lolpdrm,\n",
    "#     \"NONBM\"                          : b_nonbm,\n",
    "#     \"MID\"                            : b_mid,\n",
    "#     \"SYSTEM_PRICES\"                  : b_system_prices,\n",
    "# }\n",
    "\n",
    "BUILDERS = {\n",
    "    \"DAYAHEAD_GEN_WIND_SOLAR\"        : b_dayahead_gen_ws,\n",
    "    \"INTRADAYPROCESS_GEN_WIND_SOLAR\" : b_intradayprocess_gen_ws,\n",
    "    \"INTRADAYTOTAL_GEN_WIND_SOLAR\"   : b_intradaytotal_gen_ws,\n",
    "}\n",
    "\n",
    "\n",
    "def process_one(code: str, builder):\n",
    "    folder = RAW_DIR / code\n",
    "    if code == \"DETAILED_WINDFOR\":\n",
    "        df_raw = _folder_to_df_detailed(folder)\n",
    "    else:\n",
    "        df_raw = _folder_to_df(folder)\n",
    "    if df_raw.empty:\n",
    "        print(f\"⚠ {code}: empty → skipped\")\n",
    "        return None\n",
    "    df_tidy = builder(df_raw)\n",
    "    out = CSV_DIR / f\"{code}.csv\"\n",
    "    df_tidy.to_csv(out, index=False)\n",
    "    print(f\"✓ {code}: {len(df_tidy):,} rows → {out}\")\n",
    "    return df_tidy\n",
    "\n",
    "\n",
    "def main():\n",
    "    # # placeholders to capture the two dataframes\n",
    "    # actual_ws   = None\n",
    "    # gen_per_type = None\n",
    "\n",
    "    # 1) run all builders and write CSVs\n",
    "    for code, builder in BUILDERS.items():\n",
    "        result = process_one(code, builder)\n",
    "        # if code == \"ACTUAL_GEN_WIND_SOLAR\":\n",
    "        #     actual_ws = result.copy() if result is not None else None\n",
    "        # elif code == \"GEN_PER_TYPE\":\n",
    "        #     gen_per_type = result.copy() if result is not None else None\n",
    "\n",
    "    # # 2) mutual fill between those two\n",
    "    # if actual_ws is not None and gen_per_type is not None:\n",
    "    #     for gen_col, act_col in [\n",
    "    #         (\"windOffshore\",           \"windOffshoreGeneration\"),\n",
    "    #         (\"windOnshore\",            \"windOnshoreGeneration\"),\n",
    "    #         (\"solar\",                  \"solarGeneration\")\n",
    "    #     ]:\n",
    "    #         # fill actual from gen_per_type\n",
    "    #         actual_ws[act_col] = actual_ws[act_col].combine_first(\n",
    "    #                                  gen_per_type[gen_col]\n",
    "    #                              )\n",
    "    #         # fill gen_per_type from actual\n",
    "    #         gen_per_type[gen_col] = gen_per_type[gen_col].combine_first(\n",
    "    #                                     actual_ws[act_col]\n",
    "    #                                 )\n",
    "\n",
    "    #     # 3) overwrite the two CSVs\n",
    "    #     actual_ws.to_csv(CSV_DIR/\"ACTUAL_GEN_WIND_SOLAR.csv\", index=False)\n",
    "    #     gen_per_type.to_csv(CSV_DIR/\"GEN_PER_TYPE.csv\",       index=False)\n",
    "    #     print(\"↺ Mutual fill applied to ACTUAL_GEN_WIND_SOLAR and GEN_PER_TYPE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dde4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> raw df shape: (562254, 8)\n",
      "   processType values: ['Day ahead' 'Intraday process' 'Intraday total']\n",
      "   psrType values:     ['Wind Offshore' 'Solar' 'Wind Onshore']\n",
      "   – selecting processType == 'Day ahead': 431424 rows\n",
      "   psrType counts in filtered: {'Wind Offshore': 143808, 'Solar': 143808, 'Wind Onshore': 143808}\n",
      "   after pivot: (143808, 7)\n",
      "   pivot columns: ['settlementDate', 'settlementPeriod', 'startTime', 'publishTime', 'Solar', 'Wind Offshore', 'Wind Onshore']\n",
      "   – missing column windOnshoreGeneration, inserting NaNs\n",
      "   – missing column windOffshoreGeneration, inserting NaNs\n",
      "   – missing column solarGeneration, inserting NaNs\n",
      "→ debug day-ahead:\n",
      " psrType settlementDate  settlementPeriod                 startTime  \\\n",
      "0           2017-01-01                 1 2017-01-01 00:00:00+00:00   \n",
      "1           2017-01-01                 2 2017-01-01 00:30:00+00:00   \n",
      "2           2017-01-01                 3 2017-01-01 01:00:00+00:00   \n",
      "3           2017-01-01                 4 2017-01-01 01:30:00+00:00   \n",
      "4           2017-01-01                 5 2017-01-01 02:00:00+00:00   \n",
      "\n",
      "psrType           publishTime  Solar  Wind Offshore  Wind Onshore  \\\n",
      "0        2016-12-31T16:45:10Z    0.0       2783.344      3496.849   \n",
      "1        2016-12-31T16:45:10Z    0.0       2783.344      3496.849   \n",
      "2        2016-12-31T16:45:10Z    0.0       2830.334      3655.056   \n",
      "3        2016-12-31T16:45:10Z    0.0       2830.334      3655.056   \n",
      "4        2016-12-31T16:45:10Z    0.0       2722.812      3821.851   \n",
      "\n",
      "psrType windOnshoreGeneration windOffshoreGeneration solarGeneration  \n",
      "0                        <NA>                   <NA>            <NA>  \n",
      "1                        <NA>                   <NA>            <NA>  \n",
      "2                        <NA>                   <NA>            <NA>  \n",
      "3                        <NA>                   <NA>            <NA>  \n",
      "4                        <NA>                   <NA>            <NA>   (143808, 10)\n",
      "\n",
      ">>> raw df shape: (0, 0)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'processType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ debug day-ahead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, da_debug\u001b[38;5;241m.\u001b[39mhead(), da_debug\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     49\u001b[0m raw_int_pr \u001b[38;5;241m=\u001b[39m _folder_to_df(RAW_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTRADAY_PROCESS_GEN_WIND_SOLAR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m ip_debug \u001b[38;5;241m=\u001b[39m \u001b[43m_pivot_wind_solar_forecast_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_int_pr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIntraday process\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforecast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ debug intraday-process:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, ip_debug\u001b[38;5;241m.\u001b[39mhead(), ip_debug\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     53\u001b[0m raw_int_total \u001b[38;5;241m=\u001b[39m _folder_to_df(RAW_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTRADAY_TOTAL_GEN_WIND_SOLAR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36m_pivot_wind_solar_forecast_debug\u001b[0;34m(df, process_type, value_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pivot_wind_solar_forecast_debug\u001b[39m(df, process_type, value_name):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 1) Before filtering\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>> raw df shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   processType values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   psrType values:    \u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsrType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 2) Filter by process_type\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'processType'"
     ]
    }
   ],
   "source": [
    "def _pivot_wind_solar_forecast_debug(df, process_type, value_name):\n",
    "    # 1) Before filtering\n",
    "    print(f\"\\n>>> raw df shape: {df.shape}\")\n",
    "    print(\"   processType values:\", df[\"processType\"].unique())\n",
    "    print(\"   psrType values:    \", df[\"psrType\"].unique())\n",
    "    \n",
    "    # 2) Filter by process_type\n",
    "    mask = df[\"processType\"] == process_type\n",
    "    print(f\"   – selecting processType == {process_type!r}: {mask.sum()} rows\")\n",
    "    df2 = df.loc[mask].rename(columns={\"quantity\": value_name})\n",
    "    if df2.empty:\n",
    "        print(f\"   !!! no rows match process_type={process_type!r}\")\n",
    "        return pd.DataFrame(columns=[\"settlementDate\",\"settlementPeriod\",\"startTime\",\"publishTime\",\n",
    "                                      \"windOnshoreGeneration\",\"windOffshoreGeneration\",\"solarGeneration\"])\n",
    "    \n",
    "    # 3) Show how many of each PSR you have\n",
    "    print(\"   psrType counts in filtered:\", df2[\"psrType\"].value_counts().to_dict())\n",
    "    \n",
    "    # 4) Pivot\n",
    "    out = (df2\n",
    "           .pivot_table(\n",
    "               index=[\"settlementDate\",\"settlementPeriod\",\"startTime\",\"publishTime\"],\n",
    "               columns=\"psrType\",\n",
    "               values=value_name,\n",
    "               aggfunc=\"first\")\n",
    "           .reset_index())\n",
    "    print(f\"   after pivot: {out.shape}\")\n",
    "    print(\"   pivot columns:\", out.columns.tolist())\n",
    "    \n",
    "    # 5) Rebuild startTime, fill missing cols\n",
    "    out[\"startTime\"] = build_start_time(out)\n",
    "    for psr, col in {\n",
    "        \"Wind Onshore\": \"windOnshoreGeneration\",\n",
    "        \"Wind Offshore\": \"windOffshoreGeneration\",\n",
    "        \"Solar\":         \"solarGeneration\"\n",
    "    }.items():\n",
    "        if col not in out.columns:\n",
    "            print(f\"   – missing column {col}, inserting NaNs\")\n",
    "            out[col] = pd.NA\n",
    "\n",
    "    return out\n",
    "\n",
    "# then, for each builder, do:\n",
    "\n",
    "raw = _folder_to_df(RAW_DIR/\"DAYAHEAD_GEN_WIND_SOLAR\")\n",
    "da_debug = _pivot_wind_solar_forecast_debug(raw, \"Day ahead\", \"forecast\")\n",
    "print(\"→ debug day-ahead:\\n\", da_debug.head(), da_debug.shape)\n",
    "\n",
    "raw_int_pr = _folder_to_df(RAW_DIR/\"INTRADAY_PROCESS_GEN_WIND_SOLAR\")\n",
    "ip_debug = _pivot_wind_solar_forecast_debug(raw_int_pr, \"Intraday process\", \"forecast\")\n",
    "print(\"→ debug intraday-process:\\n\", ip_debug.head(), ip_debug.shape)\n",
    "\n",
    "raw_int_total = _folder_to_df(RAW_DIR/\"INTRADAY_TOTAL_GEN_WIND_SOLAR\")\n",
    "it_debug = _pivot_wind_solar_forecast_debug(raw_int_total, \"Intraday total\", \"forecast\")\n",
    "print(\"→ debug intraday-total:\\n\", it_debug.head(), it_debug.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302ef00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
