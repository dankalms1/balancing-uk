{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799ef215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    x = torch.ones(1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fe1a1",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79565a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Historical df loaded, length = 70126\n",
      "                     month_idx  weekday_idx  sp_idx  dtype_idx\n",
      "startTime                                                     \n",
      "2021-07-01 00:00:00          6            3       2          0\n",
      "2021-07-01 00:30:00          6            3       3          0\n",
      "2021-07-01 01:00:00          6            3       4          0\n",
      "2021-07-01 01:30:00          6            3       5          0\n",
      "2021-07-01 02:00:00          6            3       6          0\n",
      "[DEBUG] DEMAND_FORECASTS: loaded forecast df, length = 70126\n",
      "[DEBUG] DEMAND_FORECASTS: existing columns = ['demand_f1', 'demand_f2', 'demand_f3', 'demand_f4', 'demand_f5']... (total 48)\n",
      "[DEBUG] DEMAND_FORECASTS: matrix shape = (70126, 48)\n",
      "[DEBUG] WIND_FORECASTS: loaded forecast df, length = 70126\n",
      "[DEBUG] WIND_FORECASTS: existing columns = ['wind_f1', 'wind_f2', 'wind_f3', 'wind_f4', 'wind_f5']... (total 48)\n",
      "[DEBUG] WIND_FORECASTS: matrix shape = (70126, 48)\n",
      "[DEBUG] DRM_FORECASTS: loaded forecast df, length = 70126\n",
      "[DEBUG] DRM_FORECASTS: existing columns = ['drm_f1', 'drm_f2', 'drm_f3', 'drm_f4', 'drm_f5']... (total 48)\n",
      "[DEBUG] DRM_FORECASTS: matrix shape = (70126, 48)\n",
      "[DEBUG] X_fut stacked shape = (70126, 48, 3) (should match len(df))\n",
      "train → X_hist (64272, 9), cal (64272, 4), y (64272,), X_fut (64272, 48, 3)\n",
      "val   → X_hist (2928, 9), cal (2928, 4), y (2928,), X_fut (2928, 48, 3)\n",
      "test  → X_hist (2926, 9), cal (2926, 4), y (2926,), X_fut (2926, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "# ─── configuration ─────────────────────────────────────────────────────────\n",
    "df_path        = \"df_all.csv\"\n",
    "forecast_dir   = \"bmrs_csv_filled\"\n",
    "mask_dir       = \"bmrs_csv_masks\"\n",
    "start_date     = \"2021-07-01\"\n",
    "end_date       = \"2025-06-30\"\n",
    "train_end_date = \"2025-03-01\"\n",
    "val_end_date   = \"2025-05-01\"\n",
    "horizon        = 48\n",
    "use_time_feats = False   # False to skip trig-based time features\n",
    "\n",
    "# ─── 1) load & crop historical data ────────────────────────────────────────\n",
    "df = pd.read_csv(df_path, index_col=\"startTime\", parse_dates=True).loc[start_date:end_date]\n",
    "print(f\"[DEBUG] Historical df loaded, length = {len(df)}\")\n",
    "\n",
    "# drop exactly the columns you specified\n",
    "cols_to_drop = [\n",
    "    \"Forecast Wind\",\n",
    "    \"Forecast Solar\",\n",
    "    \"Actual Wind\",\n",
    "    \"Actual Solar\"\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# ─── 2) embedded time features ─────────────────────────────────\n",
    "# Month 0–11\n",
    "df[\"month_idx\"]   = df.index.month - 1\n",
    "# Day-of-week 0–6\n",
    "df[\"weekday_idx\"] = df.index.dayofweek\n",
    "# settlement period 0–47\n",
    "if \"Settlement Period\" in df.columns:\n",
    "    df[\"sp_idx\"] = df[\"Settlement Period\"].astype(int) - 1\n",
    "    df = df.drop(columns=[\"Settlement Period\"], errors=\"ignore\")\n",
    "else:\n",
    "    print(\"[WARN] 'Settlement Period' not found; setting sp_idx to 0\")\n",
    "    df[\"sp_idx\"] = 0\n",
    "# Day-type: weekday=0, weekend=1\n",
    "df[\"dtype_idx\"]   = (df.index.dayofweek >= 5).astype(int)\n",
    "\n",
    "print(df[[\"month_idx\",\"weekday_idx\",\"sp_idx\",\"dtype_idx\"]].head())\n",
    "\n",
    "# ─── 3) load & stack future forecasts ──────────────────────────────────────\n",
    "def load_future(name, prefix, horizon=horizon):\n",
    "    fdf = (\n",
    "        pd.read_csv(f\"{forecast_dir}/{name}.csv\",\n",
    "                    index_col=\"startTime\", parse_dates=True)\n",
    "          .loc[start_date:end_date]\n",
    "    )\n",
    "    # reindex to match historical df exactly\n",
    "    fdf = fdf.reindex(df.index)\n",
    "    print(f\"[DEBUG] {name}: loaded forecast df, length = {len(fdf)}\")\n",
    "\n",
    "    cols = [f\"{prefix}_f{i}\" for i in range(1, horizon+1)]\n",
    "    existing = [c for c in cols if c in fdf.columns]\n",
    "    print(f\"[DEBUG] {name}: existing columns = {existing[:5]}... (total {len(existing)})\")\n",
    "\n",
    "    # fill any missing forecast steps with zero\n",
    "    mat = fdf[existing].fillna(0).values\n",
    "    if mat.shape[1] < horizon:\n",
    "        pad = np.zeros((len(fdf), horizon - mat.shape[1]), dtype=mat.dtype)\n",
    "        mat = np.hstack([mat, pad])\n",
    "        print(f\"[DEBUG] {name}: padded to horizon, new shape = {mat.shape}\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] {name}: matrix shape = {mat.shape}\")\n",
    "    return mat\n",
    "\n",
    "demand_mat = load_future(\"DEMAND_FORECASTS\", \"demand\")\n",
    "wind_mat   = load_future(\"WIND_FORECASTS\",   \"wind\")\n",
    "drm_mat    = load_future(\"DRM_FORECASTS\",    \"drm\")\n",
    "\n",
    "X_fut = np.stack([demand_mat, wind_mat, drm_mat], axis=2)\n",
    "assert X_fut.shape[0] == len(df), \"Future forecasts misaligned with historical df\"\n",
    "print(f\"[DEBUG] X_fut stacked shape = {X_fut.shape} (should match len(df))\")\n",
    "\n",
    "# ─── 4) train/val/test split ───────────────────────────────────────────────\n",
    "target_col = \"Imbalance Price\"\n",
    "assert target_col in df.columns, f\"Target column '{target_col}' not found in df\"\n",
    "\n",
    "# define column groups\n",
    "cal_cols   = [\"month_idx\", \"weekday_idx\", \"sp_idx\", \"dtype_idx\"]\n",
    "hist_cols  = [c for c in df.columns if c not in cal_cols + [target_col]]\n",
    "\n",
    "# masks\n",
    "train_mask = df.index < train_end_date\n",
    "val_mask   = (df.index >= train_end_date) & (df.index < val_end_date)\n",
    "test_mask  = df.index >= val_end_date\n",
    "\n",
    "# slices\n",
    "df_train, df_val, df_test = df[train_mask], df[val_mask], df[test_mask]\n",
    "X_fut_train = X_fut[train_mask]\n",
    "X_fut_val   = X_fut[val_mask]\n",
    "X_fut_test  = X_fut[test_mask]\n",
    "\n",
    "# targets\n",
    "y_train = df_train[target_col].values\n",
    "y_val   = df_val[target_col].values\n",
    "y_test  = df_test[target_col].values\n",
    "\n",
    "# historical features\n",
    "X_train_hist = df_train[hist_cols].values\n",
    "X_val_hist   = df_val[hist_cols].values\n",
    "X_test_hist  = df_test[hist_cols].values\n",
    "\n",
    "# calendar indices\n",
    "month_train   = df_train[\"month_idx\"].values\n",
    "weekday_train = df_train[\"weekday_idx\"].values\n",
    "sp_train      = df_train[\"sp_idx\"].values\n",
    "dtype_train   = df_train[\"dtype_idx\"].values\n",
    "\n",
    "month_val   = df_val[\"month_idx\"].values\n",
    "weekday_val = df_val[\"weekday_idx\"].values\n",
    "sp_val      = df_val[\"sp_idx\"].values\n",
    "dtype_val   = df_val[\"dtype_idx\"].values\n",
    "\n",
    "month_test   = df_test[\"month_idx\"].values\n",
    "weekday_test = df_test[\"weekday_idx\"].values\n",
    "sp_test      = df_test[\"sp_idx\"].values\n",
    "dtype_test   = df_test[\"dtype_idx\"].values\n",
    "\n",
    "print(f\"train → X_hist {X_train_hist.shape}, cal {(month_train.shape[0],4)}, y {y_train.shape}, X_fut {X_fut_train.shape}\")\n",
    "print(f\"val   → X_hist {X_val_hist.shape}, cal {(month_val.shape[0],4)}, y {y_val.shape}, X_fut {X_fut_val.shape}\")\n",
    "print(f\"test  → X_hist {X_test_hist.shape}, cal {(month_test.shape[0],4)}, y {y_test.shape}, X_fut {X_fut_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53608db0",
   "metadata": {},
   "source": [
    "# Hyperparameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f449ca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "\n",
    "# ─── 2) Class Definitions ───────────────────────────────────────────\n",
    "\n",
    "# ──────────── a. Dataset Definition ─────────────────────────────────\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiFeedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset producing:\n",
    "      - x_h: past history covariates (seq_len × n_hist_feats)\n",
    "      - x_f: all known-future covariates (feed_len × n_fut_feats)\n",
    "      - y_t: target values for the next fut_len steps (fut_len,)\n",
    "      - mi, wi, si, di: calendar indices for seq_len+fut_len timestamps\n",
    "    Decouples look‐ahead length (feed_len = l) from forecast horizon (fut_len = τ_max).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hist:        torch.Tensor,  # (N, n_hist_feats)\n",
    "                 full_fut:    torch.Tensor,  # (N, feed_len, n_fut_feats)\n",
    "                 y:           torch.Tensor,  # (N,)\n",
    "                 month_idx:   torch.Tensor,  # (N,)\n",
    "                 weekday_idx: torch.Tensor,  # (N,)\n",
    "                 sp_idx:      torch.Tensor,  # (N,)\n",
    "                 dtype_idx:   torch.Tensor,  # (N,)\n",
    "                 seq_len:     int,\n",
    "                 feed_len:    int,\n",
    "                 fut_len:     int):\n",
    "        assert fut_len <= feed_len, \"forecast horizon (fut_len) must be ≤ look-ahead length (feed_len)\"\n",
    "        self.X_hist       = hist.float()           # (N, n_hist_feats)\n",
    "        self.X_fut        = full_fut.float()       # (N, feed_len, n_fut_feats)\n",
    "        self.y_full       = y.float()              # (N,)\n",
    "        self.month_full   = month_idx.long()       # (N,)\n",
    "        self.weekday_full = weekday_idx.long()     # (N,)\n",
    "        self.sp_full      = sp_idx.long()          # (N,)\n",
    "        self.dtype_full   = dtype_idx.long()       # (N,)\n",
    "        self.seq_len      = seq_len\n",
    "        self.feed_len     = feed_len\n",
    "        self.fut_len      = fut_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of valid windows = N – seq_len – fut_len + 1\n",
    "        return self.X_hist.size(0) - self.seq_len - self.fut_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) history window [idx … idx+seq_len-1]\n",
    "        x_h = self.X_hist[idx : idx + self.seq_len]                      # (seq_len, n_hist_feats)\n",
    "\n",
    "        # 2) known-future covariates all the way out to feed_len\n",
    "        anchor = idx + self.seq_len - 1\n",
    "        x_f    = self.X_fut[anchor, : self.feed_len, :]                  # (feed_len, n_fut_feats)\n",
    "\n",
    "        # 3) targets for the next fut_len steps [idx+seq_len … idx+seq_len+fut_len-1]\n",
    "        start_y = idx + self.seq_len\n",
    "        end_y   = start_y + self.fut_len\n",
    "        y_t     = self.y_full[start_y : end_y]                          # (fut_len,)\n",
    "\n",
    "        # 4) calendar indices for the full window [idx … idx+seq_len+fut_len-1]\n",
    "        ci_start = idx\n",
    "        ci_end   = idx + self.seq_len + self.fut_len\n",
    "        mi = self.month_full[ci_start : ci_end]                         # (seq_len+fut_len,)\n",
    "        wi = self.weekday_full[ci_start : ci_end]\n",
    "        si = self.sp_full[ci_start : ci_end]\n",
    "        di = self.dtype_full[ci_start : ci_end]\n",
    "\n",
    "        return x_h, x_f, y_t, mi, wi, si, di\n",
    "\n",
    "\n",
    "# ──────────── b. Layer Definitions ──────────────────────────────────\n",
    "    \n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_month = nn.Embedding(12, 4)  # 12 months, 4-dim embedding\n",
    "        self.emb_day   = nn.Embedding(7, 3)   # 7 days, 3-dim embedding\n",
    "        self.emb_sp    = nn.Embedding(48, 6)  # 48 settlement periods, 6-dim embedding\n",
    "        self.emb_dtype = nn.Embedding(2, 2)  # 2 types (weekday, weekend), 2-dim embedding\n",
    "\n",
    "    def forward(self, month_idx, day_idx, sp_idx, dtype_idx):\n",
    "        # Inputs are LongTensors of shape (B, T)\n",
    "        e1 = self.emb_month(month_idx)  \n",
    "        e2 = self.emb_day(day_idx)\n",
    "        e3 = self.emb_sp(sp_idx)\n",
    "        e4 = self.emb_dtype(dtype_idx)\n",
    "        emb = torch.cat((e1, e2, e3, e4), dim=-1)  # Concatenate along the last dimension\n",
    "        return emb\n",
    "\n",
    "class VariableSelection(nn.Module):\n",
    "    \"\"\"\n",
    "    Weights each feature of the concatenated input vector to focus on the most relevant features\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Input dim = measured_features + calendar_features\n",
    "                 or forecast_features + calendar_features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Projects the input to the same dimension\n",
    "        self.proj = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        # nn.init.eye_(self.proj.weight)\n",
    "        # nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, T, input_dim)\n",
    "        returns: Tensor of same shape, but with features reweighted\n",
    "        \"\"\"\n",
    "        # Project to scores\n",
    "        z = self.proj(x)\n",
    "        # Normalise into weights\n",
    "        w = torch.softmax(z, dim=-1)  # softmax along feature dim\n",
    "        # Re-weight inputs\n",
    "        x_weighted = w * x\n",
    "        return x_weighted\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A bidirectional LSTM encoder that maps input sequences of shape\n",
    "    (B, T, input_dim) → (B, T, 2*h).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_size=64, num_layers=1, dropout=0.0):\n",
    "        \"\"\"\n",
    "        input_dim   : dimension of each x_t (e.g. m_h + 15 or m_f + 15)\n",
    "        hidden_size : h, the per-direction LSTM hidden size\n",
    "        num_layers  : how many stacked LSTM layers\n",
    "        dropout     : dropout probability between LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_dim,\n",
    "            hidden_size= hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0.0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, T, input_dim)\n",
    "        hidden: optional tuple (h0, c0) each of shape\n",
    "                (num_layers*2, B, hidden_size)\n",
    "        returns:\n",
    "          H : (B, T, 2*hidden_size)  # all time-step outputs\n",
    "          (h_n, c_n) : final states if you ever need them\n",
    "        \"\"\"\n",
    "        # If no initial state provided, PyTorch defaults to zeros\n",
    "        H, (h_n, c_n) = self.lstm(x, hidden)\n",
    "        # H is (B, T, 2*h)\n",
    "        return H, (h_n, c_n)\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, att_dim):\n",
    "        \"\"\"\n",
    "        enc_dim : dimension of encoder states (2*h)\n",
    "        dec_dim : dimension of decoder hidden state (d_dec)\n",
    "        att_dim : size of the attention projection (d_att)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(enc_dim, att_dim, bias=False)\n",
    "        self.U = nn.Linear(dec_dim, att_dim, bias=True)\n",
    "        self.v = nn.Linear(att_dim, 1,       bias=False)\n",
    "\n",
    "    def forward(self, H, s_prev, mask=None):\n",
    "        \"\"\"\n",
    "        H       : (B, T, enc_dim)     encoder outputs\n",
    "        s_prev  : (B, dec_dim)        decoder hidden state\n",
    "        mask    : (B, T) optional (0 for pad, 1 for data)\n",
    "        Returns:\n",
    "          c     : (B, enc_dim)        context vector\n",
    "          alpha : (B, T)              attention weights\n",
    "        \"\"\"\n",
    "        # 1) Project encoder states\n",
    "        #    → (B, T, att_dim)\n",
    "        H_proj = self.W(H)\n",
    "\n",
    "        # 2) Project decoder state, unsqueeze to match T\n",
    "        #    → (B, 1, att_dim) → (B, T, att_dim)\n",
    "        S_proj = self.U(s_prev).unsqueeze(1)\n",
    "        S_proj = S_proj.expand_as(H_proj)\n",
    "\n",
    "        # 3) Combine & nonlinearity\n",
    "        E = torch.tanh(H_proj + S_proj)      # (B, T, att_dim)\n",
    "\n",
    "        # 4) Score with v → (B, T, 1), then squeeze → (B, T)\n",
    "        e = self.v(E).squeeze(-1)\n",
    "\n",
    "        # 5) Masking (if provided)\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 6) Normalize scores → weights\n",
    "        alpha = F.softmax(e, dim=1)         # (B, T)\n",
    "\n",
    "        # 7) Compute context as weighted sum\n",
    "        #    Expand alpha → (B, T, 1) to match H\n",
    "        alpha_exp = alpha.unsqueeze(-1)     # (B, T, 1)\n",
    "        c = (alpha_exp * H).sum(dim=1)       # (B, enc_dim)\n",
    "\n",
    "        return c, alpha\n",
    "\n",
    "\n",
    "class DualLSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Two parallel LSTMCells processing c_hist and c_fut,\n",
    "    then combining their hidden states to predict one-step outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 enc_dim:   int,  # = 2 * lstm_hidden\n",
    "                 dec_hidden:int   # your chosen decoder hidden size\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # LSTM for past‐attention context c_h\n",
    "        self.lstm_h = nn.LSTMCell(input_size=enc_dim,\n",
    "                                  hidden_size=dec_hidden)\n",
    "        # LSTM for future‐attention context c_f\n",
    "        self.lstm_f = nn.LSTMCell(input_size=enc_dim,\n",
    "                                  hidden_size=dec_hidden)\n",
    "        # Combine [s_h; s_f] → one scalar\n",
    "        self.ffn = nn.Linear(2*dec_hidden, 1)\n",
    "\n",
    "    def forward(self,\n",
    "                hist_ctx: torch.Tensor,   # (B, L, enc_dim)\n",
    "                fut_ctx:  torch.Tensor    # (B, L, enc_dim)\n",
    "               ) -> torch.Tensor:          # returns (B, L)\n",
    "        B, L, D = hist_ctx.shape\n",
    "        # initialize both LSTMCell states from zeros (or from encoder as discussed)\n",
    "        h_h = hist_ctx.new_zeros(B, self.lstm_h.hidden_size)\n",
    "        c_h = h_h.clone()\n",
    "        h_f = h_h.clone()\n",
    "        c_f = h_h.clone()\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(L):\n",
    "            # step each decoder\n",
    "            c_h, h_h = self.lstm_h(hist_ctx[:, t, :], (h_h, c_h))\n",
    "            c_f, h_f = self.lstm_f(fut_ctx[:,  t, :], (h_f, c_f))\n",
    "\n",
    "            # combine their hidden states\n",
    "            comb = torch.cat([h_h, h_f], dim=-1)       # (B, 2*dec_hidden)\n",
    "            y_t  = self.ffn(comb).squeeze(-1)          # (B,)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)            # (B, L)\n",
    "\n",
    "# ──────────── c. Model Definitions ──────────────────────────────────\n",
    "\n",
    "class BiAttnPointForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    5-stage bi-attentional forecaster with:\n",
    "      - hist look-back k = hist_len\n",
    "      - fut look-ahead l = feed_len\n",
    "      - forecast horizon τ_max = fut_len\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_hist_feats,\n",
    "                 num_fut_feats,\n",
    "                 num_time_feats,\n",
    "                 lstm_hidden,\n",
    "                 dec_hidden,\n",
    "                 attn_dim,\n",
    "                 hist_len,\n",
    "                 feed_len,    # ← new\n",
    "                 fut_len):    # ← renamed τ_max\n",
    "        super().__init__()\n",
    "        self.hist_len  = hist_len\n",
    "        self.feed_len  = feed_len\n",
    "        self.fut_len   = fut_len\n",
    "\n",
    "        # 1) Time feature embedding\n",
    "        self.time_embed = TimeFeatureEmbedding()\n",
    "\n",
    "        # 2) Variable selection\n",
    "        self.var_select_past   = VariableSelection(num_hist_feats  + num_time_feats)\n",
    "        self.var_select_future = VariableSelection(num_fut_feats   + num_time_feats)\n",
    "\n",
    "        # 3) Bidirectional LSTM encoders\n",
    "        self.enc_hist = BiLSTMEncoder(input_dim=num_hist_feats  + num_time_feats, hidden_size=lstm_hidden)\n",
    "        self.enc_fut  = BiLSTMEncoder(input_dim=num_fut_feats   + num_time_feats, hidden_size=lstm_hidden)\n",
    "\n",
    "        # 4) Dual additive attention\n",
    "        enc_dim = 2 * lstm_hidden\n",
    "        self.attn_hist = AdditiveAttention(enc_dim=enc_dim, dec_dim=dec_hidden, att_dim=attn_dim)\n",
    "        self.attn_fut  = AdditiveAttention(enc_dim=enc_dim, dec_dim=dec_hidden, att_dim=attn_dim)\n",
    "\n",
    "        # 4a) Initial state projection\n",
    "        self.init_h = nn.Linear(enc_dim, dec_hidden)\n",
    "        self.init_c = nn.Linear(enc_dim, dec_hidden)\n",
    "\n",
    "        # 5) Decoder\n",
    "        self.decoder = DualLSTMDecoder(enc_dim=2*lstm_hidden, dec_hidden=dec_hidden)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                cont_hist,    # (B, hist_len, num_hist_feats)\n",
    "                cont_fut,     # (B, feed_len, num_fut_feats)\n",
    "                month_idx,    # (B, hist_len+feed_len)\n",
    "                weekday_idx,  # (B, hist_len+feed_len)\n",
    "                sp_idx,       # (B, hist_len+feed_len)\n",
    "                dtype_idx,    # (B, hist_len+feed_len)\n",
    "                mask_hist=None,\n",
    "                mask_fut=None  # (B, feed_len)\n",
    "               ) -> torch.Tensor:  # returns (B, fut_len)\n",
    "        B = cont_hist.size(0)\n",
    "\n",
    "        # 1) Embed all calendar features in one go (length hist_len+feed_len)\n",
    "        emb = self.time_embed(month_idx, weekday_idx, sp_idx, dtype_idx)\n",
    "        emb_hist = emb[:, :self.hist_len, :]                  # (B, hist_len,  time_dim)\n",
    "        emb_fut  = emb[:, self.hist_len : self.hist_len+self.feed_len, :]  # (B, feed_len, time_dim)\n",
    "\n",
    "        # 2) Variable‐selection\n",
    "        x_hist = torch.cat([cont_hist, emb_hist], dim=-1)     # (B, hist_len,  m_h+time_dim)\n",
    "        x_hist = self.var_select_past(x_hist)                 # same shape\n",
    "\n",
    "        x_fut  = torch.cat([cont_fut, emb_fut], dim=-1)       # (B, feed_len, m_f+time_dim)\n",
    "        x_fut  = self.var_select_future(x_fut)\n",
    "\n",
    "        # 3) Bidir LSTM encode\n",
    "        H_hist, (h_n, c_n) = self.enc_hist(x_hist)            # (B, hist_len, 2*h)\n",
    "        H_fut,  _         = self.enc_fut(x_fut)               # (B, feed_len, 2*h)\n",
    "\n",
    "        # 3a) Seed decoder from final hist-encoder state\n",
    "        h_fwd, h_bwd = h_n[-2], h_n[-1]   # each (B, h)\n",
    "        c_fwd, c_bwd = c_n[-2], c_n[-1]\n",
    "        h_cat = torch.cat([h_fwd, h_bwd], dim=-1)  # (B, 2*h)\n",
    "        c_cat = torch.cat([c_fwd, c_bwd], dim=-1)\n",
    "        h_h = self.init_h(h_cat);  c_h = self.init_c(c_cat)\n",
    "        h_f = h_h.clone();        c_f = c_h.clone()\n",
    "\n",
    "        # 4&5) Decode τ_max steps, attending over full look-ahead l each time\n",
    "        outputs = []\n",
    "        for t in range(self.fut_len):\n",
    "            ctx_h, _ = self.attn_hist(H_hist, h_h, mask=mask_hist)\n",
    "            ctx_f, _ = self.attn_fut( H_fut,  h_f, mask=mask_fut)  # attends over all l\n",
    "\n",
    "            h_h, c_h = self.decoder.lstm_h(ctx_h, (h_h, c_h))\n",
    "            h_f, c_f = self.decoder.lstm_f(ctx_f, (h_f, c_f))\n",
    "\n",
    "            comb = torch.cat([h_h, h_f], dim=-1)\n",
    "            y_t  = self.decoder.ffn(comb).squeeze(-1)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)  # (B, fut_len)\n",
    "\n",
    "\n",
    "# ─── 3) Device ───────────────────────────────────────────────────────\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ─── 4) Time‐feature lists and scaler ────────────────────────────────\n",
    "\n",
    "\n",
    "# ─── 5) CV Splitter & Hyperparameters ───────────────────────────────\n",
    "\n",
    "TRANSFORMER_FACTORY = {\n",
    "    \"MinMax\": MinMaxScaler,\n",
    "    \"Robust\": RobustScaler,\n",
    "    \"Standard\": StandardScaler,\n",
    "    \"MaxAbs\": MaxAbsScaler\n",
    "}\n",
    "MODEL_FACTORY = {\n",
    "    \"BiAttnPointForecaster\": BiAttnPointForecaster\n",
    "}\n",
    "LOSS_FACTORY = {\n",
    "    \"MAE\": nn.L1Loss,\n",
    "    \"MSE\": nn.MSELoss,\n",
    "    \"Huber\": nn.SmoothL1Loss\n",
    "}\n",
    "\n",
    "# look-back, look-ahead, forecast horizon\n",
    "seq_len   = 24\n",
    "feed_len  = 24\n",
    "fut_len   = 24 \n",
    "\n",
    "# model widths\n",
    "lstm_hidden = 24  \n",
    "dec_hidden  = 12  \n",
    "attn_dim    = 24\n",
    "num_layers  = 1   \n",
    "dropout     = 0.0\n",
    "\n",
    "# training\n",
    "batch_size = 48       \n",
    "lr         = 1e-4     \n",
    "patience   = 20       \n",
    "max_epochs = 200      \n",
    "scaler_used = \"MaxAbs\"\n",
    "model_used  = \"BiAttnPointForecaster\"\n",
    "loss_used   = \"Huber\"\n",
    "beta        = 0.01 # For Huber loss only\n",
    "notes       = None\n",
    "\n",
    "# ─── 6) Model Tag & Directory ────────────────────────────────────────         \n",
    "\n",
    "md = {\n",
    "    \"model\":        model_used,\n",
    "    \"seq_len\":      seq_len,\n",
    "    \"feed_len\":     feed_len,\n",
    "    \"horizon\":      fut_len,\n",
    "    \"lstm_hidden\":  lstm_hidden,\n",
    "    \"dec_hidden\":   dec_hidden,\n",
    "    \"num_layers\":   num_layers,\n",
    "    \"batch_size\":   batch_size,\n",
    "    \"learning_rate\":lr,\n",
    "    \"max_epochs\":   max_epochs,\n",
    "    \"patience\":     patience,\n",
    "    \"scaler\":       scaler_used,\n",
    "    \"loss\":         loss_used,\n",
    "    **({\"notes\": notes} if notes is not None else {}),\n",
    "}\n",
    "\n",
    "initials = lambda s: \"\".join(w[0] for w in s.split(\"_\"))\n",
    "\n",
    "parts = []\n",
    "for k, v in md.items():\n",
    "    if k in (\"model\", \"scaler\", \"loss\", \"notes\"):\n",
    "        s = str(v)\n",
    "    else:\n",
    "        s = str(v)\n",
    "        if isinstance(v, float) and s.startswith(\"0.\"):\n",
    "            s = s.replace(\"0.\", \".\")\n",
    "        s = f\"{initials(k)}{s}\"\n",
    "    parts.append(s)\n",
    "\n",
    "tag = \"_\".join(parts)\n",
    "\n",
    "models_root = \"models\"\n",
    "candidate   = os.path.join(models_root, tag)\n",
    "version     = 0\n",
    "while os.path.exists(candidate):\n",
    "    version   += 1\n",
    "    candidate = f\"{os.path.join(models_root, tag)}_v{version}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90918d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d75d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Saving run in: models/BiAttnPointForecaster_sl24_fl24_h24_lh24_dh12_nl1_bs48_lr.0001_me200_p20_MaxAbs_Huber_v1\n",
      "[Epoch 001] train=0.01302  val=0.00389\n",
      "[Epoch 002] train=0.01035  val=0.00423\n",
      "[Epoch 003] train=0.00905  val=0.00420\n",
      "[Epoch 004] train=0.00881  val=0.00400\n",
      "[Epoch 005] train=0.00868  val=0.00395\n",
      "[Epoch 006] train=0.00858  val=0.00375\n",
      "[Epoch 007] train=0.00848  val=0.00373\n",
      "[Epoch 008] train=0.00841  val=0.00387\n",
      "[Epoch 009] train=0.00836  val=0.00383\n",
      "[Epoch 010] train=0.00831  val=0.00401\n",
      "[Epoch 011] train=0.00827  val=0.00354\n",
      "[Epoch 012] train=0.00824  val=0.00356\n",
      "[Epoch 013] train=0.00821  val=0.00391\n",
      "[Epoch 014] train=0.00819  val=0.00423\n",
      "[Epoch 015] train=0.00815  val=0.00372\n",
      "[Epoch 016] train=0.00812  val=0.00353\n",
      "[Epoch 017] train=0.00810  val=0.00355\n",
      "[Epoch 018] train=0.00808  val=0.00351\n",
      "[Epoch 019] train=0.00805  val=0.00367\n",
      "[Epoch 020] train=0.00803  val=0.00354\n",
      "[Epoch 021] train=0.00802  val=0.00356\n",
      "[Epoch 022] train=0.00823  val=0.00349\n",
      "[Epoch 023] train=0.00798  val=0.00352\n",
      "[Epoch 024] train=0.00796  val=0.00349\n",
      "[Epoch 025] train=0.00794  val=0.00347\n",
      "[Epoch 026] train=0.00792  val=0.00360\n",
      "[Epoch 027] train=0.00790  val=0.00348\n",
      "[Epoch 028] train=0.00789  val=0.00386\n",
      "[Epoch 029] train=0.00787  val=0.00341\n",
      "[Epoch 030] train=0.00786  val=0.00342\n",
      "[Epoch 031] train=0.00784  val=0.00343\n",
      "[Epoch 032] train=0.00782  val=0.00345\n",
      "[Epoch 033] train=0.00781  val=0.00343\n",
      "[Epoch 034] train=0.00781  val=0.00340\n",
      "[Epoch 035] train=0.00779  val=0.00370\n",
      "[Epoch 036] train=0.00778  val=0.00335\n",
      "[Epoch 037] train=0.00776  val=0.00340\n",
      "[Epoch 038] train=0.00774  val=0.00348\n",
      "[Epoch 039] train=0.00775  val=0.00339\n",
      "[Epoch 040] train=0.00773  val=0.00341\n",
      "[Epoch 041] train=0.00772  val=0.00348\n",
      "→ LR reduced from 1.00e-04 to 5.00e-05\n",
      "[Epoch 042] train=0.00771  val=0.00363\n",
      "[Epoch 043] train=0.00768  val=0.00341\n",
      "[Epoch 044] train=0.00768  val=0.00330\n",
      "[Epoch 045] train=0.00767  val=0.00332\n",
      "[Epoch 046] train=0.00766  val=0.00332\n",
      "[Epoch 047] train=0.00766  val=0.00372\n",
      "[Epoch 048] train=0.00766  val=0.00333\n",
      "[Epoch 049] train=0.00765  val=0.00349\n",
      "→ LR reduced from 5.00e-05 to 2.50e-05\n",
      "[Epoch 050] train=0.00767  val=0.00333\n",
      "[Epoch 051] train=0.00763  val=0.00331\n",
      "[Epoch 052] train=0.00763  val=0.00337\n",
      "[Epoch 053] train=0.00763  val=0.00330\n",
      "[Epoch 054] train=0.00762  val=0.00341\n",
      "[Epoch 055] train=0.00762  val=0.00329\n",
      "[Epoch 056] train=0.00762  val=0.00329\n",
      "[Epoch 057] train=0.00777  val=0.00330\n",
      "[Epoch 058] train=0.00761  val=0.00332\n",
      "[Epoch 059] train=0.00760  val=0.00327\n",
      "[Epoch 060] train=0.00760  val=0.00328\n",
      "[Epoch 061] train=0.00760  val=0.00331\n",
      "[Epoch 062] train=0.00760  val=0.00332\n",
      "[Epoch 063] train=0.00760  val=0.00328\n",
      "[Epoch 064] train=0.00759  val=0.00351\n",
      "→ LR reduced from 2.50e-05 to 1.25e-05\n",
      "[Epoch 065] train=0.00758  val=0.00331\n",
      "[Epoch 066] train=0.00758  val=0.00330\n",
      "[Epoch 067] train=0.00757  val=0.00331\n",
      "[Epoch 068] train=0.00758  val=0.00328\n",
      "[Epoch 069] train=0.00758  val=0.00330\n",
      "[Epoch 070] train=0.00757  val=0.00335\n",
      "→ LR reduced from 1.25e-05 to 6.25e-06\n",
      "[Epoch 071] train=0.00758  val=0.00328\n",
      "[Epoch 072] train=0.00757  val=0.00329\n",
      "[Epoch 073] train=0.00756  val=0.00328\n",
      "[Epoch 074] train=0.00756  val=0.00327\n",
      "[Epoch 075] train=0.00757  val=0.00329\n",
      "[Epoch 076] train=0.00756  val=0.00329\n",
      "[Epoch 077] train=0.00756  val=0.00333\n",
      "[Epoch 078] train=0.00757  val=0.00327\n",
      "[Epoch 079] train=0.00756  val=0.00329\n",
      "→ LR reduced from 6.25e-06 to 3.13e-06\n",
      "[Epoch 080] train=0.00756  val=0.00331\n",
      "[Epoch 081] train=0.00756  val=0.00335\n",
      "[Epoch 082] train=0.00756  val=0.00329\n",
      "[Epoch 083] train=0.00755  val=0.00330\n",
      "[Epoch 084] train=0.00755  val=0.00330\n",
      "[Epoch 085] train=0.00756  val=0.00328\n",
      "→ LR reduced from 3.13e-06 to 1.56e-06\n",
      "[Epoch 086] train=0.00755  val=0.00328\n",
      "[Epoch 087] train=0.00756  val=0.00329\n",
      "[Epoch 088] train=0.00755  val=0.00329\n",
      "[Epoch 089] train=0.00756  val=0.00329\n",
      "[Epoch 090] train=0.00755  val=0.00333\n",
      "[Epoch 091] train=0.00756  val=0.00329\n",
      "→ LR reduced from 1.56e-06 to 7.81e-07\n",
      "[Epoch 092] train=0.00755  val=0.00330\n",
      "[Epoch 093] train=0.00755  val=0.00330\n",
      "[Epoch 094] train=0.00755  val=0.00329\n",
      "→ early stopping after 94 epochs\n",
      "\n",
      "TEST → MAE=31.1079, RMSE=38.4205, SMAPE=61.6999%, Huber=31.1029\n",
      "✅ Saved all outputs to models/BiAttnPointForecaster_sl24_fl24_h24_lh24_dh12_nl1_bs48_lr.0001_me200_p20_MaxAbs_Huber_v1\n"
     ]
    }
   ],
   "source": [
    "# ─── TEST-EVALUATION + EARLY STOPPING ────────────────────────────────\n",
    "import os, json, copy, joblib\n",
    "import numpy as np\n",
    "import torch, random, sys\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ─── Create directory ──────────────────────────────────────────────\n",
    "os.makedirs(candidate, exist_ok=True)\n",
    "base_dir = candidate\n",
    "print(f\"→ Saving run in: {base_dir}\")\n",
    "\n",
    "# ─── 1) SCALE TRAIN/VAL/TEST ──────────────────────────────────────────────\n",
    "# scale historical\n",
    "scaler_X = TRANSFORMER_FACTORY[scaler_used]()\n",
    "X_train_hist_scaled = scaler_X.fit_transform(X_train_hist)\n",
    "X_val_hist_scaled   = scaler_X.transform(X_val_hist)\n",
    "X_test_hist_scaled  = scaler_X.transform(X_test_hist)\n",
    "\n",
    "# scale future\n",
    "n_fut_feats = X_fut_train.shape[2]\n",
    "scaler_F    = TRANSFORMER_FACTORY[scaler_used]()\n",
    "flat_F_train = X_fut_train.reshape(-1, n_fut_feats)\n",
    "flat_F_train = scaler_F.fit_transform(flat_F_train)\n",
    "X_fut_train_scaled = flat_F_train.reshape(X_fut_train.shape)\n",
    "\n",
    "flat_F_val   = X_fut_val.reshape(-1, n_fut_feats)\n",
    "flat_F_val   = scaler_F.transform(flat_F_val)\n",
    "X_fut_val_scaled = flat_F_val.reshape(X_fut_val.shape)\n",
    "\n",
    "flat_F_test  = X_fut_test.reshape(-1, n_fut_feats)\n",
    "flat_F_test  = scaler_F.transform(flat_F_test)\n",
    "X_fut_test_scaled = flat_F_test.reshape(X_fut_test.shape)\n",
    "\n",
    "# scale targets\n",
    "scaler_y     = TRANSFORMER_FACTORY[scaler_used]()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_val_scaled   = scaler_y.transform(y_val.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "# ─── 2) BUILD DATALOADERS ──────────────────────────────────────────────\n",
    "def to_tensor(x, dtype):\n",
    "    return torch.tensor(x, dtype=dtype)\n",
    "\n",
    "train_ds = MultiFeedDataset(\n",
    "    hist       = to_tensor(X_train_hist_scaled, torch.float32),\n",
    "    full_fut   = to_tensor(X_fut_train_scaled,  torch.float32),\n",
    "    y          = to_tensor(y_train_scaled,      torch.float32),\n",
    "    month_idx  = to_tensor(month_train,         torch.long),\n",
    "    weekday_idx= to_tensor(weekday_train,       torch.long),\n",
    "    sp_idx     = to_tensor(sp_train,            torch.long),\n",
    "    dtype_idx  = to_tensor(dtype_train,         torch.long),\n",
    "    seq_len    = seq_len,\n",
    "    feed_len   = feed_len,\n",
    "    fut_len    = fut_len\n",
    ")\n",
    "val_ds = MultiFeedDataset(\n",
    "    hist       = to_tensor(X_val_hist_scaled,   torch.float32),\n",
    "    full_fut   = to_tensor(X_fut_val_scaled,    torch.float32),\n",
    "    y          = to_tensor(y_val_scaled,        torch.float32),\n",
    "    month_idx  = to_tensor(month_val,           torch.long),\n",
    "    weekday_idx= to_tensor(weekday_val,         torch.long),\n",
    "    sp_idx     = to_tensor(sp_val,              torch.long),\n",
    "    dtype_idx  = to_tensor(dtype_val,           torch.long),\n",
    "    seq_len    = seq_len,\n",
    "    feed_len   = feed_len,\n",
    "    fut_len    = fut_len\n",
    ")\n",
    "test_ds = MultiFeedDataset(\n",
    "    hist       = to_tensor(X_test_hist_scaled,  torch.float32),\n",
    "    full_fut   = to_tensor(X_fut_test_scaled,   torch.float32),\n",
    "    y          = to_tensor(y_test_scaled,       torch.float32),\n",
    "    month_idx  = to_tensor(month_test,          torch.long),\n",
    "    weekday_idx= to_tensor(weekday_test,        torch.long),\n",
    "    sp_idx     = to_tensor(sp_test,             torch.long),\n",
    "    dtype_idx  = to_tensor(dtype_test,          torch.long),\n",
    "    seq_len    = seq_len,\n",
    "    feed_len   = feed_len,\n",
    "    fut_len    = fut_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# ─── 3) INSTANTIATE MODEL, OPTIMIZER, CRITERION ──────────────────────\n",
    "model     = MODEL_FACTORY[model_used](\n",
    "                num_hist_feats = X_train_hist_scaled.shape[1],\n",
    "                num_fut_feats  = n_fut_feats,\n",
    "                num_time_feats = sum([4,3,6,2]),\n",
    "                lstm_hidden    = lstm_hidden,\n",
    "                dec_hidden     = dec_hidden,\n",
    "                attn_dim       = attn_dim,\n",
    "                hist_len       = seq_len,\n",
    "                feed_len       = feed_len,\n",
    "                fut_len        = fut_len\n",
    "            ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = (LOSS_FACTORY[loss_used](beta=beta)\n",
    "             if loss_used.startswith(\"Huber\")\n",
    "             else LOSS_FACTORY[loss_used]())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7\n",
    "            )\n",
    "last_lrs = scheduler.get_last_lr()\n",
    "# ─── 4) TRAIN w/ EARLY STOPPING ON VAL ───────────────────────────────\n",
    "best_val, epochs_no_improve, best_ckpt = float('inf'), 0, None\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x_h, x_f, y_t, mi, wi, si, di in train_loader:\n",
    "        x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
    "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out  = model(x_h, x_f, mi, wi, si, di)\n",
    "        loss = criterion(out, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_h, x_f, y_t, mi, wi, si, di in val_loader:\n",
    "            x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
    "            mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "            out = model(x_h, x_f, mi, wi, si, di)\n",
    "            val_loss += criterion(out, y_t).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    if scheduler.get_last_lr()[0] != last_lrs[0]:\n",
    "        print(f\"→ LR reduced from {last_lrs[0]:.2e} to {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    last_lrs = scheduler.get_last_lr()\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}] train={train_loss:.5f}  val={val_loss:.5f}\")\n",
    "    if val_loss < best_val:\n",
    "        best_val, epochs_no_improve = val_loss, 0\n",
    "        best_ckpt = {\n",
    "            'model':     copy.deepcopy(model.state_dict()),\n",
    "            'optimizer': copy.deepcopy(optimizer.state_dict())\n",
    "        }\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"→ early stopping after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "# load best checkpoint\n",
    "model.load_state_dict(best_ckpt['model'])\n",
    "optimizer.load_state_dict(best_ckpt['optimizer'])\n",
    "\n",
    "# ─── 5) INFERENCE ON TEST ──────────────────────────────────────────\n",
    "model.eval()\n",
    "preds_all, trues_all = [], []\n",
    "with torch.no_grad():\n",
    "    for x_h, x_f, y_t, mi, wi, si, di in test_loader:\n",
    "        x_h, x_f = x_h.to(device), x_f.to(device)\n",
    "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "        out = model(x_h, x_f, mi, wi, si, di)\n",
    "        preds_all.append(out.cpu().numpy())\n",
    "        trues_all.append(y_t.numpy())\n",
    "\n",
    "preds_all = np.concatenate(preds_all, axis=0)\n",
    "trues_all = np.concatenate(trues_all, axis=0)\n",
    "\n",
    "preds = scaler_y.inverse_transform(preds_all.reshape(-1,1)).flatten()\n",
    "trues = scaler_y.inverse_transform(trues_all.reshape(-1,1)).flatten()\n",
    "err   = trues - preds\n",
    "\n",
    "mae   = mean_absolute_error(trues, preds)\n",
    "rmse  = np.sqrt(mean_squared_error(trues, preds))\n",
    "smape = np.mean(2.0 * np.abs(err) / (np.abs(trues) + np.abs(preds) + 1e-8)) * 100\n",
    "huber_vals = np.where(np.abs(err) <= beta,\n",
    "                      0.5 * err**2 / beta,\n",
    "                      np.abs(err) - 0.5 * beta)\n",
    "huber = huber_vals.mean()\n",
    "\n",
    "print(f\"\\nTEST → MAE={mae:.4f}, RMSE={rmse:.4f}, SMAPE={smape:.4f}%, Huber={huber:.4f}\")\n",
    "# ─── 6) BUILD METADATA & SAVE ────────────────────────────────────────\n",
    "class NpTorchJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.generic):   return o.item()\n",
    "        if isinstance(o, np.ndarray):   return o.tolist()\n",
    "        if isinstance(o, torch.Tensor): return o.detach().cpu().tolist()\n",
    "        if isinstance(o, torch.device): return str(o)\n",
    "        if isinstance(o, datetime):     return o.isoformat()\n",
    "        return super().default(o)\n",
    "\n",
    "env_meta = {\n",
    "    \"seed_torch\":  torch.initial_seed(),\n",
    "    \"seed_numpy\":  np.random.get_state()[1][0],\n",
    "    \"seed_python\": random.getstate()[1][0],\n",
    "    \"run_time\":    datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "data_meta = {\n",
    "    \"start\":     df_train.index.min().strftime('%Y-%m-%d'),\n",
    "    \"train_end\": train_end_date,\n",
    "    \"val_end\":   val_end_date,\n",
    "    \"end\":       df_test.index.max().strftime('%Y-%m-%d'),\n",
    "    \"seq_len\":   seq_len,\n",
    "    \"feed_len\":  feed_len,\n",
    "    \"horizon\":   fut_len,\n",
    "    \"n_train\":   len(train_loader.dataset),\n",
    "    \"n_val\":     len(val_loader.dataset),\n",
    "    \"n_test\":    len(test_loader.dataset)\n",
    "}\n",
    "\n",
    "feat_meta = {\n",
    "    \"hist_feats\": hist_cols,\n",
    "    \"time_feats\": cal_cols,\n",
    "    \"n_fut_feats\": n_fut_feats\n",
    "}\n",
    "\n",
    "loader_meta = {\n",
    "    \"batch_size\":  batch_size,\n",
    "    \"pin_memory\":  True,\n",
    "    \"shuffle\":     {\"train\": True, \"val\": False, \"test\": False}\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"model\":        model_used,\n",
    "    \"seq_len\":      seq_len,\n",
    "    \"feed_len\":     feed_len,\n",
    "    \"horizon\":      fut_len,\n",
    "    \"lstm_hidden\":  lstm_hidden,\n",
    "    \"dec_hidden\":   dec_hidden,\n",
    "    \"attn_dim\":     attn_dim,\n",
    "    \"num_layers\":   1,\n",
    "    \"batch_size\":   batch_size,\n",
    "    \"learning_rate\":lr,\n",
    "    \"max_epochs\":   max_epochs,\n",
    "    \"patience\":     patience,\n",
    "    \"scaler\":       scaler_used,\n",
    "    \"loss\":         loss_used,\n",
    "    **({\"beta\": beta} if loss_used.startswith(\"Huber\") else {})\n",
    "}\n",
    "\n",
    "optim_meta = {\n",
    "    \"type\": optimizer.__class__.__name__,\n",
    "    \"lr\":   optimizer.defaults.get(\"lr\")\n",
    "}\n",
    "\n",
    "sched_meta = {\n",
    "    \"type\":     scheduler.__class__.__name__,\n",
    "    \"factor\":   getattr(scheduler, \"factor\", None),\n",
    "    \"patience\": getattr(scheduler, \"patience\", None)\n",
    "}\n",
    "\n",
    "earlystop_meta = {\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"patience\":   patience\n",
    "}\n",
    "\n",
    "metrics_meta = {\n",
    "    \"mae\":   mae,\n",
    "    \"rmse\":  rmse,\n",
    "    \"smape\": smape,\n",
    "    \"huber\": huber\n",
    "}\n",
    "\n",
    "# Save model & scalers\n",
    "torch.save({\n",
    "    \"model_state_dict\":     model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict()\n",
    "}, os.path.join(base_dir, \"torch_model.pt\"))\n",
    "\n",
    "joblib.dump({\n",
    "    \"scaler_X\": scaler_X,\n",
    "    \"scaler_F\": scaler_F,\n",
    "    \"scaler_y\": scaler_y\n",
    "}, os.path.join(base_dir, \"scalers.joblib\"))\n",
    "\n",
    "# Save summary\n",
    "with open(os.path.join(base_dir, \"test_summary.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"environment\": env_meta,\n",
    "        \"data\":        data_meta,\n",
    "        \"features\":    feat_meta,\n",
    "        \"dataloader\":  loader_meta,\n",
    "        \"hyperparams\": hyperparams,\n",
    "        \"optimizer\":   optim_meta,\n",
    "        \"scheduler\":   sched_meta,\n",
    "        \"early_stop\":  earlystop_meta,\n",
    "        \"metrics\":     metrics_meta\n",
    "    }, f, indent=2, cls=NpTorchJSONEncoder)\n",
    "\n",
    "print(f\"✅ Saved all outputs to {base_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
