{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f158ee02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f158ee02",
        "outputId": "917897f1-fd0b-42bf-bba5-c6a3f97cec23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "77d4616b",
      "metadata": {
        "id": "77d4616b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler\n",
        "\n",
        "# ─── configuration ───────────────────────────────────────────────────────────\n",
        "root           = Path(\".\")\n",
        "forecast_dir   = root / \"bmrs_csv_filled\"\n",
        "df_path        = root / forecast_dir / \"df_all.csv\"\n",
        "mask_dir       = root / \"bmrs_csv_masks\"\n",
        "\n",
        "date_start     = \"2021-07-01\"\n",
        "date_end       = \"2025-06-30\"\n",
        "train_end_date = \"2025-03-01\"\n",
        "val_end_date   = \"2025-05-01\"\n",
        "\n",
        "horizon        = 48\n",
        "use_time_feat  = False   # whether to add trig-based time features\n",
        "\n",
        "# ─── sanity checks & seeding ─────────────────────────────────────────────────\n",
        "assert df_path.exists(), f\"{df_path} not found\"\n",
        "for d in (forecast_dir, mask_dir):\n",
        "    assert d.exists(), f\"{d} not found\"\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b6933fe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6933fe5",
        "outputId": "e7a57324-bed5-4446-fb2c-541b6e8de7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[debug] loaded df rows = 70126\n",
            "                     month_idx  weekday_idx  sp_idx  dtype_idx\n",
            "startTime                                                     \n",
            "2021-07-01 00:00:00          6            3       2          0\n",
            "2021-07-01 00:30:00          6            3       3          0\n",
            "2021-07-01 01:00:00          6            3       4          0\n",
            "2021-07-01 01:30:00          6            3       5          0\n",
            "2021-07-01 02:00:00          6            3       6          0\n",
            "[debug] DEMAND_FORECASTS: 70126 rows reindexed\n",
            "[debug] DEMAND_FORECASTS: full horizon loaded\n",
            "[debug] WIND_FORECASTS: 70126 rows reindexed\n",
            "[debug] WIND_FORECASTS: full horizon loaded\n",
            "[debug] DRM_FORECASTS: 70126 rows reindexed\n",
            "[debug] DRM_FORECASTS: full horizon loaded\n",
            "[debug] x_fut shape = (70126, 48, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bc/9771s35x1x92qwjdzftm3svc0000gn/T/ipykernel_97099/3808848117.py:28: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  pd.read_csv(path, index_col=\"startTime\", parse_dates=True)\n"
          ]
        }
      ],
      "source": [
        "df = (\n",
        "    pd.read_csv(df_path, index_col=\"startTime\", parse_dates=True)\n",
        "      .loc[date_start:date_end]\n",
        ")\n",
        "print(f\"[debug] loaded df rows = {len(df)}\")\n",
        "\n",
        "# drop any forecast/actual cols\n",
        "to_drop = [c for c in df.columns if \"forecast\" in c.lower() or \"actual\" in c.lower()]\n",
        "df.drop(columns=to_drop, errors=\"ignore\", inplace=True)\n",
        "\n",
        "# embed time features\n",
        "df[\"month_idx\"]   = df.index.month - 1\n",
        "df[\"weekday_idx\"] = df.index.dayofweek\n",
        "if \"settlement period\" in (c.lower() for c in df.columns):\n",
        "    df[\"sp_idx\"] = df.pop(\"Settlement Period\").astype(int) - 1\n",
        "else:\n",
        "    df[\"sp_idx\"] = 0\n",
        "    print(\"[warn] 'settlement period' not found; sp_idx set to 0\")\n",
        "df[\"dtype_idx\"]   = (df.index.dayofweek >= 5).astype(int)\n",
        "\n",
        "print(df[[\"month_idx\",\"weekday_idx\",\"sp_idx\",\"dtype_idx\"]].head())\n",
        "\n",
        "def load_forecast_matrix(name, prefix, idx, horizon):\n",
        "    path = forecast_dir / f\"{name}.csv\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"{path} not found\")\n",
        "    fdf = (\n",
        "        pd.read_csv(path, index_col=\"startTime\", parse_dates=True)\n",
        "          .loc[date_start:date_end]\n",
        "          .reindex(idx)\n",
        "    )\n",
        "    print(f\"[debug] {name}: {len(fdf)} rows reindexed\")\n",
        "    cols    = [f\"{prefix}_f{i}\" for i in range(1, horizon+1)]\n",
        "    present = [c for c in cols if c in fdf.columns]\n",
        "    mat     = fdf[present].fillna(0).to_numpy()\n",
        "    if mat.shape[1] < horizon:\n",
        "        pad = np.zeros((len(mat), horizon - mat.shape[1]), dtype=mat.dtype)\n",
        "        mat = np.hstack([mat, pad])\n",
        "        print(f\"[debug] {name}: padded from {len(present)}→{horizon}\")\n",
        "    else:\n",
        "        print(f\"[debug] {name}: full horizon loaded\")\n",
        "    return mat\n",
        "\n",
        "idx        = df.index\n",
        "demand_mat = load_forecast_matrix(\"DEMAND_FORECASTS\", \"demand\", idx, horizon)\n",
        "wind_mat   = load_forecast_matrix(\"WIND_FORECASTS\",   \"wind\",   idx, horizon)\n",
        "drm_mat    = load_forecast_matrix(\"DRM_FORECASTS\",    \"drm\",    idx, horizon)\n",
        "x_fut      = np.stack([demand_mat, wind_mat, drm_mat], axis=2)\n",
        "assert x_fut.shape[0] == len(df)\n",
        "print(f\"[debug] x_fut shape = {x_fut.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "439e5f6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "439e5f6f",
        "outputId": "9b467681-095a-4637-c668-b1fff37956fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train → x_hist (64272, 10), x_cal (64272, 4), y (64272,), x_fut (64272, 48, 3)\n",
            "val   → x_hist (2928, 10), x_cal (2928, 4), y (2928,), x_fut (2928, 48, 3)\n",
            "test  → x_hist (2926, 10), x_cal (2926, 4), y (2926,), x_fut (2926, 48, 3)\n"
          ]
        }
      ],
      "source": [
        "target_col = \"Imbalance Price\"\n",
        "assert target_col in df.columns, f\"missing target: {target_col}\"\n",
        "\n",
        "masks = {\n",
        "    \"train\": df.index < train_end_date,\n",
        "    \"val\"  : (df.index >= train_end_date) & (df.index < val_end_date),\n",
        "    \"test\" : df.index >= val_end_date,\n",
        "}\n",
        "\n",
        "cal_cols  = [\"month_idx\",\"weekday_idx\",\"sp_idx\",\"dtype_idx\"]\n",
        "hist_cols = [c for c in df.columns if c not in cal_cols]\n",
        "\n",
        "splits = {}\n",
        "for split, mask in masks.items():\n",
        "    sub = df.loc[mask]\n",
        "    splits[split] = {\n",
        "        \"x_hist\": sub[hist_cols].to_numpy(),\n",
        "        \"x_cal\" : sub[cal_cols].to_numpy(),\n",
        "        \"y\"     : sub[target_col].to_numpy(),\n",
        "        \"x_fut\" : x_fut[mask],\n",
        "    }\n",
        "    print(\n",
        "        f\"{split:5s} → \"\n",
        "        f\"x_hist {splits[split]['x_hist'].shape}, \"\n",
        "        f\"x_cal {splits[split]['x_cal'].shape}, \"\n",
        "        f\"y {splits[split]['y'].shape}, \"\n",
        "        f\"x_fut {splits[split]['x_fut'].shape}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce6e443",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bce6e443",
        "outputId": "e0d5eb16-2c97-475a-f677-d54390cd1058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "configs → seq=48, feed=48, fut=4; batch=144, lr=0.0001, epochs=200\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 ─── dataset & model class definitions (with inline comments) ──────\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from src.models.factory import MODEL_FACTORY\n",
        "from src.datasets.factory import DATASET_FACTORY\n",
        "\n",
        "transformer_factory = {\n",
        "    \"MinMax\":   MinMaxScaler,\n",
        "    \"Robust\":   RobustScaler,\n",
        "    \"Standard\": StandardScaler,\n",
        "    \"MaxAbs\":   MaxAbsScaler,\n",
        "}\n",
        "\n",
        "loss_factory = {\n",
        "    \"MAE\":   nn.L1Loss,\n",
        "    \"MSE\":   nn.MSELoss,\n",
        "    \"Huber\": nn.SmoothL1Loss,\n",
        "}\n",
        "\n",
        "# model & sequence configuration\n",
        "seq_len   = 48   # look-back window\n",
        "feed_len  = 48   # known-future window\n",
        "fut_len   = 4   # forecast horizon\n",
        "\n",
        "# network widths & depths\n",
        "lstm_hidden = 64\n",
        "dec_hidden  = 64\n",
        "attn_dim    = 48\n",
        "num_layers  = 1\n",
        "dropout     = 0.0\n",
        "\n",
        "# training settings\n",
        "batch_size = 144\n",
        "lr         = 1e-4\n",
        "patience   = 20\n",
        "max_epochs = 200\n",
        "\n",
        "# scaler & loss choices (must match factory keys)\n",
        "scaler_used = \"MaxAbs\"\n",
        "model_used  = \"BiAttnPointForecaster\"\n",
        "loss_used   = \"Huber\"\n",
        "beta        = 0.01   # only for Huber loss\n",
        "notes       = None\n",
        "\n",
        "print(f\"configs → seq={seq_len}, feed={feed_len}, fut={fut_len}; \"\n",
        "      f\"batch={batch_size}, lr={lr}, epochs={max_epochs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7d84e7ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d84e7ee",
        "outputId": "68e562c6-7f36-48c0-a846-64a29df797b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] saving model to: saved_runs/BiAttnPointForecaster_sl48_fl48_h4_lh64_dh64_nl1_bs144_lr.0001_me200_p20_MaxAbs_Huber_v1\n"
          ]
        }
      ],
      "source": [
        "# ─── metadata & model directory ─────────────────────────────────────────────\n",
        "from pathlib import Path\n",
        "\n",
        "# metadata dict\n",
        "md = {\n",
        "    \"model\":        model_used,\n",
        "    \"seq_len\":      seq_len,\n",
        "    \"feed_len\":     feed_len,\n",
        "    \"horizon\":      fut_len,\n",
        "    \"lstm_hidden\":  lstm_hidden,\n",
        "    \"dec_hidden\":   dec_hidden,\n",
        "    \"num_layers\":   num_layers,\n",
        "    \"batch_size\":   batch_size,\n",
        "    \"learning_rate\":lr,\n",
        "    \"max_epochs\":   max_epochs,\n",
        "    \"patience\":     patience,\n",
        "    \"scaler\":       scaler_used,\n",
        "    \"loss\":         loss_used,\n",
        "    **({\"notes\": notes} if notes else {}),\n",
        "}\n",
        "\n",
        "# helper to abbreviate keys\n",
        "initials = lambda s: \"\".join(w[0] for w in s.split(\"_\"))\n",
        "\n",
        "# build tag parts\n",
        "parts = []\n",
        "for k, v in md.items():\n",
        "    sv = str(v)\n",
        "    if isinstance(v, float) and sv.startswith(\"0.\"):\n",
        "        sv = sv.replace(\"0.\", \".\")\n",
        "    part = sv if k in {\"model\",\"scaler\",\"loss\",\"notes\"} else f\"{initials(k)}{sv}\"\n",
        "    parts.append(part)\n",
        "\n",
        "# combine into tag\n",
        "tag = \"_\".join(parts)\n",
        "\n",
        "# determine candidate path, bumping version if needed\n",
        "models_root = Path(\"saved_runs\")\n",
        "candidate   = models_root / tag\n",
        "version     = 0\n",
        "while candidate.exists():\n",
        "    version    += 1\n",
        "    candidate   = models_root / f\"{tag}_v{version}\"\n",
        "\n",
        "print(f\"[info] saving model to: {candidate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4745bbb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4745bbb1",
        "outputId": "cfaa9b79-1c5b-43cf-a032-0be64443ccf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_hist_feats: 10\n",
            "num_fut_feats: 3\n",
            "time_feat_dim: 15\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m out  \u001b[38;5;241m=\u001b[39m model(x_h, x_f, mi, wi, si, di)\n\u001b[1;32m    137\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y_t)\n\u001b[0;32m--> 138\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    140\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ─── train/val/test loop + early stopping ─────────────────────────────────\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import joblib\n",
        "import sys\n",
        "import platform\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from src.datasets.datasets import MultiFeedDataset\n",
        "\n",
        "# create output directory\n",
        "base_dir = candidate  # candidate is a pathlib.Path\n",
        "base_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"→ saving run in: {base_dir}\")\n",
        "\n",
        "# rebuild train/val/test df slices for metadata\n",
        "df_train = df.loc[masks[\"train\"]]\n",
        "df_val   = df.loc[masks[\"val\"]]\n",
        "df_test  = df.loc[masks[\"test\"]]\n",
        "\n",
        "# 1) scale train/val/test\n",
        "\n",
        "# a) history features\n",
        "scaler_x = transformer_factory[scaler_used]()\n",
        "x_train_hist_scaled = scaler_x.fit_transform(splits[\"train\"][\"x_hist\"])\n",
        "x_val_hist_scaled   = scaler_x.transform(splits[\"val\"][\"x_hist\"])\n",
        "x_test_hist_scaled  = scaler_x.transform(splits[\"test\"][\"x_hist\"])\n",
        "\n",
        "# b) future features\n",
        "n_fut_feats = splits[\"train\"][\"x_fut\"].shape[2]\n",
        "scaler_f    = transformer_factory[scaler_used]()\n",
        "flat_f_train = splits[\"train\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
        "flat_f_train = scaler_f.fit_transform(flat_f_train)\n",
        "x_fut_train_scaled = flat_f_train.reshape(splits[\"train\"][\"x_fut\"].shape)\n",
        "\n",
        "flat_f_val   = splits[\"val\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
        "flat_f_val   = scaler_f.transform(flat_f_val)\n",
        "x_fut_val_scaled = flat_f_val.reshape(splits[\"val\"][\"x_fut\"].shape)\n",
        "\n",
        "flat_f_test  = splits[\"test\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
        "flat_f_test  = scaler_f.transform(flat_f_test)\n",
        "x_fut_test_scaled = flat_f_test.reshape(splits[\"test\"][\"x_fut\"].shape)\n",
        "\n",
        "# c) targets\n",
        "scaler_y       = transformer_factory[scaler_used]()\n",
        "y_train_scaled = scaler_y.fit_transform(splits[\"train\"][\"y\"].reshape(-1,1)).flatten()\n",
        "y_val_scaled   = scaler_y.transform(splits[\"val\"][\"y\"].reshape(-1,1)).flatten()\n",
        "y_test_scaled  = scaler_y.transform(splits[\"test\"][\"y\"].reshape(-1,1)).flatten()\n",
        "\n",
        "# 2) build dataloaders\n",
        "pin_memory = (device.type == \"cuda\")\n",
        "\n",
        "def to_tensor(x, dtype):\n",
        "    return torch.tensor(x, dtype=dtype)\n",
        "\n",
        "train_ds = MultiFeedDataset(\n",
        "    hist        = to_tensor(x_train_hist_scaled, torch.float32),\n",
        "    full_fut    = to_tensor(x_fut_train_scaled,    torch.float32),\n",
        "    y           = to_tensor(y_train_scaled,        torch.float32),\n",
        "    month_idx   = to_tensor(splits[\"train\"][\"x_cal\"][:,0], torch.long),\n",
        "    weekday_idx = to_tensor(splits[\"train\"][\"x_cal\"][:,1], torch.long),\n",
        "    sp_idx      = to_tensor(splits[\"train\"][\"x_cal\"][:,2], torch.long),\n",
        "    dtype_idx   = to_tensor(splits[\"train\"][\"x_cal\"][:,3], torch.long),\n",
        "    seq_len     = seq_len,\n",
        "    feed_len    = feed_len,\n",
        "    fut_len     = fut_len\n",
        ")\n",
        "val_ds = MultiFeedDataset(\n",
        "    hist        = to_tensor(x_val_hist_scaled,   torch.float32),\n",
        "    full_fut    = to_tensor(x_fut_val_scaled,    torch.float32),\n",
        "    y           = to_tensor(y_val_scaled,        torch.float32),\n",
        "    month_idx   = to_tensor(splits[\"val\"][\"x_cal\"][:,0], torch.long),\n",
        "    weekday_idx = to_tensor(splits[\"val\"][\"x_cal\"][:,1], torch.long),\n",
        "    sp_idx      = to_tensor(splits[\"val\"][\"x_cal\"][:,2], torch.long),\n",
        "    dtype_idx   = to_tensor(splits[\"val\"][\"x_cal\"][:,3], torch.long),\n",
        "    seq_len     = seq_len,\n",
        "    feed_len    = feed_len,\n",
        "    fut_len     = fut_len\n",
        ")\n",
        "test_ds = MultiFeedDataset(\n",
        "    hist        = to_tensor(x_test_hist_scaled,  torch.float32),\n",
        "    full_fut    = to_tensor(x_fut_test_scaled,   torch.float32),\n",
        "    y           = to_tensor(y_test_scaled,       torch.float32),\n",
        "    month_idx   = to_tensor(splits[\"test\"][\"x_cal\"][:,0], torch.long),\n",
        "    weekday_idx = to_tensor(splits[\"test\"][\"x_cal\"][:,1], torch.long),\n",
        "    sp_idx      = to_tensor(splits[\"test\"][\"x_cal\"][:,2], torch.long),\n",
        "    dtype_idx   = to_tensor(splits[\"test\"][\"x_cal\"][:,3], torch.long),\n",
        "    seq_len     = seq_len,\n",
        "    feed_len    = feed_len,\n",
        "    fut_len     = fut_len\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  pin_memory=pin_memory)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
        "\n",
        "# 3) instantiate model, optimizer, criterion\n",
        "time_feat_dim = 4 + 3 + 6 + 2\n",
        "model = MODEL_FACTORY[model_used](\n",
        "    num_hist_feats = x_train_hist_scaled.shape[1],\n",
        "    num_fut_feats  = n_fut_feats,\n",
        "    time_feat_dim  = time_feat_dim,\n",
        "    lstm_hidden    = lstm_hidden,\n",
        "    dec_hidden     = dec_hidden,\n",
        "    attn_dim       = attn_dim,\n",
        "    hist_len       = seq_len,\n",
        "    feed_len       = feed_len,\n",
        "    fut_len        = fut_len\n",
        ").to(device)\n",
        "\n",
        "print(f\"num_hist_feats: {x_train_hist_scaled.shape[1]}\")\n",
        "print(f\"num_fut_feats: {n_fut_feats}\")\n",
        "print(f\"time_feat_dim: {time_feat_dim}\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = (\n",
        "    loss_factory[loss_used](beta=beta)\n",
        "    if loss_used == \"huber\"\n",
        "    else loss_factory[loss_used]()\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7\n",
        ")\n",
        "last_lrs = scheduler.get_last_lr()\n",
        "\n",
        "# 4) train w/ early stopping on val\n",
        "best_val, epochs_no_improve, best_ckpt = float('inf'), 0, None\n",
        "\n",
        "for epoch in range(1, max_epochs+1):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    for x_h, x_f, y_t, mi, wi, si, di in train_loader:\n",
        "        x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
        "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out  = model(x_h, x_f, mi, wi, si, di)\n",
        "        loss = criterion(out, y_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "    train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_h, x_f, y_t, mi, wi, si, di in val_loader:\n",
        "            x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
        "            mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
        "            out = model(x_h, x_f, mi, wi, si, di)\n",
        "            total_val_loss += criterion(out, y_t).item()\n",
        "    val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    new_lr = scheduler.get_last_lr()[0]\n",
        "    if new_lr != last_lrs[0]:\n",
        "        print(f\"→ lr reduced from {last_lrs[0]:.2e} to {new_lr:.2e}\")\n",
        "    last_lrs = scheduler.get_last_lr()\n",
        "\n",
        "    print(f\"[epoch {epoch:03d}] train={train_loss:.5f} val={val_loss:.5f}\")\n",
        "    if val_loss < best_val:\n",
        "        best_val, epochs_no_improve = val_loss, 0\n",
        "        best_ckpt = {\n",
        "            \"model\":     copy.deepcopy(model.state_dict()),\n",
        "            \"optimizer\": copy.deepcopy(optimizer.state_dict())\n",
        "        }\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"→ early stopping after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "# restore best checkpoint\n",
        "model.load_state_dict(best_ckpt[\"model\"])\n",
        "optimizer.load_state_dict(best_ckpt[\"optimizer\"])\n",
        "\n",
        "# 5) inference on test\n",
        "model.eval()\n",
        "preds_all, trues_all = [], []\n",
        "with torch.no_grad():\n",
        "    for x_h, x_f, y_t, mi, wi, si, di in test_loader:\n",
        "        x_h, x_f = x_h.to(device), x_f.to(device)\n",
        "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
        "        out = model(x_h, x_f, mi, wi, si, di)\n",
        "        preds_all.append(out.cpu().numpy())\n",
        "        trues_all.append(y_t.numpy())\n",
        "\n",
        "preds_all = np.concatenate(preds_all, axis=0)\n",
        "trues_all = np.concatenate(trues_all, axis=0)\n",
        "preds     = scaler_y.inverse_transform(preds_all.reshape(-1,1)).flatten()\n",
        "trues     = scaler_y.inverse_transform(trues_all.reshape(-1,1)).flatten()\n",
        "errors    = trues - preds\n",
        "\n",
        "mae   = mean_absolute_error(trues, preds)\n",
        "rmse  = np.sqrt(mean_squared_error(trues, preds))\n",
        "smape = np.mean(2.0 * np.abs(errors) / (np.abs(trues) + np.abs(preds) + 1e-8)) * 100\n",
        "huber_vals = np.where(np.abs(errors) <= beta,\n",
        "                      0.5 * errors**2 / beta,\n",
        "                      np.abs(errors) - 0.5 * beta)\n",
        "huber = huber_vals.mean()\n",
        "\n",
        "print(f\"\\ntest → mae={mae:.4f}, rmse={rmse:.4f}, smape={smape:.4f}%, huber={huber:.4f}\")\n",
        "\n",
        "# 6) build metadata & save\n",
        "class NpTorchJSONEncoder(json.JSONEncoder):\n",
        "    def default(self, o):\n",
        "        if isinstance(o, np.generic):   return o.item()\n",
        "        if isinstance(o, np.ndarray):   return o.tolist()\n",
        "        if isinstance(o, torch.Tensor): return o.detach().cpu().tolist()\n",
        "        if isinstance(o, torch.device): return str(o)\n",
        "        if isinstance(o, datetime):     return o.isoformat()\n",
        "        return super().default(o)\n",
        "\n",
        "env_meta = {\n",
        "    \"seed_torch\":          torch.initial_seed(),\n",
        "    \"seed_numpy\":          np.random.get_state()[1][0],\n",
        "    \"seed_python\":         random.getstate()[1][0],\n",
        "    \"cudnn_deterministic\": getattr(torch.backends.cudnn, \"deterministic\", None),\n",
        "    \"cudnn_benchmark\":     getattr(torch.backends.cudnn, \"benchmark\", None),\n",
        "    \"torch_version\":       torch.__version__,\n",
        "    \"python_version\":      platform.python_version(),\n",
        "    \"run_timestamp\":       datetime.now(timezone.utc).isoformat()\n",
        "}\n",
        "\n",
        "data_meta = {\n",
        "    \"start\":     df_train.index.min().strftime(\"%Y-%m-%d\"),\n",
        "    \"train_end\": train_end_date,\n",
        "    \"val_end\":   val_end_date,\n",
        "    \"end\":       df_test.index.max().strftime(\"%Y-%m-%d\"),\n",
        "    \"n_train\":   len(train_loader.dataset),\n",
        "    \"n_val\":     len(val_loader.dataset),\n",
        "    \"n_test\":    len(test_loader.dataset)\n",
        "}\n",
        "\n",
        "feat_meta = {\n",
        "    \"hist_feats\": {\n",
        "        \"cols\": hist_cols,\n",
        "        \"n\": x_train_hist_scaled.shape[1]\n",
        "    },\n",
        "    \"time_feats\": {\n",
        "        \"cols\": cal_cols,\n",
        "        \"n\": time_feat_dim,\n",
        "    },\n",
        "    \"fut_feats\": {\n",
        "        \"prefixes\": [\"demand\", \"wind\", \"drm\"],  # update this to match your data\n",
        "        \"n\": n_fut_feats,\n",
        "        \"feed_len\": feed_len,\n",
        "        \"fut_len\": fut_len\n",
        "    },\n",
        "    \"total_feats\": len(hist_cols) + len(cal_cols) + n_fut_feats\n",
        "}\n",
        "\n",
        "loader_meta = {\n",
        "    \"batch_size\":  batch_size,\n",
        "    \"shuffle\":     {\"train\": True, \"val\": False, \"test\": False},\n",
        "    \"num_workers\": os.cpu_count() or 1,\n",
        "    \"pin_memory\":  pin_memory,\n",
        "    \"device\":      str(device),\n",
        "}\n",
        "\n",
        "hyperparams_meta = {\n",
        "    \"model\":         model_used,\n",
        "    \"seq_len\":       seq_len,\n",
        "    \"feed_len\":      feed_len,\n",
        "    \"horizon\":       fut_len,\n",
        "    \"lstm_hidden\":   lstm_hidden,\n",
        "    \"dec_hidden\":    dec_hidden,\n",
        "    \"attn_dim\":      attn_dim,\n",
        "    \"num_layers\":    num_layers,\n",
        "    \"batch_size\":    batch_size,\n",
        "    \"learning_rate\": lr,\n",
        "    \"scaler\":        scaler_used,\n",
        "    \"loss\":          loss_used,\n",
        "    **({\"beta\": beta} if loss_used == \"huber\" else {})\n",
        "}\n",
        "\n",
        "optim_meta = {\n",
        "    \"type\": optimizer.__class__.__name__,\n",
        "    \"lr\":   optimizer.defaults.get(\"lr\"),\n",
        "    **{k: optimizer.defaults[k]\n",
        "       for k in (\"betas\",\"eps\",\"weight_decay\")\n",
        "       if k in optimizer.defaults}\n",
        "}\n",
        "\n",
        "sched_meta = {\n",
        "    \"type\":     scheduler.__class__.__name__,\n",
        "    \"mode\":     getattr(scheduler, \"mode\", None),\n",
        "    \"factor\":   getattr(scheduler, \"factor\", None),\n",
        "    \"patience\": getattr(scheduler, \"patience\", None),\n",
        "    \"min_lr\":   (scheduler.min_lrs[0]\n",
        "                 if hasattr(scheduler, \"min_lrs\")\n",
        "                 else getattr(scheduler, \"eta_min\", None)),\n",
        "    \"last_lr\":  scheduler.get_last_lr()\n",
        "}\n",
        "\n",
        "earlystop_meta = {\n",
        "    \"max_epochs\": max_epochs,\n",
        "    \"patience\":  patience,\n",
        "    \"final_epoch\": epoch,\n",
        "    \"best_epoch\": epoch - epochs_no_improve\n",
        "}\n",
        "\n",
        "metrics_meta = {\n",
        "    \"mae\":   float(mae),\n",
        "    \"rmse\":  float(rmse),\n",
        "    \"smape\": float(smape),\n",
        "    \"huber\": float(huber)\n",
        "}\n",
        "\n",
        "\n",
        "# save model & scalers\n",
        "torch.save({\n",
        "    \"model_state_dict\":     model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"scheduler_state_dict\": scheduler.state_dict()\n",
        "}, base_dir / \"torch_model.pt\")\n",
        "\n",
        "joblib.dump({\n",
        "    \"scaler_x\": scaler_x,\n",
        "    \"scaler_f\": scaler_f,\n",
        "    \"scaler_y\": scaler_y\n",
        "}, str(base_dir / \"scalers.joblib\"))\n",
        "\n",
        "with open(base_dir / \"test_summary.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"environment\":  env_meta,\n",
        "        \"data\":         data_meta,\n",
        "        \"features\":     feat_meta,\n",
        "        \"dataloader\":   loader_meta,\n",
        "        \"hyperparams\":  hyperparams_meta,\n",
        "        \"optimizer\":    optim_meta,\n",
        "        \"scheduler\":    sched_meta,\n",
        "        \"early_stop\":   earlystop_meta,\n",
        "        \"metrics\":      metrics_meta\n",
        "    }, f, indent=2, cls=NpTorchJSONEncoder)\n",
        "\n",
        "print(f\"✅ saved all outputs to {base_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7518b056",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (thesis)",
      "language": "python",
      "name": "thesis"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
