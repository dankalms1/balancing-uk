{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f158ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1185498f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "# ─── configuration ───────────────────────────────────────────────────────────\n",
    "root           = Path(\".\")\n",
    "df_path        = root / \"df_all.csv\"\n",
    "forecast_dir   = root / \"bmrs_csv_filled\"\n",
    "mask_dir       = root / \"bmrs_csv_masks\"\n",
    "\n",
    "date_start     = \"2021-07-01\"\n",
    "date_end       = \"2025-06-30\"\n",
    "train_end_date = \"2025-03-01\"\n",
    "val_end_date   = \"2025-05-01\"\n",
    "\n",
    "horizon        = 48\n",
    "use_time_feat  = False   # whether to add trig-based time features\n",
    "\n",
    "# ─── sanity checks & seeding ─────────────────────────────────────────────────\n",
    "assert df_path.exists(), f\"{df_path} not found\"\n",
    "for d in (forecast_dir, mask_dir):\n",
    "    assert d.exists(), f\"{d} not found\"\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6933fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] loaded df rows = 70126\n",
      "                     month_idx  weekday_idx  sp_idx  dtype_idx\n",
      "startTime                                                     \n",
      "2021-07-01 00:00:00          6            3       2          0\n",
      "2021-07-01 00:30:00          6            3       3          0\n",
      "2021-07-01 01:00:00          6            3       4          0\n",
      "2021-07-01 01:30:00          6            3       5          0\n",
      "2021-07-01 02:00:00          6            3       6          0\n",
      "[debug] DEMAND_FORECASTS: 70126 rows reindexed\n",
      "[debug] DEMAND_FORECASTS: full horizon loaded\n",
      "[debug] WIND_FORECASTS: 70126 rows reindexed\n",
      "[debug] WIND_FORECASTS: full horizon loaded\n",
      "[debug] DRM_FORECASTS: 70126 rows reindexed\n",
      "[debug] DRM_FORECASTS: full horizon loaded\n",
      "[debug] x_fut shape = (70126, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    pd.read_csv(df_path, index_col=\"startTime\", parse_dates=True)\n",
    "      .loc[date_start:date_end]\n",
    ")\n",
    "print(f\"[debug] loaded df rows = {len(df)}\")\n",
    "\n",
    "# drop any forecast/actual cols\n",
    "to_drop = [c for c in df.columns if \"forecast\" in c.lower() or \"actual\" in c.lower()]\n",
    "df.drop(columns=to_drop, errors=\"ignore\", inplace=True)\n",
    "\n",
    "# embed time features\n",
    "df[\"month_idx\"]   = df.index.month - 1\n",
    "df[\"weekday_idx\"] = df.index.dayofweek\n",
    "if \"settlement period\" in (c.lower() for c in df.columns):\n",
    "    df[\"sp_idx\"] = df.pop(\"Settlement Period\").astype(int) - 1\n",
    "else:\n",
    "    df[\"sp_idx\"] = 0\n",
    "    print(\"[warn] 'settlement period' not found; sp_idx set to 0\")\n",
    "df[\"dtype_idx\"]   = (df.index.dayofweek >= 5).astype(int)\n",
    "\n",
    "print(df[[\"month_idx\",\"weekday_idx\",\"sp_idx\",\"dtype_idx\"]].head())\n",
    "\n",
    "def load_forecast_matrix(name, prefix, idx, horizon):\n",
    "    path = forecast_dir / f\"{name}.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{path} not found\")\n",
    "    fdf = (\n",
    "        pd.read_csv(path, index_col=\"startTime\", parse_dates=True)\n",
    "          .loc[date_start:date_end]\n",
    "          .reindex(idx)\n",
    "    )\n",
    "    print(f\"[debug] {name}: {len(fdf)} rows reindexed\")\n",
    "    cols    = [f\"{prefix}_f{i}\" for i in range(1, horizon+1)]\n",
    "    present = [c for c in cols if c in fdf.columns]\n",
    "    mat     = fdf[present].fillna(0).to_numpy()\n",
    "    if mat.shape[1] < horizon:\n",
    "        pad = np.zeros((len(mat), horizon - mat.shape[1]), dtype=mat.dtype)\n",
    "        mat = np.hstack([mat, pad])\n",
    "        print(f\"[debug] {name}: padded from {len(present)}→{horizon}\")\n",
    "    else:\n",
    "        print(f\"[debug] {name}: full horizon loaded\")\n",
    "    return mat\n",
    "\n",
    "idx        = df.index\n",
    "demand_mat = load_forecast_matrix(\"DEMAND_FORECASTS\", \"demand\", idx, horizon)\n",
    "wind_mat   = load_forecast_matrix(\"WIND_FORECASTS\",   \"wind\",   idx, horizon)\n",
    "drm_mat    = load_forecast_matrix(\"DRM_FORECASTS\",    \"drm\",    idx, horizon)\n",
    "x_fut      = np.stack([demand_mat, wind_mat, drm_mat], axis=2)\n",
    "assert x_fut.shape[0] == len(df)\n",
    "print(f\"[debug] x_fut shape = {x_fut.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439e5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train → x_hist (64272, 9), x_cal (64272, 4), y (64272,), x_fut (64272, 48, 3)\n",
      "val   → x_hist (2928, 9), x_cal (2928, 4), y (2928,), x_fut (2928, 48, 3)\n",
      "test  → x_hist (2926, 9), x_cal (2926, 4), y (2926,), x_fut (2926, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "target_col = \"Imbalance Price\"\n",
    "assert target_col in df.columns, f\"missing target: {target_col}\"\n",
    "\n",
    "masks = {\n",
    "    \"train\": df.index < train_end_date,\n",
    "    \"val\"  : (df.index >= train_end_date) & (df.index < val_end_date),\n",
    "    \"test\" : df.index >= val_end_date,\n",
    "}\n",
    "\n",
    "cal_cols  = [\"month_idx\",\"weekday_idx\",\"sp_idx\",\"dtype_idx\"]\n",
    "hist_cols = [c for c in df.columns if c not in cal_cols + [target_col]]\n",
    "\n",
    "splits = {}\n",
    "for split, mask in masks.items():\n",
    "    sub = df.loc[mask]\n",
    "    splits[split] = {\n",
    "        \"x_hist\": sub[hist_cols].to_numpy(),\n",
    "        \"x_cal\" : sub[cal_cols].to_numpy(),\n",
    "        \"y\"     : sub[target_col].to_numpy(),\n",
    "        \"x_fut\" : x_fut[mask],\n",
    "    }\n",
    "    print(\n",
    "        f\"{split:5s} → \"\n",
    "        f\"x_hist {splits[split]['x_hist'].shape}, \"\n",
    "        f\"x_cal {splits[split]['x_cal'].shape}, \"\n",
    "        f\"y {splits[split]['y'].shape}, \"\n",
    "        f\"x_fut {splits[split]['x_fut'].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849b4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 ─── dataset & model class definitions (with inline comments) ──────\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiFeedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sliding‐window dataset producing:\n",
    "      x_h   (history covariates):   (seq_len, n_hist_feats)\n",
    "      x_f   (known‐future covars):  (feed_len, n_fut_feats)\n",
    "      y_t   (targets):              (fut_len,)\n",
    "      mi, wi, si, di (calendar idx): (seq_len + feed_len,)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hist: torch.Tensor,        # (T, n_hist_feats)\n",
    "        full_fut: torch.Tensor,    # (T, feed_len, n_fut_feats)\n",
    "        y: torch.Tensor,           # (T,)\n",
    "        month_idx: torch.Tensor,   # (T,)\n",
    "        weekday_idx: torch.Tensor, # (T,)\n",
    "        sp_idx: torch.Tensor,      # (T,)\n",
    "        dtype_idx: torch.Tensor,   # (T,)\n",
    "        seq_len: int,\n",
    "        feed_len: int,\n",
    "        fut_len: int,\n",
    "    ):\n",
    "        assert fut_len <= feed_len, \"fut_len must be ≤ feed_len\"\n",
    "        self.hist        = hist.float()\n",
    "        self.fut         = full_fut.float()\n",
    "        self.y           = y.float()\n",
    "        self.month_idx   = month_idx.long()\n",
    "        self.weekday_idx = weekday_idx.long()\n",
    "        self.sp_idx      = sp_idx.long()\n",
    "        self.dtype_idx   = dtype_idx.long()\n",
    "        self.seq_len     = seq_len\n",
    "        self.feed_len    = feed_len\n",
    "        self.fut_len     = fut_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # only allow windows where history, future covariates, and targets are all available\n",
    "        # i.e., idx + seq_len + feed_len <= T, and idx + seq_len + fut_len <= T\n",
    "        # since fut_len ≤ feed_len, we bound by feed_len:\n",
    "        return self.hist.size(0) - self.seq_len - self.feed_len + 1\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # 1) history window\n",
    "        x_h = self.hist[idx : idx + self.seq_len]                        # (seq_len, n_hist_feats)\n",
    "\n",
    "        # 2) known‐future covariates\n",
    "        anchor = idx + self.seq_len - 1\n",
    "        x_f    = self.fut[anchor, : self.feed_len]                       # (feed_len, n_fut_feats)\n",
    "\n",
    "        # 3) targets for next fut_len steps\n",
    "        start_y = idx + self.seq_len\n",
    "        y_t     = self.y[start_y : start_y + self.fut_len]              # (fut_len,)\n",
    "\n",
    "        # 4) calendar indices for seq_len + feed_len\n",
    "        ci_start = idx\n",
    "        ci_end   = idx + self.seq_len + self.feed_len\n",
    "        mi = self.month_idx[ci_start : ci_end]                           # (seq_len+feed_len,)\n",
    "        wi = self.weekday_idx[ci_start : ci_end]\n",
    "        si = self.sp_idx[ci_start : ci_end]\n",
    "        di = self.dtype_idx[ci_start : ci_end]\n",
    "\n",
    "        return x_h, x_f, y_t, mi, wi, si, di\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    \"\"\"Embed month, weekday, settlement‐period, and day‐type indices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define embedding sizes for each calendar feature\n",
    "        self.emb_month = nn.Embedding(12, 4)  # months → 4-dim\n",
    "        self.emb_day   = nn.Embedding(7,  3)  # weekdays → 3-dim\n",
    "        self.emb_sp    = nn.Embedding(50, 6)  # periods → 6-dim\n",
    "        self.emb_dtype = nn.Embedding(2,  2)  # weekday/weekend → 2-dim\n",
    "\n",
    "    def forward(self, month_idx, day_idx, sp_idx, dtype_idx):\n",
    "        # perform each embedding lookup\n",
    "        e1 = self.emb_month(month_idx)\n",
    "        e2 = self.emb_day(day_idx)\n",
    "        e3 = self.emb_sp(sp_idx)\n",
    "        e4 = self.emb_dtype(dtype_idx)\n",
    "        # concatenate on last dim → (B, L, sum(embed_dims))\n",
    "        return torch.cat([e1, e2, e3, e4], dim=-1)\n",
    "\n",
    "\n",
    "class VariableSelection(nn.Module):\n",
    "    \"\"\"Learn feature‐wise soft attention weights via a self‐projection.\"\"\"\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        # project each feature to a score, same dim as input\n",
    "        self.proj = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        z = self.proj(x)                   # raw scores (B, T, D)\n",
    "        w = torch.softmax(z, dim=-1)       # weights across features\n",
    "        return w * x                       # reweighted input\n",
    "\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM encoder for variable‐length sequences.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_size: int = 64,\n",
    "                 num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        # batch_first=True → inputs shaped (B, T, input_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout = dropout if num_layers>1 else 0.0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # returns:\n",
    "        #   H   (B, T, 2*hidden_size): outputs at all timesteps\n",
    "        #   (h_n, c_n): final hidden/cell states\n",
    "        return self.lstm(x, hidden)\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute additive (Bahdanau) attention over encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_dim: int, dec_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        # project encoder & decoder states to common attn_dim\n",
    "        self.W = nn.Linear(enc_dim, attn_dim, bias=False)\n",
    "        self.U = nn.Linear(dec_dim, attn_dim, bias=True)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, H, s_prev, mask=None):\n",
    "        # 1) encode hidden states\n",
    "        Hp = self.W(H)                          # (B, T, attn_dim)\n",
    "        # 2) project decoder state & broadcast\n",
    "        Sp = self.U(s_prev).unsqueeze(1)        # (B, 1, attn_dim)\n",
    "        # 3) combine & nonlinearity\n",
    "        E  = torch.tanh(Hp + Sp)                # (B, T, attn_dim)\n",
    "        # 4) score → (B, T)\n",
    "        e  = self.v(E).squeeze(-1)\n",
    "        # 5) apply mask if provided\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask==0, float(\"-inf\"))\n",
    "        # 6) normalize to weights\n",
    "        alpha = torch.softmax(e, dim=1)         # (B, T)\n",
    "        # 7) context vector\n",
    "        c = (alpha.unsqueeze(-1) * H).sum(dim=1)  # (B, enc_dim)\n",
    "        return c, alpha\n",
    "\n",
    "\n",
    "class DualLSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Two parallel LSTMCells generating one‐step forecasts at each timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_dim: int, dec_hidden: int):\n",
    "        super().__init__()\n",
    "        # LSTMCell for past‐context\n",
    "        self.lstm_h = nn.LSTMCell(enc_dim, dec_hidden)\n",
    "        # LSTMCell for future‐context\n",
    "        self.lstm_f = nn.LSTMCell(enc_dim, dec_hidden)\n",
    "        # combine both hidden states to output scalar\n",
    "        self.ffn    = nn.Linear(2*dec_hidden, 1)\n",
    "\n",
    "    def forward(self, hist_ctx, fut_ctx):\n",
    "        B, L, _ = hist_ctx.shape\n",
    "        # 1) initialize decoder states to zeros\n",
    "        h_h = hist_ctx.new_zeros(B, self.lstm_h.hidden_size)\n",
    "        c_h = h_h.clone()\n",
    "        h_f = h_h.clone()\n",
    "        c_f = h_h.clone()\n",
    "\n",
    "        outputs = []\n",
    "        # 2) for each timestep t in [0…L-1]\n",
    "        for t in range(L):\n",
    "            # step both LSTMCells\n",
    "            h_h, c_h = self.lstm_h(hist_ctx[:, t], (h_h, c_h))\n",
    "            h_f, c_f = self.lstm_f(fut_ctx[:,  t], (h_f, c_f))\n",
    "            # combine and project\n",
    "            comb     = torch.cat([h_h, h_f], dim=-1)\n",
    "            y_t      = self.ffn(comb).squeeze(-1)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        # 3) stack → (B, L)\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "class BiAttnPointForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    Full bi‐attentional forecaster:\n",
    "      1) embed time features\n",
    "      2) select variables\n",
    "      3) encode with BiLSTM\n",
    "      4) apply dual additive attention\n",
    "      5) decode with DualLSTMDecoder\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_hist_feats: int,\n",
    "                 num_fut_feats:  int,\n",
    "                 time_feat_dim:  int,\n",
    "                 lstm_hidden:    int,\n",
    "                 dec_hidden:     int,\n",
    "                 attn_dim:       int,\n",
    "                 hist_len:       int,\n",
    "                 feed_len:       int,\n",
    "                 fut_len:        int):\n",
    "        super().__init__()\n",
    "        self.hist_len = hist_len\n",
    "        self.feed_len = feed_len\n",
    "        self.fut_len  = fut_len\n",
    "\n",
    "        # 1) time feature embedding\n",
    "        self.time_embed        = TimeFeatureEmbedding()\n",
    "\n",
    "        # 2) variable selection layers\n",
    "        self.var_select_past   = VariableSelection(num_hist_feats + time_feat_dim)\n",
    "        self.var_select_future = VariableSelection(num_fut_feats  + time_feat_dim)\n",
    "\n",
    "        # 3) BiLSTM encoders\n",
    "        self.enc_hist = BiLSTMEncoder(num_hist_feats + time_feat_dim, lstm_hidden)\n",
    "        self.enc_fut  = BiLSTMEncoder(num_fut_feats  + time_feat_dim, lstm_hidden)\n",
    "\n",
    "        # 4) dual additive attention\n",
    "        enc_dim = 2 * lstm_hidden\n",
    "        self.attn_hist = AdditiveAttention(enc_dim, dec_hidden, attn_dim)\n",
    "        self.attn_fut  = AdditiveAttention(enc_dim, dec_hidden, attn_dim)\n",
    "\n",
    "        # project final encoder state → decoder init\n",
    "        self.init_h = nn.Linear(enc_dim, dec_hidden)\n",
    "        self.init_c = nn.Linear(enc_dim, dec_hidden)\n",
    "\n",
    "        # 5) decoder\n",
    "        self.decoder = DualLSTMDecoder(enc_dim, dec_hidden)\n",
    "\n",
    "    def forward(self, x_hist, x_fut,\n",
    "                month_idx, weekday_idx, sp_idx, dtype_idx,\n",
    "                mask_hist=None, mask_fut=None):\n",
    "        # ─── 1) embed all calendar features ────────────────────────────\n",
    "        emb = self.time_embed(month_idx, weekday_idx, sp_idx, dtype_idx)\n",
    "        emb_hist = emb[:, :self.hist_len]\n",
    "        emb_fut  = emb[:, self.hist_len : self.hist_len+self.feed_len]\n",
    "\n",
    "        # ─── 2) variable selection ────────────────────────────────────\n",
    "        xh = torch.cat([x_hist, emb_hist], dim=-1)\n",
    "        xf = torch.cat([x_fut,  emb_fut ], dim=-1)\n",
    "        h_sel = self.var_select_past(xh)\n",
    "        f_sel = self.var_select_future(xf)\n",
    "\n",
    "        # ─── 3) encode with BiLSTM ────────────────────────────────────\n",
    "        H_hist, (h_n, c_n) = self.enc_hist(h_sel)\n",
    "        H_fut,  _          = self.enc_fut(f_sel)\n",
    "\n",
    "        # ─── 4) init decoder states from encoder final states ────────\n",
    "        h_fwd, h_bwd = h_n[-2], h_n[-1]\n",
    "        c_fwd, c_bwd = c_n[-2], c_n[-1]\n",
    "        h0 = self.init_h(torch.cat([h_fwd, h_bwd], dim=-1))\n",
    "        c0 = self.init_c(torch.cat([c_fwd, c_bwd], dim=-1))\n",
    "\n",
    "        # ─── 5) decode with dual attention each step ─────────────────\n",
    "        s_h, s_c = h0, c0\n",
    "        s_f, s_cf = h0.clone(), c0.clone()\n",
    "        outputs = []\n",
    "        for _ in range(self.fut_len):\n",
    "            ctx_h, _ = self.attn_hist(H_hist, s_h, mask=mask_hist)\n",
    "            ctx_f, _ = self.attn_fut( H_fut,  s_f, mask=mask_fut)\n",
    "            s_h, s_c = self.decoder.lstm_h(ctx_h, (s_h, s_c))\n",
    "            s_f, s_cf = self.decoder.lstm_f(ctx_f, (s_f, s_cf))\n",
    "            comb     = torch.cat([s_h, s_f], dim=-1)\n",
    "            y_t      = self.decoder.ffn(comb).squeeze(-1)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)  # (B, fut_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce6e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs → seq=48, feed=48, fut=4; batch=144, lr=0.0001, epochs=200\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 ─── factories & hyperparams ─────────────────────────────────────────\n",
    "transformer_factory = {\n",
    "    \"MinMax\":   MinMaxScaler,\n",
    "    \"Robust\":   RobustScaler,\n",
    "    \"Standard\": StandardScaler,\n",
    "    \"MaxAbs\":   MaxAbsScaler,\n",
    "}\n",
    "\n",
    "model_factory = {\n",
    "    \"BiAttnPointForecaster\": BiAttnPointForecaster,\n",
    "}\n",
    "\n",
    "loss_factory = {\n",
    "    \"MAE\":   nn.L1Loss,\n",
    "    \"MSE\":   nn.MSELoss,\n",
    "    \"Huber\": nn.SmoothL1Loss,\n",
    "}\n",
    "\n",
    "# model & sequence configuration\n",
    "seq_len   = 48   # look-back window\n",
    "feed_len  = 48   # known-future window\n",
    "fut_len   = 4   # forecast horizon\n",
    "\n",
    "# network widths & depths\n",
    "lstm_hidden = 64\n",
    "dec_hidden  = 64\n",
    "attn_dim    = 48\n",
    "num_layers  = 1\n",
    "dropout     = 0.0\n",
    "\n",
    "# training settings\n",
    "batch_size = 144\n",
    "lr         = 1e-4\n",
    "patience   = 20\n",
    "max_epochs = 200\n",
    "\n",
    "# scaler & loss choices (must match factory keys)\n",
    "scaler_used = \"MaxAbs\"\n",
    "model_used  = \"BiAttnPointForecaster\"\n",
    "loss_used   = \"Huber\"\n",
    "beta        = 0.01   # only for Huber loss\n",
    "notes       = None\n",
    "\n",
    "print(f\"configs → seq={seq_len}, feed={feed_len}, fut={fut_len}; \"\n",
    "      f\"batch={batch_size}, lr={lr}, epochs={max_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d84e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] saving model to: models/BiAttnPointForecaster_sl48_fl48_h4_lh64_dh64_nl1_bs144_lr.0001_me200_p20_MaxAbs_Huber\n"
     ]
    }
   ],
   "source": [
    "# ─── metadata & model directory ─────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "\n",
    "# metadata dict\n",
    "md = {\n",
    "    \"model\":        model_used,\n",
    "    \"seq_len\":      seq_len,\n",
    "    \"feed_len\":     feed_len,\n",
    "    \"horizon\":      fut_len,\n",
    "    \"lstm_hidden\":  lstm_hidden,\n",
    "    \"dec_hidden\":   dec_hidden,\n",
    "    \"num_layers\":   num_layers,\n",
    "    \"batch_size\":   batch_size,\n",
    "    \"learning_rate\":lr,\n",
    "    \"max_epochs\":   max_epochs,\n",
    "    \"patience\":     patience,\n",
    "    \"scaler\":       scaler_used,\n",
    "    \"loss\":         loss_used,\n",
    "    **({\"notes\": notes} if notes else {}),\n",
    "}\n",
    "\n",
    "# helper to abbreviate keys\n",
    "initials = lambda s: \"\".join(w[0] for w in s.split(\"_\"))\n",
    "\n",
    "# build tag parts\n",
    "parts = []\n",
    "for k, v in md.items():\n",
    "    sv = str(v)\n",
    "    if isinstance(v, float) and sv.startswith(\"0.\"):\n",
    "        sv = sv.replace(\"0.\", \".\")\n",
    "    part = sv if k in {\"model\",\"scaler\",\"loss\",\"notes\"} else f\"{initials(k)}{sv}\"\n",
    "    parts.append(part)\n",
    "\n",
    "# combine into tag\n",
    "tag = \"_\".join(parts)\n",
    "\n",
    "# determine candidate path, bumping version if needed\n",
    "models_root = Path(\"models\")\n",
    "candidate   = models_root / tag\n",
    "version     = 0\n",
    "while candidate.exists():\n",
    "    version    += 1\n",
    "    candidate   = models_root / f\"{tag}_v{version}\"\n",
    "\n",
    "print(f\"[info] saving model to: {candidate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4745bbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ saving run in: models/BiAttnPointForecaster_sl48_fl48_h4_lh64_dh64_nl1_bs144_lr.0001_me200_p20_MaxAbs_Huber\n",
      "[epoch 001] train=0.00047 val=0.00011\n",
      "[epoch 002] train=0.00039 val=0.00006\n",
      "[epoch 003] train=0.00034 val=0.00007\n",
      "[epoch 004] train=0.00032 val=0.00007\n",
      "[epoch 005] train=0.00030 val=0.00008\n",
      "[epoch 006] train=0.00029 val=0.00005\n",
      "[epoch 007] train=0.00029 val=0.00006\n",
      "[epoch 008] train=0.00028 val=0.00004\n",
      "[epoch 009] train=0.00028 val=0.00005\n",
      "[epoch 010] train=0.00027 val=0.00004\n",
      "[epoch 011] train=0.00027 val=0.00004\n",
      "[epoch 012] train=0.00026 val=0.00004\n",
      "[epoch 013] train=0.00026 val=0.00004\n",
      "[epoch 014] train=0.00025 val=0.00003\n",
      "[epoch 015] train=0.00025 val=0.00003\n",
      "[epoch 016] train=0.00025 val=0.00003\n",
      "[epoch 017] train=0.00024 val=0.00006\n",
      "[epoch 018] train=0.00024 val=0.00005\n",
      "[epoch 019] train=0.00024 val=0.00003\n",
      "[epoch 020] train=0.00024 val=0.00004\n",
      "→ lr reduced from 1.00e-04 to 5.00e-05\n",
      "[epoch 021] train=0.00024 val=0.00003\n",
      "[epoch 022] train=0.00023 val=0.00004\n",
      "[epoch 023] train=0.00023 val=0.00004\n",
      "[epoch 024] train=0.00023 val=0.00003\n",
      "[epoch 025] train=0.00023 val=0.00003\n",
      "[epoch 026] train=0.00023 val=0.00003\n",
      "[epoch 027] train=0.00023 val=0.00003\n",
      "[epoch 028] train=0.00023 val=0.00003\n",
      "[epoch 029] train=0.00023 val=0.00003\n",
      "[epoch 030] train=0.00023 val=0.00004\n",
      "[epoch 031] train=0.00022 val=0.00004\n",
      "→ lr reduced from 5.00e-05 to 2.50e-05\n",
      "[epoch 032] train=0.00022 val=0.00003\n",
      "[epoch 033] train=0.00022 val=0.00003\n",
      "[epoch 034] train=0.00022 val=0.00004\n",
      "[epoch 035] train=0.00022 val=0.00004\n",
      "[epoch 036] train=0.00022 val=0.00003\n",
      "[epoch 037] train=0.00022 val=0.00003\n",
      "→ lr reduced from 2.50e-05 to 1.25e-05\n",
      "[epoch 038] train=0.00022 val=0.00003\n",
      "[epoch 039] train=0.00022 val=0.00003\n",
      "[epoch 040] train=0.00022 val=0.00003\n",
      "[epoch 041] train=0.00022 val=0.00003\n",
      "[epoch 042] train=0.00022 val=0.00003\n",
      "[epoch 043] train=0.00022 val=0.00003\n",
      "→ lr reduced from 1.25e-05 to 6.25e-06\n",
      "[epoch 044] train=0.00022 val=0.00004\n",
      "[epoch 045] train=0.00022 val=0.00004\n",
      "[epoch 046] train=0.00022 val=0.00004\n",
      "→ early stopping after 46 epochs\n",
      "\n",
      "test → mae=27.6088, rmse=35.3961, smape=59.2808%, huber=27.6038\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 215\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, datetime):     \u001b[38;5;28;01mreturn\u001b[39;00m o\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdefault(o)\n\u001b[1;32m    212\u001b[0m env_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_torch\u001b[39m\u001b[38;5;124m\"\u001b[39m:  torch\u001b[38;5;241m.\u001b[39minitial_seed(),\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m:  np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mget_state()[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_python\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mgetstate()[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:    datetime\u001b[38;5;241m.\u001b[39mnow(timezone\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[1;32m    217\u001b[0m }\n\u001b[1;32m    219\u001b[0m data_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m:     df_train\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_end\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_end_date,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_test\u001b[39m\u001b[38;5;124m\"\u001b[39m:    \u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    230\u001b[0m }\n\u001b[1;32m    232\u001b[0m feat_meta \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m: hist_cols,\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m: cal_cols,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_fut_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_fut_feats\n\u001b[1;32m    236\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# ─── train/val/test loop + early stopping ─────────────────────────────────\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import joblib\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# create output directory\n",
    "base_dir = candidate  # candidate is a pathlib.Path\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"→ saving run in: {base_dir}\")\n",
    "\n",
    "# rebuild train/val/test df slices for metadata\n",
    "df_train = df.loc[masks[\"train\"]]\n",
    "df_val   = df.loc[masks[\"val\"]]\n",
    "df_test  = df.loc[masks[\"test\"]]\n",
    "\n",
    "# 1) scale train/val/test\n",
    "\n",
    "# a) history features\n",
    "scaler_x = transformer_factory[scaler_used]()\n",
    "x_train_hist_scaled = scaler_x.fit_transform(splits[\"train\"][\"x_hist\"])\n",
    "x_val_hist_scaled   = scaler_x.transform(splits[\"val\"][\"x_hist\"])\n",
    "x_test_hist_scaled  = scaler_x.transform(splits[\"test\"][\"x_hist\"])\n",
    "\n",
    "# b) future features\n",
    "n_fut_feats = splits[\"train\"][\"x_fut\"].shape[2]\n",
    "scaler_f    = transformer_factory[scaler_used]()\n",
    "flat_f_train = splits[\"train\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
    "flat_f_train = scaler_f.fit_transform(flat_f_train)\n",
    "x_fut_train_scaled = flat_f_train.reshape(splits[\"train\"][\"x_fut\"].shape)\n",
    "\n",
    "flat_f_val   = splits[\"val\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
    "flat_f_val   = scaler_f.transform(flat_f_val)\n",
    "x_fut_val_scaled = flat_f_val.reshape(splits[\"val\"][\"x_fut\"].shape)\n",
    "\n",
    "flat_f_test  = splits[\"test\"][\"x_fut\"].reshape(-1, n_fut_feats)\n",
    "flat_f_test  = scaler_f.transform(flat_f_test)\n",
    "x_fut_test_scaled = flat_f_test.reshape(splits[\"test\"][\"x_fut\"].shape)\n",
    "\n",
    "# c) targets\n",
    "scaler_y       = transformer_factory[scaler_used]()\n",
    "y_train_scaled = scaler_y.fit_transform(splits[\"train\"][\"y\"].reshape(-1,1)).flatten()\n",
    "y_val_scaled   = scaler_y.transform(splits[\"val\"][\"y\"].reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(splits[\"test\"][\"y\"].reshape(-1,1)).flatten()\n",
    "\n",
    "# 2) build dataloaders\n",
    "pin_memory = (device.type == \"cuda\")\n",
    "\n",
    "def to_tensor(x, dtype):\n",
    "    return torch.tensor(x, dtype=dtype)\n",
    "\n",
    "train_ds = MultiFeedDataset(\n",
    "    hist        = to_tensor(x_train_hist_scaled, torch.float32),\n",
    "    full_fut    = to_tensor(x_fut_train_scaled,    torch.float32),\n",
    "    y           = to_tensor(y_train_scaled,        torch.float32),\n",
    "    month_idx   = to_tensor(splits[\"train\"][\"x_cal\"][:,0], torch.long),\n",
    "    weekday_idx = to_tensor(splits[\"train\"][\"x_cal\"][:,1], torch.long),\n",
    "    sp_idx      = to_tensor(splits[\"train\"][\"x_cal\"][:,2], torch.long),\n",
    "    dtype_idx   = to_tensor(splits[\"train\"][\"x_cal\"][:,3], torch.long),\n",
    "    seq_len     = seq_len,\n",
    "    feed_len    = feed_len,\n",
    "    fut_len     = fut_len\n",
    ")\n",
    "val_ds = MultiFeedDataset(\n",
    "    hist        = to_tensor(x_val_hist_scaled,   torch.float32),\n",
    "    full_fut    = to_tensor(x_fut_val_scaled,    torch.float32),\n",
    "    y           = to_tensor(y_val_scaled,        torch.float32),\n",
    "    month_idx   = to_tensor(splits[\"val\"][\"x_cal\"][:,0], torch.long),\n",
    "    weekday_idx = to_tensor(splits[\"val\"][\"x_cal\"][:,1], torch.long),\n",
    "    sp_idx      = to_tensor(splits[\"val\"][\"x_cal\"][:,2], torch.long),\n",
    "    dtype_idx   = to_tensor(splits[\"val\"][\"x_cal\"][:,3], torch.long),\n",
    "    seq_len     = seq_len,\n",
    "    feed_len    = feed_len,\n",
    "    fut_len     = fut_len\n",
    ")\n",
    "test_ds = MultiFeedDataset(\n",
    "    hist        = to_tensor(x_test_hist_scaled,  torch.float32),\n",
    "    full_fut    = to_tensor(x_fut_test_scaled,   torch.float32),\n",
    "    y           = to_tensor(y_test_scaled,       torch.float32),\n",
    "    month_idx   = to_tensor(splits[\"test\"][\"x_cal\"][:,0], torch.long),\n",
    "    weekday_idx = to_tensor(splits[\"test\"][\"x_cal\"][:,1], torch.long),\n",
    "    sp_idx      = to_tensor(splits[\"test\"][\"x_cal\"][:,2], torch.long),\n",
    "    dtype_idx   = to_tensor(splits[\"test\"][\"x_cal\"][:,3], torch.long),\n",
    "    seq_len     = seq_len,\n",
    "    feed_len    = feed_len,\n",
    "    fut_len     = fut_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  pin_memory=pin_memory)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "# 3) instantiate model, optimizer, criterion\n",
    "time_feat_dim = 4 + 3 + 6 + 2\n",
    "model = model_factory[model_used](\n",
    "    num_hist_feats = x_train_hist_scaled.shape[1],\n",
    "    num_fut_feats  = n_fut_feats,\n",
    "    time_feat_dim  = time_feat_dim,\n",
    "    lstm_hidden    = lstm_hidden,\n",
    "    dec_hidden     = dec_hidden,\n",
    "    attn_dim       = attn_dim,\n",
    "    hist_len       = seq_len,\n",
    "    feed_len       = feed_len,\n",
    "    fut_len        = fut_len\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = (\n",
    "    loss_factory[loss_used](beta=beta)\n",
    "    if loss_used == \"huber\"\n",
    "    else loss_factory[loss_used]()\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7\n",
    ")\n",
    "last_lrs = scheduler.get_last_lr()\n",
    "\n",
    "# 4) train w/ early stopping on val\n",
    "best_val, epochs_no_improve, best_ckpt = float('inf'), 0, None\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for x_h, x_f, y_t, mi, wi, si, di in train_loader:\n",
    "        x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
    "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out  = model(x_h, x_f, mi, wi, si, di)\n",
    "        loss = criterion(out, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_h, x_f, y_t, mi, wi, si, di in val_loader:\n",
    "            x_h, x_f, y_t = x_h.to(device), x_f.to(device), y_t.to(device)\n",
    "            mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "            out = model(x_h, x_f, mi, wi, si, di)\n",
    "            total_val_loss += criterion(out, y_t).item()\n",
    "    val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = scheduler.get_last_lr()[0]\n",
    "    if new_lr != last_lrs[0]:\n",
    "        print(f\"→ lr reduced from {last_lrs[0]:.2e} to {new_lr:.2e}\")\n",
    "    last_lrs = scheduler.get_last_lr()\n",
    "\n",
    "    print(f\"[epoch {epoch:03d}] train={train_loss:.5f} val={val_loss:.5f}\")\n",
    "    if val_loss < best_val:\n",
    "        best_val, epochs_no_improve = val_loss, 0\n",
    "        best_ckpt = {\n",
    "            \"model\":     copy.deepcopy(model.state_dict()),\n",
    "            \"optimizer\": copy.deepcopy(optimizer.state_dict())\n",
    "        }\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"→ early stopping after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "# restore best checkpoint\n",
    "model.load_state_dict(best_ckpt[\"model\"])\n",
    "optimizer.load_state_dict(best_ckpt[\"optimizer\"])\n",
    "\n",
    "# 5) inference on test\n",
    "model.eval()\n",
    "preds_all, trues_all = [], []\n",
    "with torch.no_grad():\n",
    "    for x_h, x_f, y_t, mi, wi, si, di in test_loader:\n",
    "        x_h, x_f = x_h.to(device), x_f.to(device)\n",
    "        mi, wi, si, di = mi.to(device), wi.to(device), si.to(device), di.to(device)\n",
    "        out = model(x_h, x_f, mi, wi, si, di)\n",
    "        preds_all.append(out.cpu().numpy())\n",
    "        trues_all.append(y_t.numpy())\n",
    "\n",
    "preds_all = np.concatenate(preds_all, axis=0)\n",
    "trues_all = np.concatenate(trues_all, axis=0)\n",
    "preds     = scaler_y.inverse_transform(preds_all.reshape(-1,1)).flatten()\n",
    "trues     = scaler_y.inverse_transform(trues_all.reshape(-1,1)).flatten()\n",
    "errors    = trues - preds\n",
    "\n",
    "mae   = mean_absolute_error(trues, preds)\n",
    "rmse  = np.sqrt(mean_squared_error(trues, preds))\n",
    "smape = np.mean(2.0 * np.abs(errors) / (np.abs(trues) + np.abs(preds) + 1e-8)) * 100\n",
    "huber_vals = np.where(np.abs(errors) <= beta,\n",
    "                      0.5 * errors**2 / beta,\n",
    "                      np.abs(errors) - 0.5 * beta)\n",
    "huber = huber_vals.mean()\n",
    "\n",
    "print(f\"\\ntest → mae={mae:.4f}, rmse={rmse:.4f}, smape={smape:.4f}%, huber={huber:.4f}\")\n",
    "\n",
    "# 6) build metadata & save\n",
    "class NpTorchJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, np.generic):   return o.item()\n",
    "        if isinstance(o, np.ndarray):   return o.tolist()\n",
    "        if isinstance(o, torch.Tensor): return o.detach().cpu().tolist()\n",
    "        if isinstance(o, torch.device): return str(o)\n",
    "        if isinstance(o, datetime):     return o.isoformat()\n",
    "        return super().default(o)\n",
    "\n",
    "env_meta = {\n",
    "    \"seed_torch\":  torch.initial_seed(),\n",
    "    \"seed_numpy\":  np.random.get_state()[1][0],\n",
    "    \"seed_python\": random.getstate()[1][0],\n",
    "    \"run_time\":    datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "data_meta = {\n",
    "    \"start\":     df_train.index.min().strftime(\"%Y-%m-%d\"),\n",
    "    \"train_end\": train_end_date,\n",
    "    \"val_end\":   val_end_date,\n",
    "    \"end\":       df_test.index.max().strftime(\"%Y-%m-%d\"),\n",
    "    \"seq_len\":   seq_len,\n",
    "    \"feed_len\":  feed_len,\n",
    "    \"horizon\":   fut_len,\n",
    "    \"n_train\":   len(train_loader.dataset),\n",
    "    \"n_val\":     len(val_loader.dataset),\n",
    "    \"n_test\":    len(test_loader.dataset)\n",
    "}\n",
    "\n",
    "feat_meta = {\n",
    "    \"hist_feats\": hist_cols,\n",
    "    \"time_feats\": cal_cols,\n",
    "    \"n_fut_feats\": n_fut_feats\n",
    "}\n",
    "\n",
    "loader_meta = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"pin_memory\": pin_memory,\n",
    "    \"shuffle\":    {\"train\": True, \"val\": False, \"test\": False}\n",
    "}\n",
    "\n",
    "hyperparams_meta = {\n",
    "    \"model\":         model_used,\n",
    "    \"seq_len\":       seq_len,\n",
    "    \"feed_len\":      feed_len,\n",
    "    \"horizon\":       fut_len,\n",
    "    \"lstm_hidden\":   lstm_hidden,\n",
    "    \"dec_hidden\":    dec_hidden,\n",
    "    \"attn_dim\":      attn_dim,\n",
    "    \"num_layers\":    num_layers,\n",
    "    \"batch_size\":    batch_size,\n",
    "    \"learning_rate\": lr,\n",
    "    \"max_epochs\":    max_epochs,\n",
    "    \"patience\":      patience,\n",
    "    \"scaler\":        scaler_used,\n",
    "    \"loss\":          loss_used,\n",
    "    **({\"beta\": beta} if loss_used == \"huber\" else {})\n",
    "}\n",
    "\n",
    "optim_meta = {\n",
    "    \"type\": optimizer.__class__.__name__,\n",
    "    \"lr\":   optimizer.param_groups[0][\"lr\"]\n",
    "}\n",
    "\n",
    "sched_meta = {\n",
    "    \"type\":     scheduler.__class__.__name__,\n",
    "    \"factor\":   getattr(scheduler, \"factor\", None),\n",
    "    \"patience\": getattr(scheduler, \"patience\", None)\n",
    "}\n",
    "\n",
    "earlystop_meta = {\"max_epochs\": max_epochs, \"patience\": patience}\n",
    "\n",
    "metrics_meta = {\"mae\": mae, \"rmse\": rmse, \"smape\": smape, \"huber\": huber}\n",
    "\n",
    "# save model & scalers\n",
    "torch.save({\n",
    "    \"model_state_dict\":     model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict()\n",
    "}, base_dir / \"torch_model.pt\")\n",
    "\n",
    "joblib.dump({\n",
    "    \"scaler_x\": scaler_x,\n",
    "    \"scaler_f\": scaler_f,\n",
    "    \"scaler_y\": scaler_y\n",
    "}, str(base_dir / \"scalers.joblib\"))\n",
    "\n",
    "with open(base_dir / \"test_summary.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"environment\":  env_meta,\n",
    "        \"data\":         data_meta,\n",
    "        \"features\":     feat_meta,\n",
    "        \"dataloader\":   loader_meta,\n",
    "        \"hyperparams\":  hyperparams_meta,\n",
    "        \"optimizer\":    optim_meta,\n",
    "        \"scheduler\":    sched_meta,\n",
    "        \"early_stop\":   earlystop_meta,\n",
    "        \"metrics\":      metrics_meta\n",
    "    }, f, indent=2, cls=NpTorchJSONEncoder)\n",
    "\n",
    "print(f\"✅ saved all outputs to {base_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
