{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9dac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random, json, copy\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# set deterministic seeds\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# pick device\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3249b",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853f7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"df_all.csv\", index_col=\"startTime\", parse_dates=True)\n",
    "\n",
    "# Crop the DataFrame to the specified date range\n",
    "df = df.loc[\"2021-07-01\":\"2025-06-30\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\n",
    "    'Forecast Wind',\n",
    "    'Forecast Solar',\n",
    "    'Actual Wind',\n",
    "    'Actual Solar',\n",
    "    'Settlement Period',\n",
    "])\n",
    "\n",
    "# Calculate time of day features\n",
    "minutes = df.index.hour * 60 + df.index.minute\n",
    "frac_day = minutes / (24 * 60)\n",
    "df['tod_sin'] = np.sin(2 * np.pi * frac_day)\n",
    "df['tod_cos'] = np.cos(2 * np.pi * frac_day)\n",
    "\n",
    "# Calculate day of week features\n",
    "day_of_week = df.index.dayofweek\n",
    "frac_week = day_of_week / 7\n",
    "df['dow_sin'] = np.sin(2 * np.pi * frac_week)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * frac_week)\n",
    "\n",
    "# Calculate month of year features\n",
    "month = df.index.month\n",
    "frac_year = (month - 1) / 12\n",
    "df['moy_sin'] = np.sin(2 * np.pi * frac_year)\n",
    "df['moy_cos'] = np.cos(2 * np.pi * frac_year)\n",
    "\n",
    "\n",
    "# — splits\n",
    "train_end = '2025-03-01'  # start of validation\n",
    "val_end   = '2025-05-01'  # start of test\n",
    "\n",
    "# slice once …\n",
    "train_df = df.loc[:train_end]\n",
    "val_df   = df.loc[train_end:val_end]\n",
    "test_df  = df.loc[val_end:]\n",
    "\n",
    "# … then unpack X & y in one go without repeating .drop\n",
    "X_train, y_train = train_df.drop(columns=['Imbalance Price']), train_df['Imbalance Price']\n",
    "X_val,      y_val      = val_df.drop(columns=['Imbalance Price']),    val_df['Imbalance Price']\n",
    "X_test,     y_test     = test_df.drop(columns=['Imbalance Price']),   test_df['Imbalance Price']\n",
    "\n",
    "\n",
    "# lists of feature‐columns\n",
    "time_feats  = [c for c in ['tod_sin','tod_cos','dow_sin','dow_cos','moy_sin','moy_cos'] if c in df]\n",
    "other_feats = [c for c in X_train.columns if c not in time_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b120e2",
   "metadata": {},
   "source": [
    "## 2. Models, Dataset & Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101baa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────── a. Dataset Definition ─────────────────────────────────\n",
    "\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len, horizon=1):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0] - self.seq_len - self.horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq    = self.X[idx : idx + self.seq_len]\n",
    "        target_i = idx + self.seq_len - 1 + self.horizon\n",
    "        y_target = self.y[target_i]\n",
    "        return x_seq, y_target\n",
    "\n",
    "\n",
    "# ──────────── b. Layer Definitions ──────────────────────────────────\n",
    "\n",
    "class SeasonalAttn(nn.Module):\n",
    "    def __init__(self, seq_len=48):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Parameter(torch.empty(seq_len))\n",
    "        nn.init.uniform_(self.attn, -0.01, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.attn.view(1, -1, 1) # (B,N,F)\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, num_feats, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_feats,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (_ , _) = self.lstm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.v(torch.tanh(self.W_h(x)))   # (B, N, 1)\n",
    "        alpha = torch.softmax(e, dim=1)       # (B, N, 1)\n",
    "        context = torch.sum(alpha * x, dim=1) # (B, 2H)\n",
    "        return context\n",
    "\n",
    "\n",
    "# ──────────── c. Model Definitions ──────────────────────────────────\n",
    "\n",
    "class SA_BiLSTM(nn.Module):\n",
    "    def __init__(self, num_feats, seq_len=48, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.seasonal = SeasonalAttn(seq_len)\n",
    "        self.lstm = BiLSTM(num_feats, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seasonal(x)       # (B, N, F)\n",
    "        out = self.lstm(x)         # (B, N, 2H)\n",
    "        last = out[:, -1, :]       # (B, 2H)\n",
    "        return self.fc(last).squeeze(-1)  # (B,)\n",
    "\n",
    "\n",
    "class SA_BiLSTM_AttnPool(nn.Module):\n",
    "    def __init__(self, num_feats, seq_len=48, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.seasonal = SeasonalAttn(seq_len)\n",
    "        self.lstm = BiLSTM(num_feats, hidden_size, num_layers)\n",
    "        self.pool = AttentionPool(input_dim=hidden_size * 2, hidden_dim=hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seasonal(x)       # (B, N, F)\n",
    "        out = self.lstm(x)         # (B, N, 2H)\n",
    "        context = self.pool(out)   # (B, 2H)\n",
    "        return self.fc(context).squeeze(-1)  # (B,)\n",
    "    \n",
    "TRANSFORMER_FACTORY = {\n",
    "    \"MinMax\":   MinMaxScaler,\n",
    "    \"Standard\": StandardScaler,\n",
    "    \"Robust\":   RobustScaler,\n",
    "    \"MaxAbs\":   MaxAbsScaler\n",
    "}\n",
    "MODEL_FACTORY = {\n",
    "    \"SA_BiLSTM\":             SA_BiLSTM,\n",
    "    \"SA_BiLSTM_AttnPool\":    SA_BiLSTM_AttnPool\n",
    "}\n",
    "LOSS_FACTORY = {\n",
    "    \"MSE\":    nn.MSELoss,\n",
    "    \"MAE\":    nn.L1Loss,\n",
    "    \"Huber\":  nn.SmoothL1Loss\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c86939",
   "metadata": {},
   "source": [
    "## 3. Search Stage 1 -- Big 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14629f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_stage1(trial):\n",
    "    # ── 1) Sample hyper‐params ────────────────────────────────────────\n",
    "    lr          = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    # ── Fixed settings ────────────────────────────────────────────────\n",
    "    seq_len, horizon, num_layers = 48, 1, 2\n",
    "    model_name, scaler_used, loss_used = \"SA_BiLSTM\", \"MaxAbs\", \"Huber\"\n",
    "    beta, max_epochs, patience = 0.01, 20, 10  # shorter for speed\n",
    "\n",
    "    # report start\n",
    "    print(f\"\\n→ Trial {trial.number}: lr={lr:.2e}, hidden={hidden_size}, bs={batch_size}\")\n",
    "\n",
    "    # ── 2) Data prep ──────────────────────────────────────────────────\n",
    "    transformer = ColumnTransformer(\n",
    "        [(\"scale\", TRANSFORMER_FACTORY[scaler_used](), other_feats)],\n",
    "        remainder=\"passthrough\", verbose_feature_names_out=False\n",
    "    )\n",
    "    X_tr = transformer.fit_transform(X_train)\n",
    "    X_va = transformer.transform(X_val)\n",
    "\n",
    "    scaler_y = TRANSFORMER_FACTORY[scaler_used]()\\\n",
    "                   .fit(y_train.values.reshape(-1,1))\n",
    "    y_tr = scaler_y.transform(y_train.values.reshape(-1,1)).ravel()\n",
    "    y_va = scaler_y.transform(y_val.values.reshape(-1,1)).ravel()\n",
    "\n",
    "    tr_lo = DataLoader(\n",
    "        LSTMDataset(X_tr, y_tr, seq_len, horizon),\n",
    "        batch_size=batch_size, shuffle=True,  pin_memory=True\n",
    "    )\n",
    "    va_lo = DataLoader(\n",
    "        LSTMDataset(X_va, y_va, seq_len, horizon),\n",
    "        batch_size=batch_size, shuffle=False, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # ── 3) Model / Optimizer / Loss / Scheduler ───────────────────────\n",
    "    model     = MODEL_FACTORY[model_name](\n",
    "                    num_feats=X_tr.shape[1],\n",
    "                    seq_len=seq_len,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers\n",
    "                ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = (LOSS_FACTORY[loss_used](beta=beta)\n",
    "                 if loss_used==\"Huber\"\n",
    "                 else LOSS_FACTORY[loss_used]())\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #                 optimizer, mode=\"min\", factor=0.5,\n",
    "    #                 patience=5, min_lr=1e-7\n",
    "    #             )\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # ── 4) Training w/ pruning & prints ───────────────────────────────\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # — train —\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for xb, yb in tr_lo:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb).squeeze(-1), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "        tr_loss /= len(tr_lo)\n",
    "\n",
    "        # — validate —\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_lo:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                va_loss += criterion(model(xb).squeeze(-1), yb).item()\n",
    "        va_loss /= len(va_lo)\n",
    "\n",
    "        # — scheduler & LR print —\n",
    "        # scheduler.step(va_loss)\n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        print(\n",
    "            f\"[Trial {trial.number}] \"\n",
    "            f\"Epoch {epoch:02d}/{max_epochs}  \"\n",
    "            # f\"lr={current_lr:.2e}  \"\n",
    "            f\"train={tr_loss:.4f}  val={va_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # — report & prune —\n",
    "        trial.report(va_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            print(f\"→ Trial {trial.number} pruned at epoch {epoch}\")\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # — manual early stopping —\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"→ Trial {trial.number} early-stopped at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # ── 5) Load best & compute final metrics on validation set ─────────\n",
    "    model.load_state_dict(best_weights)\n",
    "    model.eval()\n",
    "    scaled_preds, scaled_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in va_lo:\n",
    "            xb = xb.to(device)\n",
    "            p  = model(xb).squeeze(-1).cpu().numpy()\n",
    "            scaled_preds.append(p)\n",
    "            scaled_trues.append(yb.numpy())\n",
    "    scaled_preds = np.concatenate(scaled_preds)\n",
    "    scaled_trues = np.concatenate(scaled_trues)\n",
    "\n",
    "    # invert scaling\n",
    "    preds = scaler_y.inverse_transform(scaled_preds.reshape(-1,1)).flatten()\n",
    "    trues = scaler_y.inverse_transform(scaled_trues.reshape(-1,1)).flatten()\n",
    "    err   = trues - preds\n",
    "\n",
    "    mae   = mean_absolute_error(trues, preds)\n",
    "    rmse  = np.sqrt(mean_squared_error(trues, preds))\n",
    "    smape = np.mean(2*np.abs(err)/(np.abs(trues)+np.abs(preds)+1e-8))*100\n",
    "\n",
    "    print(\n",
    "        f\"→ Trial {trial.number} finished: \"\n",
    "        f\"best_val={best_val:.4f}  \"\n",
    "        f\"MAE={mae:.4f}  RMSE={rmse:.4f}  sMAPE={smape:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # store metrics\n",
    "    trial.set_user_attr(\"val_mae\", mae)\n",
    "    trial.set_user_attr(\"val_rmse\", rmse)\n",
    "    trial.set_user_attr(\"val_smape\", smape)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "\n",
    "# ── Run Stage 1 ───────────────────────────────────────────────────────\n",
    "study1 = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    ")\n",
    "study1.optimize(objective_stage1, n_trials=30)\n",
    "print(\"✔ Stage 1 done:\", study1.best_params, \"val_loss=\", study1.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
